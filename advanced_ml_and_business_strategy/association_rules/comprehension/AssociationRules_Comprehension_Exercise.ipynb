{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ca3983",
   "metadata": {},
   "source": [
    "# Task 1 - Data Preparation\n",
    "For this task, you will perform the following steps:\n",
    "- Load all the necessary packages for this exercise\n",
    "- Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2af6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'numpy' and 'pandas' to work with numbers and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import 'matplotlib.pyplot' and 'seaborn' for visualizations\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import 'apriori' and 'association_rules' from 'mlxtend' for working with association rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab04dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abrasive cleaner</th>\n",
       "      <th>artif. sweetener</th>\n",
       "      <th>baby cosmetics</th>\n",
       "      <th>baby food</th>\n",
       "      <th>bags</th>\n",
       "      <th>baking powder</th>\n",
       "      <th>bathroom cleaner</th>\n",
       "      <th>beef</th>\n",
       "      <th>berries</th>\n",
       "      <th>beverages</th>\n",
       "      <th>...</th>\n",
       "      <th>UHT-milk</th>\n",
       "      <th>vinegar</th>\n",
       "      <th>waffles</th>\n",
       "      <th>whipped/sour cream</th>\n",
       "      <th>whisky</th>\n",
       "      <th>white bread</th>\n",
       "      <th>white wine</th>\n",
       "      <th>whole milk</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>zwieback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abrasive cleaner  artif. sweetener  baby cosmetics  baby food  bags  \\\n",
       "transID                                                                        \n",
       "1                       0                 0               0          0     0   \n",
       "2                       0                 0               0          0     0   \n",
       "3                       0                 0               0          0     0   \n",
       "4                       0                 0               0          0     0   \n",
       "5                       0                 0               0          0     0   \n",
       "\n",
       "         baking powder  bathroom cleaner  beef  berries  beverages  ...  \\\n",
       "transID                                                             ...   \n",
       "1                    0                 0     0        0          0  ...   \n",
       "2                    0                 0     0        0          0  ...   \n",
       "3                    0                 0     0        0          0  ...   \n",
       "4                    0                 0     0        0          0  ...   \n",
       "5                    0                 0     0        0          0  ...   \n",
       "\n",
       "         UHT-milk  vinegar  waffles  whipped/sour cream  whisky  white bread  \\\n",
       "transID                                                                        \n",
       "1               0        0        0                   0       0            0   \n",
       "2               0        0        0                   0       0            0   \n",
       "3               0        0        0                   0       0            0   \n",
       "4               0        0        0                   0       0            0   \n",
       "5               0        0        0                   0       0            0   \n",
       "\n",
       "         white wine  whole milk  yogurt  zwieback  \n",
       "transID                                            \n",
       "1                 0           0       0         0  \n",
       "2                 0           0       1         0  \n",
       "3                 0           1       0         0  \n",
       "4                 0           0       1         0  \n",
       "5                 0           1       0         0  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and take a look at it\n",
    "# Note: The data needs to be in binary matrix format\n",
    "df = pd.read_csv('supermarket_binarymat.csv', index_col = 'transID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae95c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abrasive cleaner</th>\n",
       "      <th>artif. sweetener</th>\n",
       "      <th>baby cosmetics</th>\n",
       "      <th>baby food</th>\n",
       "      <th>bags</th>\n",
       "      <th>baking powder</th>\n",
       "      <th>bathroom cleaner</th>\n",
       "      <th>beef</th>\n",
       "      <th>berries</th>\n",
       "      <th>beverages</th>\n",
       "      <th>...</th>\n",
       "      <th>UHT-milk</th>\n",
       "      <th>vinegar</th>\n",
       "      <th>waffles</th>\n",
       "      <th>whipped/sour cream</th>\n",
       "      <th>whisky</th>\n",
       "      <th>white bread</th>\n",
       "      <th>white wine</th>\n",
       "      <th>whole milk</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>zwieback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abrasive cleaner  artif. sweetener  baby cosmetics  baby food   bags  \\\n",
       "transID                                                                         \n",
       "1                   False             False           False      False  False   \n",
       "2                   False             False           False      False  False   \n",
       "3                   False             False           False      False  False   \n",
       "4                   False             False           False      False  False   \n",
       "5                   False             False           False      False  False   \n",
       "\n",
       "         baking powder  bathroom cleaner   beef  berries  beverages  ...  \\\n",
       "transID                                                              ...   \n",
       "1                False             False  False    False      False  ...   \n",
       "2                False             False  False    False      False  ...   \n",
       "3                False             False  False    False      False  ...   \n",
       "4                False             False  False    False      False  ...   \n",
       "5                False             False  False    False      False  ...   \n",
       "\n",
       "         UHT-milk  vinegar  waffles  whipped/sour cream  whisky  white bread  \\\n",
       "transID                                                                        \n",
       "1           False    False    False               False   False        False   \n",
       "2           False    False    False               False   False        False   \n",
       "3           False    False    False               False   False        False   \n",
       "4           False    False    False               False   False        False   \n",
       "5           False    False    False               False   False        False   \n",
       "\n",
       "         white wine  whole milk  yogurt  zwieback  \n",
       "transID                                            \n",
       "1             False       False   False     False  \n",
       "2             False       False    True     False  \n",
       "3             False        True   False     False  \n",
       "4             False       False    True     False  \n",
       "5             False        True   False     False  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data into the Boolean data type\n",
    "# Note: Converting data types to Boolean is not absolutely essential but it is computationally faster\n",
    "df = df.astype(bool)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c07d39d-4c01-40fb-9536-019b98792806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9835, 169)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73851601-6923-4780-97b4-e93fce65d4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abrasive cleaner</th>\n",
       "      <th>artif. sweetener</th>\n",
       "      <th>baby cosmetics</th>\n",
       "      <th>baby food</th>\n",
       "      <th>bags</th>\n",
       "      <th>baking powder</th>\n",
       "      <th>bathroom cleaner</th>\n",
       "      <th>beef</th>\n",
       "      <th>berries</th>\n",
       "      <th>beverages</th>\n",
       "      <th>...</th>\n",
       "      <th>UHT-milk</th>\n",
       "      <th>vinegar</th>\n",
       "      <th>waffles</th>\n",
       "      <th>whipped/sour cream</th>\n",
       "      <th>whisky</th>\n",
       "      <th>white bread</th>\n",
       "      <th>white wine</th>\n",
       "      <th>whole milk</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>zwieback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         abrasive cleaner  artif. sweetener  baby cosmetics  baby food  bags  \\\n",
       "transID                                                                        \n",
       "1                       0                 0               0          0     0   \n",
       "2                       0                 0               0          0     0   \n",
       "3                       0                 0               0          0     0   \n",
       "4                       0                 0               0          0     0   \n",
       "5                       0                 0               0          0     0   \n",
       "\n",
       "         baking powder  bathroom cleaner  beef  berries  beverages  ...  \\\n",
       "transID                                                             ...   \n",
       "1                    0                 0     0        0          0  ...   \n",
       "2                    0                 0     0        0          0  ...   \n",
       "3                    0                 0     0        0          0  ...   \n",
       "4                    0                 0     0        0          0  ...   \n",
       "5                    0                 0     0        0          0  ...   \n",
       "\n",
       "         UHT-milk  vinegar  waffles  whipped/sour cream  whisky  white bread  \\\n",
       "transID                                                                        \n",
       "1               0        0        0                   0       0            0   \n",
       "2               0        0        0                   0       0            0   \n",
       "3               0        0        0                   0       0            0   \n",
       "4               0        0        0                   0       0            0   \n",
       "5               0        0        0                   0       0            0   \n",
       "\n",
       "         white wine  whole milk  yogurt  zwieback  \n",
       "transID                                            \n",
       "1                 0           0       0         0  \n",
       "2                 0           0       1         0  \n",
       "3                 0           1       0         0  \n",
       "4                 0           0       1         0  \n",
       "5                 0           1       0         0  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b10121b",
   "metadata": {},
   "source": [
    "# Task 2 - Build and Analyze Frequent Itemsets\n",
    "For this task, you will perform the following steps:\n",
    "- Generate collections of frequent itemsets for different minimum support threshold values\n",
    "  - Analyze how the number of itemsets in these collections varies as the minimum support threshold is altered\n",
    "  - Analyze how the mean support of the itemsets in these collections varies as the minimum support threshold is increased\n",
    "  - Choose a single collection of itemsets on which to build rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0cd5f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017692</td>\n",
       "      <td>(baking powder)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.052466</td>\n",
       "      <td>(beef)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.033249</td>\n",
       "      <td>(berries)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026029</td>\n",
       "      <td>(beverages)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080529</td>\n",
       "      <td>(bottled beer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>0.011998</td>\n",
       "      <td>(tropical fruit, whole milk, root vegetables)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>0.014540</td>\n",
       "      <td>(whole milk, yogurt, root vegetables)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>0.010473</td>\n",
       "      <td>(soda, yogurt, whole milk)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>0.015150</td>\n",
       "      <td>(tropical fruit, whole milk, yogurt)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>0.010880</td>\n",
       "      <td>(whole milk, yogurt, whipped/sour cream)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>333 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                       itemsets\n",
       "0    0.017692                                (baking powder)\n",
       "1    0.052466                                         (beef)\n",
       "2    0.033249                                      (berries)\n",
       "3    0.026029                                    (beverages)\n",
       "4    0.080529                                 (bottled beer)\n",
       "..        ...                                            ...\n",
       "328  0.011998  (tropical fruit, whole milk, root vegetables)\n",
       "329  0.014540          (whole milk, yogurt, root vegetables)\n",
       "330  0.010473                     (soda, yogurt, whole milk)\n",
       "331  0.015150           (tropical fruit, whole milk, yogurt)\n",
       "332  0.010880       (whole milk, yogurt, whipped/sour cream)\n",
       "\n",
       "[333 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain frequent itemsets for the data using the 'apriori()' method\n",
    "# Hint: Study the documentation of the 'apriori()' method\n",
    "# Hint: Set the 'min_support' parameter to 0.01\n",
    "# Note: Set the 'use_colnames' parameter to 'True'\n",
    "frequent_itemsets1 = apriori(df,min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3846c016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of itemsets in 'frequent_itemsets1' using the 'len()' method\n",
    "len(frequent_itemsets1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b709fb36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL 5590\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\mlxtend\\frequent_patterns\\fpcommon.py:109: DeprecationWarning: DataFrames with non-bool types result in worse computationalperformance and their support might be discontinued in the future.Please use a DataFrame with bool type\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minimum support</th>\n",
       "      <th>number of itemsets</th>\n",
       "      <th>mean support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>333</td>\n",
       "      <td>0.025069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>122</td>\n",
       "      <td>0.045115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>63</td>\n",
       "      <td>0.064351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.04</td>\n",
       "      <td>41</td>\n",
       "      <td>0.080563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>31</td>\n",
       "      <td>0.092120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    minimum support  number of itemsets  mean support\n",
       "0              0.01                 333      0.025069\n",
       "1              0.02                 122      0.045115\n",
       "2              0.03                  63      0.064351\n",
       "3              0.04                  41      0.080563\n",
       "4              0.05                  31      0.092120\n",
       "..              ...                 ...           ...\n",
       "94             0.95                   0      0.000000\n",
       "95             0.96                   0      0.000000\n",
       "96             0.97                   0      0.000000\n",
       "97             0.98                   0      0.000000\n",
       "98             0.99                   0      0.000000\n",
       "\n",
       "[99 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain collections of itemsets from the data for different values of 'min_support' using the 'apriori()' method\n",
    "\n",
    "# Declare the list of minimum support values to iterate over\n",
    "suplist = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "# Initiate lists to store the different collections of itemsets, the number of itemsets in each collection, and the mean support of the itemsets in each collection\n",
    "itemsetcollections = []\n",
    "nitemsets = []\n",
    "meansuplist = []\n",
    "\n",
    "# Iterate over the list of minimum support threshold values\n",
    "# Hint: The 'min_support' parameter for the 'apriori()' method should be set to the current value of minimum support threshold\n",
    "for supvalue in suplist:\n",
    "    tempitemsetcollection = apriori(df, min_support=supvalue)\n",
    "    \n",
    "    # Store the current collection of itemsets in the list 'itemsetcollections'\n",
    "    itemsetcollections.append(tempitemsetcollection)\n",
    "    \n",
    "    # Store the number of itemsets in the current collection of itemsets in the list 'nitemsets'\n",
    "    nitemsets.append(len(tempitemsetcollection['itemsets']))\n",
    "    \n",
    "    # Store the mean support of the itemsets in the current collection of itemsets in the list 'meansuplist'\n",
    "    meansuplist.append(tempitemsetcollection['support'].mean())\n",
    "\n",
    "# Create and display a data frame that shows various attributes of the collections of itemsets for different values of minimum support\n",
    "itemsetsummary = pd.DataFrame(data = {'minimum support': suplist,\n",
    "                                      'number of itemsets': nitemsets,\n",
    "                                      'mean support': meansuplist})\n",
    "\n",
    "# Fill missing values in the data frame with 0\n",
    "# Note: If the number of itemsets in a collection is 0, then the mean support is 0 for all practical purposes\n",
    "itemsetsummary.fillna(0, inplace = True)\n",
    "itemsetsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660f0b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize how the collections of itemsets vary as the minimum support threshold is increased\n",
    "plt.figure(figsize = (16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.lineplot(data = itemsetsummary,\n",
    "             x = itemsetsummary['minimum support'].apply(lambda x: np.round(x, 2)),\n",
    "             y = 'number of itemsets',\n",
    "             color = 'red')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.title('Number of Itemsets per Collection as a Function of Minimum Support')\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.lineplot(data = itemsetsummary,\n",
    "             x = itemsetsummary['minimum support'].apply(lambda x: np.round(x, 2)),\n",
    "             y = 'mean support',\n",
    "             color = 'blue')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.title('Mean Support of Itemsets per Collection as a Function of Minimum Support');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8049bbcd-cadd-49ea-8ee6-2a2fbc9d62e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itemsetcollections[3]['itemsets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42b683cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain all the collections of itemsets that have at least 10 itemsets and whose mean support per collection is at least 0.05\n",
    "itemsetcollectionsfinal = []\n",
    "i = -1\n",
    "for supvalue in suplist:\n",
    "    i = i + 1\n",
    "    if ((len(itemsetcollections[i]['itemsets']) >= 10)  & (meansuplist[i] >= 0.05)):\n",
    "        itemsetcollectionsfinal.append(itemsetcollections[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7abdc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052466</td>\n",
       "      <td>(7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033249</td>\n",
       "      <td>(8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.080529</td>\n",
       "      <td>(10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110524</td>\n",
       "      <td>(11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064870</td>\n",
       "      <td>(13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.048907</td>\n",
       "      <td>(123, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.040061</td>\n",
       "      <td>(138, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.042298</td>\n",
       "      <td>(157, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.032232</td>\n",
       "      <td>(162, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.056024</td>\n",
       "      <td>(166, 167)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     support    itemsets\n",
       "0   0.052466         (7)\n",
       "1   0.033249         (8)\n",
       "2   0.080529        (10)\n",
       "3   0.110524        (11)\n",
       "4   0.064870        (13)\n",
       "..       ...         ...\n",
       "58  0.048907  (123, 166)\n",
       "59  0.040061  (138, 166)\n",
       "60  0.042298  (157, 166)\n",
       "61  0.032232  (162, 166)\n",
       "62  0.056024  (166, 167)\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select and view the first collection of itemsets that has the maximum number of itemsets\n",
    "# Hint: The number of itemsets per collection decreases monotonically as the minimum support threshold is increased\n",
    "itemsetcollectionfinal = itemsetcollectionsfinal[0]\n",
    "itemsetcollectionfinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825057ee",
   "metadata": {},
   "source": [
    "# Task 3 - Build and Analyze Association Rules\n",
    "For this task, you will perform the following steps:\n",
    "- Explore the rules generated from the selected collection of itemsets for different performance metrics and threshold values\n",
    "- Generate association rule sets from the selected collection of itemsets for different minimum lift threshold values\n",
    "  - Analyze how the number of rules in these collections varies as the minimum lift threshold is increased\n",
    "  - Analyze how the mean support, mean confidence and mean lift ratio of the rules in these collections vary as the minimum lift threshold is increased\n",
    "  - Choose a single collection of rules from which strong rules can be extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "465e8cd6-4586-4000-bbf0-2adf6c20d175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052466</td>\n",
       "      <td>(7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033249</td>\n",
       "      <td>(8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.080529</td>\n",
       "      <td>(10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.110524</td>\n",
       "      <td>(11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064870</td>\n",
       "      <td>(13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.048907</td>\n",
       "      <td>(123, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.040061</td>\n",
       "      <td>(138, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.042298</td>\n",
       "      <td>(157, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.032232</td>\n",
       "      <td>(162, 166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.056024</td>\n",
       "      <td>(166, 167)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     support    itemsets\n",
       "0   0.052466         (7)\n",
       "1   0.033249         (8)\n",
       "2   0.080529        (10)\n",
       "3   0.110524        (11)\n",
       "4   0.064870        (13)\n",
       "..       ...         ...\n",
       "58  0.048907  (123, 166)\n",
       "59  0.040061  (138, 166)\n",
       "60  0.042298  (157, 166)\n",
       "61  0.032232  (162, 166)\n",
       "62  0.056024  (166, 167)\n",
       "\n",
       "[63 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemsetcollectionfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62637ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [antecedents, consequents, antecedent support, consequent support, support, confidence, lift, leverage, conviction, zhangs_metric]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate association rules for the collection of itemsets 'itemsetcollectionfinal' using the 'association_rules()' method\n",
    "# Hint: Study the documentation of the 'association_rules()' method\n",
    "# Note: Keep the default values for the 'metric' and the 'min_threshold' parameters\n",
    "rules1 = association_rules(itemsetcollectionfinal)\n",
    "rules1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e195eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of rules in 'rules1' using the 'len()' method\n",
    "len(rules1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cad0289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.140548</td>\n",
       "      <td>0.455803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(102)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.386758</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.214013</td>\n",
       "      <td>0.420750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(122)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.183935</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.307905</td>\n",
       "      <td>1.205032</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>1.075696</td>\n",
       "      <td>0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(122)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.183935</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.221647</td>\n",
       "      <td>1.205032</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>1.048452</td>\n",
       "      <td>0.228543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(167)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.056024</td>\n",
       "      <td>0.219260</td>\n",
       "      <td>1.571735</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>1.102157</td>\n",
       "      <td>0.488608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0       (166)       (102)            0.255516            0.193493  0.074835   \n",
       "1       (102)       (166)            0.193493            0.255516  0.074835   \n",
       "2       (122)       (166)            0.183935            0.255516  0.056634   \n",
       "3       (166)       (122)            0.255516            0.183935  0.056634   \n",
       "4       (166)       (167)            0.255516            0.139502  0.056024   \n",
       "\n",
       "   confidence      lift  leverage  conviction  zhangs_metric  \n",
       "0    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       "1    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       "2    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       "3    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       "4    0.219260  1.571735  0.020379    1.102157       0.488608  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate association rules for the collection of itemsets 'itemsetcollectionfinal' using the 'association_rules()' method\n",
    "# Hint: Study the documentation of the 'association_rules()' method\n",
    "# Note: Use support as the metric and a minimum threshold of 0.05\n",
    "rules2 = association_rules(itemsetcollectionfinal, metric='support', min_threshold=0.05)\n",
    "rules2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0110b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of rules in 'rules2' using the 'len()' method\n",
    "len(rules2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfc72722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(11)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.110524</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.034367</td>\n",
       "      <td>0.310948</td>\n",
       "      <td>1.216940</td>\n",
       "      <td>0.006126</td>\n",
       "      <td>1.080446</td>\n",
       "      <td>0.200417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(29)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.082766</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.030503</td>\n",
       "      <td>0.368550</td>\n",
       "      <td>1.442377</td>\n",
       "      <td>0.009355</td>\n",
       "      <td>1.179008</td>\n",
       "      <td>0.334375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(123)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.108998</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.434701</td>\n",
       "      <td>2.246605</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>1.426693</td>\n",
       "      <td>0.622764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(157)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.104931</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.035892</td>\n",
       "      <td>0.342054</td>\n",
       "      <td>1.767790</td>\n",
       "      <td>0.015589</td>\n",
       "      <td>1.225796</td>\n",
       "      <td>0.485239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.140548</td>\n",
       "      <td>0.455803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0        (11)       (166)            0.110524            0.255516  0.034367   \n",
       "1        (29)       (166)            0.082766            0.255516  0.030503   \n",
       "2       (123)       (102)            0.108998            0.193493  0.047382   \n",
       "3       (157)       (102)            0.104931            0.193493  0.035892   \n",
       "4       (166)       (102)            0.255516            0.193493  0.074835   \n",
       "\n",
       "   confidence      lift  leverage  conviction  zhangs_metric  \n",
       "0    0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       "1    0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       "2    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       "3    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       "4    0.292877  1.513634  0.025394    1.140548       0.455803  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate association rules for the collection of itemsets 'itemsetcollectionfinal' using the 'association_rules()' method\n",
    "# Hint: Study the documentation of the 'association_rules()' method\n",
    "# Note: Use confidence as the metric and a minimum threshold of 0.25\n",
    "rules3 = association_rules(itemsetcollectionfinal, metric='confidence',min_threshold=0.25)\n",
    "rules3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea228c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of rules in 'rules3' using the 'len()' method\n",
    "len(rules3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ecfbacc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(123)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.108998</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.434701</td>\n",
       "      <td>2.246605</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>1.426693</td>\n",
       "      <td>0.622764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(102)</td>\n",
       "      <td>(123)</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.108998</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.244877</td>\n",
       "      <td>2.246605</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>1.179941</td>\n",
       "      <td>0.688008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(157)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.104931</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.035892</td>\n",
       "      <td>0.342054</td>\n",
       "      <td>1.767790</td>\n",
       "      <td>0.015589</td>\n",
       "      <td>1.225796</td>\n",
       "      <td>0.485239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(102)</td>\n",
       "      <td>(157)</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.104931</td>\n",
       "      <td>0.035892</td>\n",
       "      <td>0.185497</td>\n",
       "      <td>1.767790</td>\n",
       "      <td>0.015589</td>\n",
       "      <td>1.098913</td>\n",
       "      <td>0.538522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.140548</td>\n",
       "      <td>0.455803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       "1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       "2       (157)       (102)            0.104931            0.193493  0.035892   \n",
       "3       (102)       (157)            0.193493            0.104931  0.035892   \n",
       "4       (166)       (102)            0.255516            0.193493  0.074835   \n",
       "\n",
       "   confidence      lift  leverage  conviction  zhangs_metric  \n",
       "0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       "1    0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       "2    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       "3    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       "4    0.292877  1.513634  0.025394    1.140548       0.455803  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate association rules for the collection of itemsets 'itemsetcollectionfinal' using the 'association_rules()' method\n",
    "# Hint: Study the documentation of the 'association_rules()' method\n",
    "# Note: Use lift as the metric and a minimum threshold of 1.5\n",
    "rules4 = association_rules(itemsetcollectionfinal, metric='lift', min_threshold=1.5)\n",
    "rules4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dc59c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the number of rules in 'rules4' using the 'len()' method\n",
    "len(rules4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8c853d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.213913</td>\n",
       "      <td>0.213913</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.305008</td>\n",
       "      <td>1.430134</td>\n",
       "      <td>0.018470</td>\n",
       "      <td>1.137499</td>\n",
       "      <td>0.370822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.049082</td>\n",
       "      <td>0.049082</td>\n",
       "      <td>0.009560</td>\n",
       "      <td>0.078065</td>\n",
       "      <td>0.176289</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.077759</td>\n",
       "      <td>0.120728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.056024</td>\n",
       "      <td>0.219260</td>\n",
       "      <td>1.205032</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>1.048452</td>\n",
       "      <td>0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.186324</td>\n",
       "      <td>0.186324</td>\n",
       "      <td>0.056177</td>\n",
       "      <td>0.239455</td>\n",
       "      <td>1.282182</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>1.082311</td>\n",
       "      <td>0.276595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.224504</td>\n",
       "      <td>0.224504</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.300391</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>1.121352</td>\n",
       "      <td>0.421741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.070285</td>\n",
       "      <td>0.367045</td>\n",
       "      <td>1.557210</td>\n",
       "      <td>0.024141</td>\n",
       "      <td>1.195647</td>\n",
       "      <td>0.447536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.401603</td>\n",
       "      <td>1.571735</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.244132</td>\n",
       "      <td>0.488608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       antecedent support  consequent support   support  confidence      lift  \\\n",
       "count            6.000000            6.000000  6.000000    6.000000  6.000000   \n",
       "mean             0.213913            0.213913  0.062498    0.305008  1.430134   \n",
       "std              0.049082            0.049082  0.009560    0.078065  0.176289   \n",
       "min              0.139502            0.139502  0.056024    0.219260  1.205032   \n",
       "25%              0.186324            0.186324  0.056177    0.239455  1.282182   \n",
       "50%              0.224504            0.224504  0.056634    0.300391  1.513634   \n",
       "75%              0.255516            0.255516  0.070285    0.367045  1.557210   \n",
       "max              0.255516            0.255516  0.074835    0.401603  1.571735   \n",
       "\n",
       "       leverage  conviction  zhangs_metric  \n",
       "count  6.000000    6.000000       6.000000  \n",
       "mean   0.018470    1.137499       0.370822  \n",
       "std    0.007201    0.077759       0.120728  \n",
       "min    0.009636    1.048452       0.208496  \n",
       "25%    0.012322    1.082311       0.276595  \n",
       "50%    0.020379    1.121352       0.421741  \n",
       "75%    0.024141    1.195647       0.447536  \n",
       "max    0.025394    1.244132       0.488608  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the basic summary statistics of the ruleset 'rules2' using the 'describe()' method\n",
    "rules2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40cd4900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.124223</td>\n",
       "      <td>0.234204</td>\n",
       "      <td>0.044752</td>\n",
       "      <td>0.370359</td>\n",
       "      <td>1.598017</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>1.222873</td>\n",
       "      <td>0.410815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.051034</td>\n",
       "      <td>0.031281</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.053030</td>\n",
       "      <td>0.252122</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.096605</td>\n",
       "      <td>0.107657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.071683</td>\n",
       "      <td>0.183935</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>1.205032</td>\n",
       "      <td>0.006126</td>\n",
       "      <td>1.075696</td>\n",
       "      <td>0.200417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.091459</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.032740</td>\n",
       "      <td>0.318491</td>\n",
       "      <td>1.488110</td>\n",
       "      <td>0.010642</td>\n",
       "      <td>1.174968</td>\n",
       "      <td>0.367101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.108998</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.042298</td>\n",
       "      <td>0.373714</td>\n",
       "      <td>1.571735</td>\n",
       "      <td>0.015486</td>\n",
       "      <td>1.214013</td>\n",
       "      <td>0.422732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>0.402352</td>\n",
       "      <td>1.757893</td>\n",
       "      <td>0.020718</td>\n",
       "      <td>1.245692</td>\n",
       "      <td>0.472792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.449645</td>\n",
       "      <td>2.246605</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>1.426693</td>\n",
       "      <td>0.622764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       antecedent support  consequent support    support  confidence  \\\n",
       "count           15.000000           15.000000  15.000000   15.000000   \n",
       "mean             0.124223            0.234204   0.044752    0.370359   \n",
       "std              0.051034            0.031281   0.015127    0.053030   \n",
       "min              0.071683            0.183935   0.030097    0.292877   \n",
       "25%              0.091459            0.193493   0.032740    0.318491   \n",
       "50%              0.108998            0.255516   0.042298    0.373714   \n",
       "75%              0.139502            0.255516   0.052466    0.402352   \n",
       "max              0.255516            0.255516   0.074835    0.449645   \n",
       "\n",
       "            lift   leverage  conviction  zhangs_metric  \n",
       "count  15.000000  15.000000   15.000000      15.000000  \n",
       "mean    1.598017   0.015977    1.222873       0.410815  \n",
       "std     0.252122   0.006430    0.096605       0.107657  \n",
       "min     1.205032   0.006126    1.075696       0.200417  \n",
       "25%     1.488110   0.010642    1.174968       0.367101  \n",
       "50%     1.571735   0.015486    1.214013       0.422732  \n",
       "75%     1.757893   0.020718    1.245692       0.472792  \n",
       "max     2.246605   0.026291    1.426693       0.622764  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the basic summary statistics of the ruleset 'rules3' using the 'describe()' method\n",
    "rules3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "106cb972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.171957</td>\n",
       "      <td>0.171957</td>\n",
       "      <td>0.044169</td>\n",
       "      <td>0.291777</td>\n",
       "      <td>1.712969</td>\n",
       "      <td>0.017863</td>\n",
       "      <td>1.184060</td>\n",
       "      <td>0.495999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.112740</td>\n",
       "      <td>0.207347</td>\n",
       "      <td>0.005065</td>\n",
       "      <td>0.105698</td>\n",
       "      <td>0.075303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.071683</td>\n",
       "      <td>0.071683</td>\n",
       "      <td>0.030097</td>\n",
       "      <td>0.117788</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.010767</td>\n",
       "      <td>1.047765</td>\n",
       "      <td>0.387036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.107982</td>\n",
       "      <td>0.107982</td>\n",
       "      <td>0.032232</td>\n",
       "      <td>0.189928</td>\n",
       "      <td>1.571735</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>1.101163</td>\n",
       "      <td>0.451756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.188714</td>\n",
       "      <td>0.188714</td>\n",
       "      <td>0.042857</td>\n",
       "      <td>0.302051</td>\n",
       "      <td>1.682244</td>\n",
       "      <td>0.016006</td>\n",
       "      <td>1.175435</td>\n",
       "      <td>0.481873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.048907</td>\n",
       "      <td>0.398788</td>\n",
       "      <td>1.767790</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>1.238314</td>\n",
       "      <td>0.534748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.449645</td>\n",
       "      <td>2.246605</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>1.426693</td>\n",
       "      <td>0.688008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       antecedent support  consequent support    support  confidence  \\\n",
       "count           20.000000           20.000000  20.000000   20.000000   \n",
       "mean             0.171957            0.171957   0.044169    0.291777   \n",
       "std              0.068182            0.068182   0.013420    0.112740   \n",
       "min              0.071683            0.071683   0.030097    0.117788   \n",
       "25%              0.107982            0.107982   0.032232    0.189928   \n",
       "50%              0.188714            0.188714   0.042857    0.302051   \n",
       "75%              0.255516            0.255516   0.048907    0.398788   \n",
       "max              0.255516            0.255516   0.074835    0.449645   \n",
       "\n",
       "            lift   leverage  conviction  zhangs_metric  \n",
       "count  20.000000  20.000000   20.000000      20.000000  \n",
       "mean    1.712969   0.017863    1.184060       0.495999  \n",
       "std     0.207347   0.005065    0.105698       0.075303  \n",
       "min     1.513634   0.010767    1.047765       0.387036  \n",
       "25%     1.571735   0.013916    1.101163       0.451756  \n",
       "50%     1.682244   0.016006    1.175435       0.481873  \n",
       "75%     1.767790   0.021056    1.238314       0.534748  \n",
       "max     2.246605   0.026291    1.426693       0.688008  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the basic summary statistics of the ruleset 'rules4' using the 'describe()' method\n",
    "rules4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19ce820e-ab18-4741-9ec5-473e9a34b164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "      <th>zhangs_metric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(102)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.292877</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.140548</td>\n",
       "      <td>0.455803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(102)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.193493</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.074835</td>\n",
       "      <td>0.386758</td>\n",
       "      <td>1.513634</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>1.214013</td>\n",
       "      <td>0.420750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(122)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.183935</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.307905</td>\n",
       "      <td>1.205032</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>1.075696</td>\n",
       "      <td>0.208496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(122)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.183935</td>\n",
       "      <td>0.056634</td>\n",
       "      <td>0.221647</td>\n",
       "      <td>1.205032</td>\n",
       "      <td>0.009636</td>\n",
       "      <td>1.048452</td>\n",
       "      <td>0.228543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(166)</td>\n",
       "      <td>(167)</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.056024</td>\n",
       "      <td>0.219260</td>\n",
       "      <td>1.571735</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>1.102157</td>\n",
       "      <td>0.488608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(167)</td>\n",
       "      <td>(166)</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.255516</td>\n",
       "      <td>0.056024</td>\n",
       "      <td>0.401603</td>\n",
       "      <td>1.571735</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>1.244132</td>\n",
       "      <td>0.422732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  antecedents consequents  antecedent support  consequent support   support  \\\n",
       "0       (166)       (102)            0.255516            0.193493  0.074835   \n",
       "1       (102)       (166)            0.193493            0.255516  0.074835   \n",
       "2       (122)       (166)            0.183935            0.255516  0.056634   \n",
       "3       (166)       (122)            0.255516            0.183935  0.056634   \n",
       "4       (166)       (167)            0.255516            0.139502  0.056024   \n",
       "5       (167)       (166)            0.139502            0.255516  0.056024   \n",
       "\n",
       "   confidence      lift  leverage  conviction  zhangs_metric  \n",
       "0    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       "1    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       "2    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       "3    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       "4    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       "5    0.401603  1.571735  0.020379    1.244132       0.422732  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb592b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>minimum lift</th>\n",
       "      <th>number of rules</th>\n",
       "      <th>mean support</th>\n",
       "      <th>mean confidence</th>\n",
       "      <th>mean lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.15</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.20</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.30</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.35</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.40</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.45</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.50</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.55</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.60</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.65</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.70</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.75</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.80</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.85</td>\n",
       "      <td>38</td>\n",
       "      <td>0.041292</td>\n",
       "      <td>0.259448</td>\n",
       "      <td>1.476717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.90</td>\n",
       "      <td>36</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.263125</td>\n",
       "      <td>1.508806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95</td>\n",
       "      <td>36</td>\n",
       "      <td>0.041360</td>\n",
       "      <td>0.263125</td>\n",
       "      <td>1.508806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.00</td>\n",
       "      <td>34</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.268104</td>\n",
       "      <td>1.540480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.05</td>\n",
       "      <td>34</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.268104</td>\n",
       "      <td>1.540480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.10</td>\n",
       "      <td>34</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.268104</td>\n",
       "      <td>1.540480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.15</td>\n",
       "      <td>34</td>\n",
       "      <td>0.041867</td>\n",
       "      <td>0.268104</td>\n",
       "      <td>1.540480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.20</td>\n",
       "      <td>30</td>\n",
       "      <td>0.042054</td>\n",
       "      <td>0.274517</td>\n",
       "      <td>1.586399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.25</td>\n",
       "      <td>26</td>\n",
       "      <td>0.041524</td>\n",
       "      <td>0.279251</td>\n",
       "      <td>1.644155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.30</td>\n",
       "      <td>26</td>\n",
       "      <td>0.041524</td>\n",
       "      <td>0.279251</td>\n",
       "      <td>1.644155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.35</td>\n",
       "      <td>24</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.284471</td>\n",
       "      <td>1.669555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.40</td>\n",
       "      <td>24</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.284471</td>\n",
       "      <td>1.669555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.45</td>\n",
       "      <td>22</td>\n",
       "      <td>0.043176</td>\n",
       "      <td>0.288154</td>\n",
       "      <td>1.690207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.50</td>\n",
       "      <td>20</td>\n",
       "      <td>0.044169</td>\n",
       "      <td>0.291777</td>\n",
       "      <td>1.712969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.55</td>\n",
       "      <td>18</td>\n",
       "      <td>0.040761</td>\n",
       "      <td>0.286440</td>\n",
       "      <td>1.735118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.60</td>\n",
       "      <td>12</td>\n",
       "      <td>0.039739</td>\n",
       "      <td>0.287564</td>\n",
       "      <td>1.818281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.65</td>\n",
       "      <td>10</td>\n",
       "      <td>0.039004</td>\n",
       "      <td>0.291516</td>\n",
       "      <td>1.860246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.70</td>\n",
       "      <td>10</td>\n",
       "      <td>0.039004</td>\n",
       "      <td>0.291516</td>\n",
       "      <td>1.860246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.75</td>\n",
       "      <td>10</td>\n",
       "      <td>0.039004</td>\n",
       "      <td>0.291516</td>\n",
       "      <td>1.860246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.339789</td>\n",
       "      <td>2.246605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.85</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.339789</td>\n",
       "      <td>2.246605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.339789</td>\n",
       "      <td>2.246605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.95</td>\n",
       "      <td>2</td>\n",
       "      <td>0.047382</td>\n",
       "      <td>0.339789</td>\n",
       "      <td>2.246605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    minimum lift  number of rules  mean support  mean confidence  mean lift\n",
       "0           0.00               38      0.041292         0.259448   1.476717\n",
       "1           0.05               38      0.041292         0.259448   1.476717\n",
       "2           0.10               38      0.041292         0.259448   1.476717\n",
       "3           0.15               38      0.041292         0.259448   1.476717\n",
       "4           0.20               38      0.041292         0.259448   1.476717\n",
       "5           0.25               38      0.041292         0.259448   1.476717\n",
       "6           0.30               38      0.041292         0.259448   1.476717\n",
       "7           0.35               38      0.041292         0.259448   1.476717\n",
       "8           0.40               38      0.041292         0.259448   1.476717\n",
       "9           0.45               38      0.041292         0.259448   1.476717\n",
       "10          0.50               38      0.041292         0.259448   1.476717\n",
       "11          0.55               38      0.041292         0.259448   1.476717\n",
       "12          0.60               38      0.041292         0.259448   1.476717\n",
       "13          0.65               38      0.041292         0.259448   1.476717\n",
       "14          0.70               38      0.041292         0.259448   1.476717\n",
       "15          0.75               38      0.041292         0.259448   1.476717\n",
       "16          0.80               38      0.041292         0.259448   1.476717\n",
       "17          0.85               38      0.041292         0.259448   1.476717\n",
       "18          0.90               36      0.041360         0.263125   1.508806\n",
       "19          0.95               36      0.041360         0.263125   1.508806\n",
       "20          1.00               34      0.041867         0.268104   1.540480\n",
       "21          1.05               34      0.041867         0.268104   1.540480\n",
       "22          1.10               34      0.041867         0.268104   1.540480\n",
       "23          1.15               34      0.041867         0.268104   1.540480\n",
       "24          1.20               30      0.042054         0.274517   1.586399\n",
       "25          1.25               26      0.041524         0.279251   1.644155\n",
       "26          1.30               26      0.041524         0.279251   1.644155\n",
       "27          1.35               24      0.042120         0.284471   1.669555\n",
       "28          1.40               24      0.042120         0.284471   1.669555\n",
       "29          1.45               22      0.043176         0.288154   1.690207\n",
       "30          1.50               20      0.044169         0.291777   1.712969\n",
       "31          1.55               18      0.040761         0.286440   1.735118\n",
       "32          1.60               12      0.039739         0.287564   1.818281\n",
       "33          1.65               10      0.039004         0.291516   1.860246\n",
       "34          1.70               10      0.039004         0.291516   1.860246\n",
       "35          1.75               10      0.039004         0.291516   1.860246\n",
       "36          1.80                2      0.047382         0.339789   2.246605\n",
       "37          1.85                2      0.047382         0.339789   2.246605\n",
       "38          1.90                2      0.047382         0.339789   2.246605\n",
       "39          1.95                2      0.047382         0.339789   2.246605"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain collections of rules from 'itemsetcollectionfinal' for different values of minimum lift using the 'association_rules()' method\n",
    "\n",
    "# Declare the list of minimum lift values to iterate over\n",
    "liftlist = np.arange(0, 2, 0.05)\n",
    "\n",
    "# Initiate lists to store the different collections of rules, the number of rules in each collection, and the mean support, the mean confidence, and the mean lift ratio of the rules in each collection\n",
    "rulesets = []\n",
    "nrules = []\n",
    "meansuplist = []\n",
    "meanconflist = []\n",
    "meanliftlist = []\n",
    "\n",
    "# Iterate over the list of minimum lift threshold values\n",
    "# Note: Use lift as the metric\n",
    "# Hint: The 'min_threshold' parameter for the 'association_rules()' method should be set to the current value of minimum lift threshold\n",
    "for liftvalue in liftlist:\n",
    "    tempruleset = association_rules(itemsetcollectionfinal, metric='lift', min_threshold=liftvalue)\n",
    "    \n",
    "    # Store the current collection of rules in the list 'rulesets'\n",
    "    rulesets.append(tempruleset)\n",
    "    \n",
    "    # Store the number of rules in the current collection of rules in the list 'nrules'\n",
    "    nrules.append(len(tempruleset))\n",
    "    \n",
    "    # Store the mean support of the rules in the current collection of rules in the list 'meansuplist'\n",
    "    meansuplist.append(tempruleset['support'].mean())\n",
    "    \n",
    "    # Store the mean confidence of the rules in the current collection of rules in the list 'meanconflist'\n",
    "    meanconflist.append(tempruleset['confidence'].mean())\n",
    "    \n",
    "    # Store the mean lift ratio of the rules in the current collection of rules in the list 'meanliftlist'\n",
    "    meanliftlist.append(tempruleset['lift'].mean())\n",
    "\n",
    "# Create and display a data frame that shows various attributes of the collections of rules for different values of minimum lift\n",
    "rulesetsummary = pd.DataFrame(data = {'minimum lift': liftlist,\n",
    "                                      'number of rules': nrules,\n",
    "                                      'mean support': meansuplist,\n",
    "                                      'mean confidence': meanconflist,\n",
    "                                      'mean lift': meanliftlist})\n",
    "\n",
    "# Fill missing values in the data frame with 0\n",
    "# Note: If the number of rules in a collection is 0, then the mean support, confidence, and lift are 0 for all practical purposes\n",
    "rulesetsummary.fillna(0, inplace = True)\n",
    "rulesetsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "038c325f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize how the collections of rules vary as the minimum lift threshold is increased\n",
    "plt.figure(figsize = (16, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.lineplot(data = rulesetsummary,\n",
    "             x = rulesetsummary['minimum lift'].apply(lambda x: np.round(x, 2)),\n",
    "             y = 'number of rules',\n",
    "             color = 'red')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.title('Number of Rules per Collection as a Function of Minimum Lift')\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.lineplot(data = rulesetsummary,\n",
    "             x = rulesetsummary['minimum lift'].apply(lambda x: np.round(x, 2)),\n",
    "             y = 'mean support',\n",
    "             color = 'blue')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.title('Mean Support of Rules per Collection as a Function of Minimum Lift')\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.lineplot(data = rulesetsummary,\n",
    "             x = rulesetsummary['minimum lift'].apply(lambda x: np.round(x, 2)),\n",
    "             y = 'mean confidence',\n",
    "             color = 'green')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.title('Mean Confidence of Rules per Collection as a Function of Minimum Lift')\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.lineplot(data = rulesetsummary,\n",
    "             x = rulesetsummary['minimum lift'].apply(lambda x: np.round(x, 2)),\n",
    "             y = 'mean lift',\n",
    "             color = 'orange')\n",
    "plt.grid(alpha=0.4)\n",
    "plt.title('Mean Lift Ratio of Rules per Collection as a Function of Minimum Lift')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fffd717d-49db-4757-87dc-c01127f580d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rulesets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1be001d7-bf35-415f-a2b2-a3089aa18e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rulesets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f51b7442-a2a5-4cda-b81b-89a8a367a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter = (len(rulesets[0]) >= 20) & (rulesets[0]['support'].mean() >= 0.04) & (rulesets[0]['confidence'].mean() >= 0.2) & (rulesets[0]['lift'] >= 1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb801fd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10360\\888383342.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrulesetsfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mliftvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mliftlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrulesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrulesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'support'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.04\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrulesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'confidence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrulesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lift'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1.6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mrulesetsfinal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrulesets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\codes\\upgrad_maryland_analytics\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1464\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1465\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1466\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   1467\u001b[0m             \u001b[1;34mf\"The truth value of a {type(self).__name__} is ambiguous. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1468\u001b[0m             \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1469\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# Obtain all the collections of rules that have at least 20 rules, whose mean support per collection is at least 0.04, whose mean confidence per collection is at least 0.2, and whose mean lift ratio per collection is at least 1.6\n",
    "rulesetsfinal = []\n",
    "i = -1\n",
    "for liftvalue in liftlist:\n",
    "    i = i + 1\n",
    "    if ((len(rulesets[0]) >= 20) & (rulesets[0]['support'].mean() >= 0.04) & (rulesets[0]['confidence'].mean() >= 0.2) & (rulesets[0]['lift'] >= 1.6)):\n",
    "        rulesetsfinal.append(rulesets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37691fe2-5ee5-4d70-9275-64f5abd7be78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (138)       (166)            0.174377            0.255516  0.040061   \n",
       " 31       (166)       (138)            0.255516            0.174377  0.040061   \n",
       " 32       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 33       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 34       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 35       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 36       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 37       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.229738  0.899112 -0.004495    0.966533      -0.119646  \n",
       " 31    0.156785  0.899112 -0.004495    0.979136      -0.130978  \n",
       " 32    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 33    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 34    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 35    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 36    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 37    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 31       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 32       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 33       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 34       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 35       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 31    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 32    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 33    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 34    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 35    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (138)       (102)            0.174377            0.193493  0.032740   \n",
       " 9        (102)       (138)            0.193493            0.174377  0.032740   \n",
       " 10       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 11       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 12       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 13       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 14       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 15       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 16       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 17       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 18       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 19       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 20       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 21       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 22       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 23       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 24       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 25       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 26       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 27       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 28       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 29       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 30       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 31       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 32       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 33       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 34       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 35       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.187755  0.970348 -0.001000    0.992936      -0.035692  \n",
       " 9     0.169207  0.970348 -0.001000    0.993776      -0.036507  \n",
       " 10    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 11    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 12    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 13    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 14    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 15    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 16    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 17    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 18    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 19    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 20    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 21    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 22    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 23    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 24    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 25    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 26    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 27    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 28    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 29    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 30    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 31    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 32    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 33    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 34    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 35    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 9        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 10       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 11       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 12       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 13       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 14       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 15       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 16       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 17       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 18       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 19       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 20       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 21       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 22       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 23       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 24       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 25       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 26       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 27       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 28       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 29       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 30       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 31       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 32       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 33       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 9     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 10    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 11    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 12    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 13    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 14    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 15    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 16    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 17    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 18    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 19    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 20    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 21    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 22    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 23    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 24    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 25    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 26    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 27    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 28    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 29    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 30    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 31    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 32    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 33    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 9        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 10       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 11       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 12       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 13       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 14       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 15       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 16       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 17       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 18       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 19       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 20       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 21       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 22       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 23       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 24       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 25       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 26       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 27       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 28       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 29       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 30       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 31       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 32       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 33       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 9     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 10    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 11    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 12    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 13    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 14    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 15    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 16    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 17    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 18    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 19    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 20    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 21    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 22    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 23    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 24    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 25    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 26    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 27    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 28    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 29    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 30    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 31    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 32    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 33    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 9        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 10       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 11       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 12       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 13       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 14       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 15       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 16       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 17       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 18       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 19       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 20       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 21       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 22       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 23       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 24       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 25       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 26       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 27       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 28       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 29       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 30       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 31       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 32       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 33       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 9     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 10    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 11    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 12    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 13    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 14    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 15    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 16    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 17    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 18    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 19    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 20    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 21    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 22    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 23    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 24    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 25    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 26    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 27    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 28    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 29    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 30    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 31    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 32    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 33    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (122)       (102)            0.183935            0.193493  0.042603   \n",
       " 5        (102)       (122)            0.193493            0.183935  0.042603   \n",
       " 6        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 7        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 8        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 9        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 10       (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 11       (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 12       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 13       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 14       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 15       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 16       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 17       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 18       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 19       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 20       (122)       (138)            0.183935            0.174377  0.038332   \n",
       " 21       (138)       (122)            0.174377            0.183935  0.038332   \n",
       " 22       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 23       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 24       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 25       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 26       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 27       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 28       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 29       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 30       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 31       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 32       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 33       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.231620  1.197047  0.007013    1.049620       0.201713  \n",
       " 5     0.220179  1.197047  0.007013    1.046477       0.204103  \n",
       " 6     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 7     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 8     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 9     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 10    0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 11    0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 12    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 13    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 14    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 15    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 16    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 17    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 18    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 19    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 20    0.208402  1.195124  0.006258    1.042983       0.200066  \n",
       " 21    0.219825  1.195124  0.006258    1.046003       0.197750  \n",
       " 22    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 23    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 24    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 25    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 26    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 27    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 28    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 29    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 30    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 31    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 32    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 33    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (11)       (166)            0.110524            0.255516  0.034367   \n",
       " 1        (166)        (11)            0.255516            0.110524  0.034367   \n",
       " 2         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 3        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 4        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 5        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 6        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 7        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 8        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 9        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 10       (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 11       (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 12       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 13       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 14       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 15       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 16       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 17       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 18       (122)       (166)            0.183935            0.255516  0.056634   \n",
       " 19       (166)       (122)            0.255516            0.183935  0.056634   \n",
       " 20       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 21       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 22       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 23       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 24       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 25       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 26       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 27       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 28       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 29       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.310948  1.216940  0.006126    1.080446       0.200417  \n",
       " 1     0.134501  1.216940  0.006126    1.027703       0.239450  \n",
       " 2     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 3     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 4     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 5     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 6     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 7     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 8     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 9     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 10    0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 11    0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 12    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 13    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 14    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 15    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 16    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 17    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 18    0.307905  1.205032  0.009636    1.075696       0.208496  \n",
       " 19    0.221647  1.205032  0.009636    1.048452       0.228543  \n",
       " 20    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 21    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 22    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 23    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 24    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 25    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 26    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 27    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 28    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 29    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 1        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 2        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 3        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 4        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 5        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 6        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 7        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 8        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 9        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 10       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 11       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 12       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 13       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 14       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 15       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 16       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 17       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 18       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 19       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 20       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 21       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 22       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 23       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 24       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 25       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 1     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 2     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 3     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 4     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 5     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 6     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 7     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 8     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 9     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 10    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 11    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 12    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 13    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 14    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 15    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 16    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 17    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 18    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 19    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 20    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 21    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 22    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 23    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 24    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 25    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 1        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 2        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 3        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 4        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 5        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 6        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 7        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 8        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 9        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 10       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 11       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 12       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 13       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 14       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 15       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 16       (122)       (167)            0.183935            0.139502  0.034367   \n",
       " 17       (167)       (122)            0.139502            0.183935  0.034367   \n",
       " 18       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 19       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 20       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 21       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 22       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 23       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 24       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 25       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 1     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 2     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 3     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 4     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 5     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 6     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 7     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 8     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 9     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 10    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 11    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 12    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 13    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 14    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 15    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 16    0.186844  1.339363  0.008708    1.058220       0.310486  \n",
       " 17    0.246356  1.339363  0.008708    1.082825       0.294453  \n",
       " 18    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 19    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 20    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 21    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 22    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 23    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 24    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 25    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 1        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 2        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 3        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 4        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 5        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 6        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 7        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 8        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 9        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 10       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 11       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 12       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 13       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 14       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 15       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 16       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 17       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 18       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 19       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 20       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 21       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 22       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 23       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 1     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 2     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 3     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 4     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 5     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 6     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 7     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 8     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 9     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 10    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 11    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 12    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 13    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 14    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 15    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 16    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 17    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 18    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 19    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 20    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 21    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 22    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 23    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0         (29)       (166)            0.082766            0.255516  0.030503   \n",
       " 1        (166)        (29)            0.255516            0.082766  0.030503   \n",
       " 2        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 3        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 4        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 5        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 6        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 7        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 8        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 9        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 10       (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 11       (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 12       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 13       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 14       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 15       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 16       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 17       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 18       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 19       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 20       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 21       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 22       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 23       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.368550  1.442377  0.009355    1.179008       0.334375  \n",
       " 1     0.119379  1.442377  0.009355    1.041577       0.411963  \n",
       " 2     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 3     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 4     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 5     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 6     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 7     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 8     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 9     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 10    0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 11    0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 12    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 13    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 14    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 15    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 16    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 17    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 18    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 19    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 20    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 21    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 22    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 23    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 5        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 6        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 7        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 8        (105)       (166)            0.088968            0.255516  0.033249   \n",
       " 9        (166)       (105)            0.255516            0.088968  0.033249   \n",
       " 10       (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 11       (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 12       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 13       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 14       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 15       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 16       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 17       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 18       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 19       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 20       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 21       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 5     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 6     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 7     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 8     0.373714  1.462587  0.010516    1.188729       0.347166  \n",
       " 9     0.130123  1.462587  0.010516    1.047312       0.424831  \n",
       " 10    0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 11    0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 12    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 13    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 14    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 15    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 16    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 17    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 18    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 19    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 20    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 21    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4        (166)       (102)            0.255516            0.193493  0.074835   \n",
       " 5        (102)       (166)            0.193493            0.255516  0.074835   \n",
       " 6        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 7        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 8        (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 9        (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 10       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 11       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 12       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 13       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 14       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 15       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 16       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 17       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 18       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 19       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4     0.292877  1.513634  0.025394    1.140548       0.455803  \n",
       " 5     0.386758  1.513634  0.025394    1.214013       0.420750  \n",
       " 6     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 7     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 8     0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 9     0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 10    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 11    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 12    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 13    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 14    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 15    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 16    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 17    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 18    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 19    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 5        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 6        (109)       (166)            0.075648            0.255516  0.030097   \n",
       " 7        (166)       (109)            0.255516            0.075648  0.030097   \n",
       " 8        (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 9        (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 10       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 11       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 12       (157)       (166)            0.104931            0.255516  0.042298   \n",
       " 13       (166)       (157)            0.255516            0.104931  0.042298   \n",
       " 14       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 15       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " 16       (166)       (167)            0.255516            0.139502  0.056024   \n",
       " 17       (167)       (166)            0.139502            0.255516  0.056024   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 5     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 6     0.397849  1.557043  0.010767    1.236375       0.387036  \n",
       " 7     0.117788  1.557043  0.010767    1.047765       0.480544  \n",
       " 8     0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 9     0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 10    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 11    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 12    0.403101  1.577595  0.015486    1.247252       0.409045  \n",
       " 13    0.165539  1.577595  0.015486    1.072631       0.491782  \n",
       " 14    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 15    0.126144  1.759754  0.013916    1.062323       0.579917  \n",
       " 16    0.219260  1.571735  0.020379    1.102157       0.488608  \n",
       " 17    0.401603  1.571735  0.020379    1.244132       0.422732  ,\n",
       "    antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0        (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1        (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2        (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3        (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4        (102)       (167)            0.193493            0.139502  0.043416   \n",
       " 5        (167)       (102)            0.139502            0.193493  0.043416   \n",
       " 6        (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 7        (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 8        (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 9        (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 10       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 11       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " \n",
       "     confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0     0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1     0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2     0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3     0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4     0.224383  1.608457  0.016424    1.109436       0.469042  \n",
       " 5     0.311224  1.608457  0.016424    1.170929       0.439613  \n",
       " 6     0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 7     0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 8     0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 9     0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 10    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 11    0.126144  1.759754  0.013916    1.062323       0.579917  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 5       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 6       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 7       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 8       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 9       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 5    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 6    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 7    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 8    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 9    0.126144  1.759754  0.013916    1.062323       0.579917  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 5       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 6       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 7       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 8       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 9       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 5    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 6    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 7    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 8    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 9    0.126144  1.759754  0.013916    1.062323       0.579917  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " 2       (157)       (102)            0.104931            0.193493  0.035892   \n",
       " 3       (102)       (157)            0.193493            0.104931  0.035892   \n",
       " 4       (122)       (130)            0.183935            0.093950  0.030605   \n",
       " 5       (130)       (122)            0.093950            0.183935  0.030605   \n",
       " 6       (123)       (166)            0.108998            0.255516  0.048907   \n",
       " 7       (166)       (123)            0.255516            0.108998  0.048907   \n",
       " 8       (162)       (166)            0.071683            0.255516  0.032232   \n",
       " 9       (166)       (162)            0.255516            0.071683  0.032232   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  \n",
       " 2    0.342054  1.767790  0.015589    1.225796       0.485239  \n",
       " 3    0.185497  1.767790  0.015589    1.098913       0.538522  \n",
       " 4    0.166390  1.771048  0.013324    1.086899       0.533490  \n",
       " 5    0.325758  1.771048  0.013324    1.210344       0.480506  \n",
       " 6    0.448694  1.756031  0.021056    1.350401       0.483202  \n",
       " 7    0.191405  1.756031  0.021056    1.101913       0.578298  \n",
       " 8    0.449645  1.759754  0.013916    1.352735       0.465077  \n",
       " 9    0.126144  1.759754  0.013916    1.062323       0.579917  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  ,\n",
       "   antecedents consequents  antecedent support  consequent support   support  \\\n",
       " 0       (123)       (102)            0.108998            0.193493  0.047382   \n",
       " 1       (102)       (123)            0.193493            0.108998  0.047382   \n",
       " \n",
       "    confidence      lift  leverage  conviction  zhangs_metric  \n",
       " 0    0.434701  2.246605  0.026291    1.426693       0.622764  \n",
       " 1    0.244877  2.246605  0.026291    1.179941       0.688008  ]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rulesetsfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6801da32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of collections of rules that satisfy these requirements\n",
    "len(rulesetsfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c78b8f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Select and view the first collection of rules that has the maximum number of rules\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Hint: The number of rules per collection decreases monotonically as the minimum lift threshold is increased\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m rulesetfinal \u001b[38;5;241m=\u001b[39m \u001b[43mrulesetsfinal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m rulesetfinal\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Select and view the first collection of rules that has the maximum number of rules\n",
    "# Hint: The number of rules per collection decreases monotonically as the minimum lift threshold is increased\n",
    "rulesetfinal = rulesetsfinal[0]\n",
    "rulesetfinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30251cb2",
   "metadata": {},
   "source": [
    "# Task 4 - Identify Strong Rules\n",
    "For this task, you will perform the following steps:\n",
    "- Sort the selected collection of rules by support, confidence and lift and see which rules come up at the top\n",
    "- Visualize the strengths of the rules using a scatter plot\n",
    "- Select rules with high support, confidence and lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the collection 'rulesetfinal' by 'support' using the 'sort_values()' method\n",
    "# Note: Set 'ascending' to 'False'\n",
    "rulesetfinal.########## CODE HERE ##########.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b903ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the collection 'rulesetfinal' by 'confidence' using the 'sort_values()' method\n",
    "# Note: Set 'ascending' to 'False'\n",
    "rulesetfinal.########## CODE HERE ##########.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b893ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the collection 'rulesetfinal' by 'lift' using the 'sort_values()' method\n",
    "# Note: Set 'ascending' to 'False'\n",
    "rulesetfinal.########## CODE HERE ##########.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067782d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the strengths of the rules in the collection 'rulesetfinal'\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.scatterplot(data = rulesetfinal, x = 'support', y = 'confidence', hue = 'lift', palette = 'Oranges')\n",
    "plt.legend(bbox_to_anchor = (1.15, 1), title = 'Lift')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target the rules that have high values of both support and confidence\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.scatterplot(data = rulesetfinal, x = 'support', y = 'confidence', hue = 'lift', palette = 'Oranges')\n",
    "plt.vlines(x = 0.04, ymin = 0.4, ymax = 0.5, colors = 'red', linestyles = '--')\n",
    "plt.hlines(y = 0.4, xmin = 0.04, xmax = 0.06, colors = 'red', linestyles = '--')\n",
    "plt.vlines(x = 0.06, ymin = 0.4, ymax = 0.5, colors = 'red', linestyles = '--')\n",
    "plt.hlines(y = 0.5, xmin = 0.04, xmax = 0.06, colors = 'red', linestyles = '--')\n",
    "plt.legend(bbox_to_anchor = (1.15, 1), title = 'Lift')\n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rules that have a support of more than 0.04 and a confidence of more than 0.4\n",
    "rulesetfinal[########## CODE HERE ##########]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13561fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rules that have a support of more than 0.04, a confidence of more than 0.4 and a lift ratio of more than 1.75\n",
    "rulesetfinal[########## CODE HERE ##########]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract rules which contain the antecedent 'root vegetables'\n",
    "rulesetfinal[########## CODE HERE ##########]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1011257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
