{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0891ba",
   "metadata": {},
   "source": [
    "# Task 1 - Data Preparation\n",
    "For this task, you will perform the following steps:\n",
    "- Load all the necessary packages for this exercise\n",
    "- Load the data\n",
    "- Split the data into input features and the target variable\n",
    "- Split the data into training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94f8e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'numpy' and 'pandas' for working with numbers and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import 'matplotlib.pyplot' and 'seaborn' for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import 'train_test_split' from 'sklearn'\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import 'LogisticRegression' from 'sklearn'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import various suitable classification performance metrics from 'sklearn'\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92a6bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Income</th>\n",
       "      <th>Age</th>\n",
       "      <th>Preference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31779</td>\n",
       "      <td>46</td>\n",
       "      <td>Sweetened</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>32739</td>\n",
       "      <td>50</td>\n",
       "      <td>Sweetened</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Gender  Married  Income  Age Preference\n",
       "0       0        0   31779   46  Sweetened\n",
       "1       1        1   32739   50  Sweetened"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and take a look at it\n",
    "# Note: Make sure that the data is in the same folder as the Jupyter notebook or specify the address correctly\n",
    "teadata = pd.read_csv('MLTeaInc.csv')\n",
    "teadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded9473-c8fa-45f8-9465-6a2ddf5e24d9",
   "metadata": {},
   "source": [
    "## MApping the Preference to numeric value.\n",
    "\n",
    "I personally change the string data that we want to classify to the number/boolean of 0 and 1. \\\n",
    "So the mappings are as follows:\n",
    "\n",
    "```python\n",
    "preference_mapping = {\n",
    "    \"Sweetened\": 0,\n",
    "    \"Unsweetened\": 1,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7c597dd6-f774-486b-b359-892256311b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"Sweetened\": 0, \"Unsweetened\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aba73566-44b8-4d03-856d-24f6b2fd9d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "teadata['Preference'] = teadata['Preference'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "88a17268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into input features and the target variable\n",
    "# Note: The target variable here is the 'Preference' feature\n",
    "# Note: The class 'Unsweetened' is the class of interest or the positive class in this exercise\n",
    "X = teadata.drop('Preference', axis = 1)\n",
    "y = teadata['Preference']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f344f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation datasets\n",
    "# Note: Use 'test_size = 0.2'\n",
    "# Note: Use 'random_state = 0'\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b59b5e",
   "metadata": {},
   "source": [
    "# Task 2 - Logistic Regression Model\n",
    "For this task, you will perform the following steps:\n",
    "- Build a logistic regression model on the training data with no regularization penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f0f30f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a logistic regression model for the data without any regularization\n",
    "# Hint: You need to set the 'penalty' parameter to 'none'\n",
    "# Note: Use 'max_iter = 1000' and 'random_state = 0' for the model\n",
    "logreg_model = LogisticRegression(penalty=None, max_iter=1000, random_state=0)\n",
    "logreg_model = logreg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1875bcf2",
   "metadata": {},
   "source": [
    "# Task 3 - Analyze the Model\n",
    "For this task, you will perform the following steps:\n",
    "- Compute the ROC AUC score for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "57c3221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC for validation data = 0.9700000000000001\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the ROC AUC score for 'logreg_model' on the validation data\n",
    "# Hint: You will need to obtain the predicted probabilities for 'logreg_model' using the 'predict_proba()' method\n",
    "\n",
    "# Compute predicted probabilities for 'logreg_model' on the validation data\n",
    "# Hint: Study the documentation of the 'predict_proba()' method\n",
    "# Hint: The second column of the matrix returned by the 'predict_proba()' method contains the positive class probabilities\n",
    "val_probabilities = logreg_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Use the predicted probabilities to compute the ROC AUC score\n",
    "# Hint: Study the documentation of the 'roc_auc_score()' method\n",
    "val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "print('ROC AUC for validation data = {}'.format(val_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7117d4a3",
   "metadata": {},
   "source": [
    "# Task 4 - Misclassification Cost\n",
    "For this task, you will perform the following steps:\n",
    "- Compute the baseline misclassification cost\n",
    "- Observe how the misclassification cost varies as the cut-off for classification is increased\n",
    "- Compute the potentially best misclassification cost of the model on the validation data using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "82e94ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted probabilities for the class 'Unsweetened' records in the training and the validation data using 'logreg_model'\n",
    "# Hint: Study the documentation of the 'predict_proba()' method\n",
    "# Hint: The second column of the matrix returned by the 'predict_proba()' method contains the positive class probabilities\n",
    "train_probabilities = logreg_model.predict_proba(X_train)[:, 1]\n",
    "val_probabilities = logreg_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Obtain predicted class labels for the training and the validation data using 'logreg_model'\n",
    "# Hint: Study the documentation of the 'predict()' method\n",
    "train_y_pred = logreg_model.predict(X_train)\n",
    "val_y_pred = logreg_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f23ac93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the cost of false positives and false negatives\n",
    "# Note: The model will be used to send invitations to customers who potentially prefer unsweetened tea for a reveal party\n",
    "# Note: The cost of false positives and false negatives are provided to you for this exercise\n",
    "# Note: Labeling a customer who prefers sweetened tea as 'Unsweetened' leads to just sending a wrong invitation, so its cost is less\n",
    "# Note: Labeling a customer who prefers unsweetened tea as 'Sweetened' leads to losing out on a potential customer, so its cost is more\n",
    "fp_cost = 300\n",
    "fn_cost = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2fcfb499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The misclassification cost of classifying everyone in the training data as preferring unsweetened tea = 40000 dollars\n",
      "The misclassification cost of classifying everyone in the validation data as preferring unsweetened tea = 10000 dollars\n"
     ]
    }
   ],
   "source": [
    "# Calculate the misclassification cost for the naive model on the training and the validation data\n",
    "# Hint: The naive model in this case would classify everyone as preferring unsweetened tea or class 'Unsweetened'\n",
    "# Hint: That means all class 'Unsweetened' people are classified as class 'Unsweetened' accurately\n",
    "# Hint: The class 'Sweetened' people are the only ones that contribute to the misclassification\n",
    "# Hint: So, you need to count how many 'y_train' and 'y_val' values are actually class 'Sweetened', since all of them are labeled as class 'Unsweetened'\n",
    "# Hint: Recall the formula for the misclassification cost\n",
    "train_mc_cost_0 = y_train[y_train == 0].count() *  fn_cost\n",
    "val_mc_cost_0 = y_val[y_val == 0].count() *  fn_cost\n",
    "\n",
    "print('The misclassification cost of classifying everyone in the training data as preferring unsweetened tea = {} dollars'.format(train_mc_cost_0))\n",
    "print('The misclassification cost of classifying everyone in the validation data as preferring unsweetened tea = {} dollars'.format(val_mc_cost_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7864369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of misclassification costs for various cut-off values for 'logreg_model' on the training and the validation data\n",
    "# Hint: If the 'train_probabilities' or the 'val_probabilities' value is greater than the cut-off, then the label is class 'Unsweetened', else it's class 'Sweetened'\n",
    "# Hint: Try using the 'np.where()' method to obtain the predictions for each cut-off value\n",
    "train_mc_cost = []\n",
    "val_mc_cost = []\n",
    "cutoffs = np.arange(0, 1, 0.01)\n",
    "for cutoff in cutoffs:\n",
    "    train_curr_cf = confusion_matrix(y_train, train_y_pred)\n",
    "    train_fp_count = train_curr_cf[0, 1]\n",
    "    train_fn_count = train_curr_cf[1, 0]\n",
    "    train_mc_cost_current = train_fp_count * fp_cost + train_fn_count * fn_cost\n",
    "    train_mc_cost.append(train_mc_cost_current)\n",
    "    \n",
    "    curr_preds = np.where(val_y_pred > cutoff, 1, 0)\n",
    "    val_curr_cf = confusion_matrix(y_val, curr_preds)\n",
    "    val_fp_count = val_curr_cf[0, 1]\n",
    "    val_fn_count = val_curr_cf[1, 0]\n",
    "    val_mc_cost_current = val_fp_count * fp_cost + val_fn_count * fn_cost\n",
    "    val_mc_cost.append(val_mc_cost_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "520c1d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Misclassification Cost on the training is 5900.00 at Cut-off 0.000\n",
      "Best misclassification cost on the validation data for the model = 3000 dollars\n"
     ]
    }
   ],
   "source": [
    "# Use the cut-off associated with the minimum misclassification cost on the training data to get the best cost for the validation data\n",
    "# Hint: The expected best misclassification cost on the validation data is the cost associated with the best cut-off from the training data\n",
    "# Note: The best cut-off for the training data is the cut-off associated with the minimum misclassification cost for the training data\n",
    "\n",
    "best_cost = min(train_mc_cost)\n",
    "\n",
    "best_cutoff = cutoffs[train_mc_cost.index(best_cost)]\n",
    "best_valcost= val_mc_cost[train_mc_cost.index(best_cost)]\n",
    "print('Best Misclassification Cost on the training is %.2f at Cut-off %.3f' % (best_cost, best_cutoff));\n",
    "print('Best misclassification cost on the validation data for the model = {} dollars'.format(best_valcost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b222404-baa2-4f29-9709-dd6e3f5f671a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
