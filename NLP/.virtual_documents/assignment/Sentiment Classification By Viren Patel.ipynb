








!pip install tqdm nltk lightgbm





import warnings
import string
import re
import socket
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
import typing
import lightgbm as lgbm

from tqdm import trange
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.probability import FreqDist
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import LabelEncoder
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

from collections.abc import Callable

nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download("wordnet")


warnings.filterwarnings('ignore')
sns.set_style('darkgrid')








reviews_df = pd.read_csv('reviewsdata.csv')








plt.pie(reviews_df['Label'].value_counts(), labels=reviews_df['Label'].unique().tolist(), autopct='%1.1f%%')
plt.show()











def find_patterns(re_pattern: str, text_list: typing.List[str]):
    """
    Args:
    ----------
    re_pattern [type: str]: Regular expression pattern to be used on search.
    text_list [type: list]: List with text strings.

    Returns:
    ---------
    positions_dict: python dictionary with key-value pairs as below:
        text_idx: [(start_pattern1, end_pattern1), (start_pattern1, end_pattern2), ..., (start_pattern_n, end_pattern_n)]
    """

    # Compiling the ReGex passed as arg.
    pattern = re.compile(re_pattern)
    positions_dict = {}
    i = 0
    for c in text_list:
        match_list = []
        iterator = pattern.finditer(c)
        for match in iterator:
            match_list.append(match.span())

        control_key = f'Text idx {i}'
        if len(match_list) == 0:
            continue
        positions_dict[control_key] = match_list
        i += 1

    return positions_dict


def print_step_result(text_list_before: typing.List[str], text_list_after: typing.List[str], idx_list: typing.List[int]):
    """
    Args:
    -------

    text_list_before [type: List[str]]: list object with text content before transformation.
    text_list_after [type: List[str]]: list object with text content after transformation.
    idx_list [type: List[str]]: list object with indexes to be printed.
    """

    # iterating over string examples.
    i = 1
    for idx in idx_list:
        print(f"--- Text {i} --- \n")
        print(f"Before: \n{text_list_before[idx]}\n")
        print(f"After: \n{text_list_after[idx]}\n")
        i += 1








def re_breakline(text_list: typing.List[str]) -> typing.List[str]:
    """
    Args:
    ---------
    text_list [type: List[str]]: List object with text content to be cleaned.

    Returns:
    ---------
    [type: List[str]]: Transformed / Cleaned text context.
    """

    return [re.sub('[\n\r]', ' ', r) for r in text_list]


# creating list of reviews.
reviews = reviews_df['Review'].values.tolist()

# Applying the ReGex.
reviews_breakline = re_breakline(reviews)
reviews_df['Review'] = reviews_breakline

# verifying results
print_step_result(reviews, reviews_breakline, idx_list=[6])








def re_hyperlinks(text_list: typing.List[str]) -> typing.List[str]:
    """
    Args:
    ----------
    text_list: list object with text content to be prepared [type: list]

    Returns:
    ----------
    [type: List[str]]: Transformed / Cleaned text context.
    """

    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    return [re.sub(pattern, ' link ', r) for r in text_list]


# Applying ReGex.
reviews_hyperlinks = re_hyperlinks(reviews_breakline)
reviews_df['Review'] = reviews_hyperlinks

# Verifying results
print_step_result(reviews_breakline, reviews_hyperlinks, idx_list=[15, 4])








negations = r"\b(?:never|no|nothing|nowhere|noone|none|not|havent|hasn't|hadn't|can't|couldn't|shouldn't|won't|wouldn't|don't|doesn't|didn't|isn't|aren't|ain't)\b"
negation_pattern = re.compile(f"({negations})\s+([a-zA-Z]+)")

def re_negation(text_list: typing.List[str], pattern) -> typing.List[str]:
    """
    Args:
    -------
    text_series [text: List[str]]: List object with text content to be transformed.

    Returns:
    ----------
    [type: List[str]]: Transformed / Cleaned text context.
    """
    return [pattern.sub(lambda match: f"{match.group(1)}_{match.group(2)}", review) for review in text_list]


# Applying ReGex.
reviews_negation = re_negation(reviews_breakline, negation_pattern)
reviews_df['Review'] = reviews_negation

# Verifying results
print_step_result(reviews_breakline, reviews_negation, idx_list=[4, 8])








def re_special_chars(text_list):
    """
    Args:
    ----------
    text_series: list object with text content to be prepared [type: list]

    Returns:
    ----------
    [type: List[str]]: Transformed / Cleaned text context.
    """
    
    # Applying regex
    return [re.sub('\W', ' ', r) for r in text_list]


# Applying RegEx
reviews_special_chars = re_special_chars(reviews_negation)
reviews_df['Review'] = reviews_special_chars

# Verifying results
print_step_result(reviews_negation, reviews_special_chars, idx_list=[4, 8])








def re_whitespaces(text_list):
    """
    Args:
    ----------
    text_series: list object with text content to be prepared [type: list]
    """
    
    # Applying regex
    white_spaces = [re.sub('\s+', ' ', r) for r in text_list]
    white_spaces_end = [re.sub('[ \t]+$', '', r) for r in white_spaces]
    return white_spaces_end


# Applying RegEx
reviews_whitespaces = re_whitespaces(reviews_special_chars)
reviews_df['Review'] = reviews_whitespaces

# Verifying results
print_step_result(reviews_special_chars, reviews_whitespaces, idx_list=[4, 8])








def remove_stopwords(text_list: typing.List[str]) -> typing.List[str]:
    """
    Args:
    ----------
    text_series: list object with text content to be prepared [type: list]
    """
    return [" ".join([word for word in review.split() if word not in stopwords.words('english')]) for review in text_list]


reviews_stopwords = remove_stopwords(reviews_whitespaces)
reviews_df['Review'] = reviews_stopwords

# Verifying results
print_step_result(reviews_whitespaces, reviews_stopwords, idx_list=[4, 8])














print(f"The dataset consists of {reviews_df.shape[0]} rows, and {reviews_df.shape[1]} columns")








reviews_df.sample(5)








reviews_df.info()











(reviews_df.isnull().sum() / reviews_df.shape[0]) * 100














eda_df = reviews_df.copy()








length = len(eda_df.sample(1)['Review'].to_list()[0])
print(f"Length of a sample review: {length}")


eda_df['Length'] = eda_df['Review'].str.len()


eda_df.sample(10)








word_count = eda_df['Review'][0].split()
print(f'Word count in a sample review: {len(word_count)}')


def get_word_count(review: str) -> int:
    review_list = review.split()
    return len(review_list)


eda_df['Word_count'] = eda_df['Review'].apply(get_word_count)


eda_df.sample(10)








eda_df['mean_word_length'] = eda_df['Review'].map(lambda rev: np.mean([len(word) for word in rev.split()]))


eda_df.sample(10)








np.mean([len(sent) for sent in tokenize.sent_tokenize(eda_df['Review'][0])])


eda_df['mean_sent_length'] = eda_df['Review'].map(lambda rev: np.mean([len(sent) for sent in tokenize.sent_tokenize(rev)]))


eda_df.sample(10)








fig, ax = plt.subplots(3, 2, figsize=(10, 10), layout='tight')
fig.suptitle("Central tendancy of the Reviews.", fontsize=18)

len_box_plot = sns.boxplot(y=eda_df['Length'], hue=eda_df['Label'], ax=ax[0][0])
ax[0][0].set_title("Range of Word Length.")


len_line_plot = sns.kdeplot(x=eda_df['Length'], hue=eda_df['Label'], ax=ax[0][1])
ax[0][1].set_title("Distribution of word length.")


mean_word_count_box_plot = sns.boxplot(y=eda_df['mean_word_length'], hue=eda_df['Label'], ax=ax[1][0])
ax[1][0].set_title("Range of Mean Word Length.")


mean_word_count_line_plot = sns.kdeplot(x=eda_df['mean_word_length'], hue=eda_df['Label'], ax=ax[1][1])
ax[1][1].set_title("Distribution of Mean Word Length.")


mean_sent_count_box_plot = sns.boxplot(y=eda_df['mean_sent_length'], hue=eda_df['Label'], ax=ax[2][0])
ax[2][0].set_title("Range of Mean Sentence Length.")


mean_sent_count_line_plot = sns.kdeplot(x=eda_df['mean_sent_length'], hue=eda_df['Label'], ax=ax[2][1])
ax[2][1].set_title("Distribution of Mean Sentence Length.")



plt.show()








term_freq_df = reviews_df.copy()








term_freq_df['Review_lists'] = term_freq_df['Review'].apply(lambda x: x.split())


term_freq_df.sample(10)


corpus = []
for i in trange(term_freq_df.shape[0], ncols=150, nrows=10, colour='orange', smoothing=0.8):
    corpus += term_freq_df['Review_lists'][i]
len(corpus)








most_common = Counter(corpus).most_common(10)
most_common


words = []
freq = []

for word, count in most_common:
    words.append(word)
    freq.append(count)


sns.barplot(x=freq, y=words)
plt.title('Top 10 Most Frequently occuring words.')
plt.show()


term_freq_df['Label'].value_counts()


good_reviews_corpus = []
bad_reviews_corpus = []
for i in trange(term_freq_df.shape[0], ncols=150, nrows=10, colour='orange', smoothing=0.8):
    if term_freq_df['Label'][i] == 'pos':
        good_reviews_corpus += term_freq_df['Review_lists'][i]
    else:
        bad_reviews_corpus += term_freq_df['Review_lists'][i]
print(len(good_reviews_corpus))
print(len(bad_reviews_corpus))


good_most_common = Counter(good_reviews_corpus).most_common(10)
bad_most_common = Counter(bad_reviews_corpus).most_common(10)


gr_plot_data = pd.DataFrame([{'word':k, 'freq': v} for k,v in good_most_common])
br_plot_data = pd.DataFrame([{'word':k, 'freq': v} for k,v in bad_most_common])


fig, ax = plt.subplots(1, 2, figsize=(15, 5))

good_reviews_plot = sns.barplot(data=gr_plot_data, x='freq', y='word', ax=ax[0])
ax[0].set_title('Good Reviews Most Common Words.')

good_reviews_plot = sns.barplot(data=br_plot_data, x='freq', y='word', ax=ax[1])
ax[1].set_title('Bad Reviews Most Common Words.')

plt.show()














cv = CountVectorizer(ngram_range=(2, 2))
bigrams = cv.fit_transform(term_freq_df['Review'])


count_values = bigrams.toarray().sum(axis=0)
ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse=True))
ngram_freq.columns = ['frequency', 'ngram']








cv = CountVectorizer(ngram_range=(3, 3))
trigrams = cv.fit_transform(term_freq_df['Review'])
count_values = trigrams.toarray().sum(axis=0)
ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse=True))
ngram_freq.columns = ['frequency', 'ngram']








def get_n_grams(dataset: pd.Series | typing.List[str], ngram_range: typing.Tuple[int, int]=(2, 2)) -> pd.DataFrame:
    cv = CountVectorizer(ngram_range=ngram_range)
    trigrams = cv.fit_transform(dataset)
    count_values = trigrams.toarray().sum(axis=0)
    ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse=True))
    ngram_freq.columns = ['frequency', 'ngram']
    return ngram_freq








positive_reviews_bi_grams = get_n_grams(term_freq_df[term_freq_df['Label'] == 'pos']['Review'])
negative_reviews_bi_grams = get_n_grams(term_freq_df[term_freq_df['Label'] == 'neg']['Review'])


positive_reviews_tri_grams = get_n_grams(term_freq_df[term_freq_df['Label'] == 'pos']['Review'], (3, 3))
negative_reviews_tri_grams = get_n_grams(term_freq_df[term_freq_df['Label'] == 'neg']['Review'], (3, 3))


vfig, ax = plt.subplots(2, 2, figsize=(10, 10), layout='tight')
fig.suptitle("Ngrams of Reviews.", fontsize=18)

len_box_plot = sns.barplot(x=positive_reviews_bi_grams['frequency'][:10], y=positive_reviews_bi_grams['ngram'][:10],  ax=ax[0][0])
ax[0][0].set_title("Top 10 Bigrams of Positive reviews.")


len_line_plot = sns.barplot(x=negative_reviews_bi_grams['frequency'][:10], y=negative_reviews_bi_grams['ngram'][:10],  ax=ax[0][1])
ax[0][1].set_title("Top 10 Bigrams of Negative reviews.")

mean_word_count_box_plot = sns.barplot(x=positive_reviews_tri_grams['frequency'][:10], y=positive_reviews_tri_grams['ngram'][:10],  ax=ax[1][0])
ax[1][0].set_title("Top 10 Triigrams of Positive reviews.")


mean_word_count_line_plot = sns.barplot(x=negative_reviews_tri_grams['frequency'][:10], y=negative_reviews_tri_grams['ngram'][:10],  ax=ax[1][1])
ax[1][1].set_title("Top 10 Trigrams of Negative reviews.")

plt.show()








def stemming_process(text, stemmer=PorterStemmer()):
    """
    Args:
    ------
    text [type: List[str]]: The list objects whose stopwords will be removed.
    lemmatizer [type: class, default: WordNetLemmatizer()]: Type of lemmatizer to be applied.

    Returns:
    ------
    [type: List[str]]: list of lematized text.
    """
    return [stemmer.stem(c) for c in text.split()]


reviews_df['Review'] = [' '.join(stemming_process(review)) for review in reviews_df['Review']]


reviews_df['Review']








def lematizing_process(text, lemmatizer=WordNetLemmatizer()):
    """
    Args:
    ------
    text [type: List[str]]: The list objects whose stopwords will be removed.
    lemmatizer [type: class, default: WordNetLemmatizer()]: Type of lemmatizer to be applied.

    Returns:
    ------
    [type: List[str]]: list of lematized text.
    """
    return [lemmatizer.lemmatize(c) for c in text.split()]


reviews_df['Review'] = [' '.join(lematizing_process(review)) for review in reviews_df['Review']]


reviews_df['Review']

















X = reviews_df['Review']
y = reviews_df['Label']








X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)








label_encoder = LabelEncoder().fit(y)


y_train_encoded = label_encoder.fit_transform(y_train)


y_test_encoded = label_encoder.transform(y_test)














tfid_vect = TfidfVectorizer(use_idf=True, ngram_range=(1, 2), max_features=500, min_df=2)








X_train_vectors = tfid_vect.fit_transform(X_train)








X_test_vectors = tfid_vect.transform(X_test)








print("X train tfid vectors: ", X_train_vectors.shape)
print("X test tfid vectors: ", X_test_vectors.shape)




















# Class for regex application
class ApplyRegex(BaseEstimator, TransformerMixin):

    def __init__(self, regex_transformers: dict[str, Callable[[str], str]]):
        self.regex_transformers = regex_transformers

    
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        for regex_name, regex_function in self.regex_transformers.items():
            X = regex_function(X)

        return X








# Defining a function to remove the stopwords and to lower the comments
def stopwords_removal(text, cached_stopwords=stopwords.words('english')):
    """
    Args:
    ----------
    text: list object where the stopwords will be removed [type: list]
    cached_stopwords: stopwords to be applied on the process [type: list, default: stopwords.words('portuguese')]
    """
    
    return [c.lower() for c in text.split() if c.lower() not in cached_stopwords]


class StopWordsRemoval(BaseEstimator, TransformerMixin):
    def __init__(self, text_stopwords):
        self.text_stopwords = text_stopwords

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return [' '.join(stopwords_removal(review, self.text_stopwords)) for review in X]








class StemmingProcess(BaseEstimator, TransformerMixin):
    def __init__(self, stemmer):
        self.stemmer = stemmer

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return [' '.join(stemming_process(review, self.stemmer)) for review in X]








class TextFeatureExtraction(BaseEstimator, TransformerMixin):
    def __init__(self, vectorizer: TfidfVectorizer()):
        self.vectorizer = vectorizer

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        return self.vectorizer.fit_transform(X).toarray()














regex_transformers = {
    'break_line': re_breakline,
    'hyperlinks': re_hyperlinks,
    'negations': re_negation,
    'special_chars': re_special_chars,
    'whitespaces': re_whitespaces
}








vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1, 2), max_features=500)





text_pipeline = Pipeline([
    ('regex', ApplyRegex(regex_transformers)),
    ('stopwords', StopWordsRemoval(stopwords.words('english'))),
    ('stemming', StemmingProcess(PorterStemmer())),
    ('text_features', TextFeatureExtraction(vectorizer)),
])


text_pipeline.fit(reviews_df['Review'])











log_reg_model = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)


log_reg_tfidf = log_reg_model.fit(X_train_vectors, y_train_encoded)


print(log_reg_tfidf)








y_pred = log_reg_model.predict(X_test_vectors)
print(y_pred)


accuracy_score(y_test_encoded, y_pred)



