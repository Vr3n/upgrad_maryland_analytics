{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1313368e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "1313368e",
    "papermill": {
     "duration": 1.577006,
     "end_time": "2021-11-12T22:01:01.585954",
     "exception": false,
     "start_time": "2021-11-12T22:01:00.008948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\DELL\n",
      "[nltk_data]     5590\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\DELL\n",
      "[nltk_data]     5590\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G_bmpdKa-JV7",
   "metadata": {
    "id": "G_bmpdKa-JV7"
   },
   "source": [
    "# Graded Question part - I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9465120d",
   "metadata": {},
   "source": [
    "#### Question 1\n",
    "Which of the following words is not a stopword in the English language according to NLTKâ€™s list of English stopwords?\n",
    "- I\n",
    "- Has\n",
    "- Yes\n",
    "- Was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e88e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb43adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2929be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['i','has','yes','was']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "659774bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word not in stopword_list:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sYXYrDI-a18n",
   "metadata": {
    "id": "sYXYrDI-a18n"
   },
   "source": [
    "**Question 3:**\n",
    "\n",
    "To answer the following question, use the text corpus given below. \\\\\n",
    "\n",
    "**corpus =** [ \"john told you to come market\", \"can you please come here\", \"can you please come to meet sofia\", \"i suggest you do not come to party\", \"please come with harry\", \"may i come in\", \"dreams come true\", \"why don't you come to audition\", \"may i come help you\" ] \\\\\n",
    "First, remove all the stop words from the corpus. Now, extract all the features from the corpus using the TF IDF model. Which element is in the (1,11) location? \n",
    " \\\\\n",
    "\n",
    "***Hint:***\n",
    "When you print the vectorizer, each non-zero element is printed along with its location.\n",
    "Code to remove stop words from the corpus is also given below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Ut4S3hBca18n",
   "metadata": {
    "id": "Ut4S3hBca18n"
   },
   "outputs": [],
   "source": [
    "corpus = [\"john told you to come market\", \"can you please come here\", \"can you please come to meet sofia\", \"i suggest you do not come to party\", \"please come with harry\", \"may i come in\", \"dreams come true\", \"why don't you come to audition\", \"may i come help you\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "IGZAvTtZa18o",
   "metadata": {
    "id": "IGZAvTtZa18o"
   },
   "outputs": [],
   "source": [
    "#Removing the stopwords. This function takes the value of a single review text as an argument.\n",
    "#Tokenize the text, remove the stopwords and return the cleaned review text\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens =  word_tokenize(text)\n",
    "\n",
    "    #Logic - Strip any extra spaces in each word of the list tokens\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    \n",
    "    #Removing stop words from the tokens and creating a list containing only non-stopword tokens\n",
    "    # Logic - convert each token in tokens to lowercase and check if it is a stopword. If it is not a stopword, then add it to the filtered_tokens list\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "\n",
    "    # Individual tokens(words) are joined with whitespace as a separator \n",
    "    filtered_text = ' '.join(filtered_tokens)   \n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qtElJ6jOa18o",
   "metadata": {
    "id": "qtElJ6jOa18o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john told come market',\n",
       " 'please come',\n",
       " 'please come meet sofia',\n",
       " 'suggest come party',\n",
       " 'please come harry',\n",
       " 'may come',\n",
       " 'dreams come true',\n",
       " \"n't come audition\",\n",
       " 'may come help']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying the function remove_stopwords on each element of the corpus\n",
    "cleaned_corpus=list(map(remove_stopwords, corpus))\n",
    "cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Rhmzj8wya18o",
   "metadata": {
    "id": "Rhmzj8wya18o"
   },
   "outputs": [],
   "source": [
    "##### write your code here#####\n",
    "#import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    " \n",
    "#Creating a matrix with reviews in row and unique words as columns and frequency of word in review as values. \n",
    "# TfidfVectorizer for tf idf calculation\n",
    "tf= TfidfVectorizer()\n",
    "\n",
    "#Fit transform model on entire \"cleaned_corpus\" dataframe\n",
    "tf_fit = tf.fit_transform(cleaned_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2a69a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.5637171267451105\n",
      "  (0, 1)\t0.21603009753900276\n",
      "  (0, 13)\t0.5637171267451105\n",
      "  (0, 5)\t0.5637171267451105\n",
      "  (1, 10)\t0.8865476297873808\n",
      "  (1, 1)\t0.46263733109032296\n",
      "  (2, 11)\t0.6101465333688625\n",
      "  (2, 8)\t0.6101465333688625\n",
      "  (2, 10)\t0.4480727981334654\n",
      "  (2, 1)\t0.23382297408245817\n",
      "  (3, 9)\t0.6824927635661049\n",
      "  (3, 12)\t0.6824927635661049\n",
      "  (3, 1)\t0.2615478070254108\n",
      "  (4, 3)\t0.7701064259345348\n",
      "  (4, 10)\t0.5655424103187389\n",
      "  (4, 1)\t0.2951234908732412\n",
      "  (5, 7)\t0.9106473463953395\n",
      "  (5, 1)\t0.4131844751477559\n",
      "  (6, 14)\t0.6824927635661049\n",
      "  (6, 2)\t0.6824927635661049\n",
      "  (6, 1)\t0.2615478070254108\n",
      "  (7, 0)\t0.9337801432948295\n",
      "  (7, 1)\t0.3578472355465217\n",
      "  (8, 4)\t0.7331888826871095\n",
      "  (8, 7)\t0.6192630030308558\n",
      "  (8, 1)\t0.28097579145049906\n"
     ]
    }
   ],
   "source": [
    "###Check the feature at (1,10) location\n",
    "print(tf_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0c197-d7c1-446e-89c1-3b44683b4444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f6c96d4d",
    "a889ded2"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.617378,
   "end_time": "2021-11-12T22:01:10.284200",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-12T22:00:51.666822",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
