{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZZdrYJ-xlqo"
   },
   "source": [
    "## Analyzing IMDb Reviews\n",
    "\n",
    "In this code demonstration,\n",
    "- We will do some **text preprocessing steps**.\n",
    "- Then, we will create features using **Bag of Words model** and **TF-IDF model**.\n",
    "- Finally, we will build the **logistic regression model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session: Text Analytics I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1424638f5259100af9f9a5c1b05bd23cf5b71e51",
    "id": "LonRsbKYNrGB"
   },
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5ulho010zoj"
   },
   "source": [
    "To start off, you will import the required libraries for visualising and analysing the text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PS0ezMQKNrGB",
    "outputId": "ae200dfd-cfc0-41eb-cf7b-f35ad887e311"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\DELL\n",
      "[nltk_data]     5590\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\DELL\n",
      "[nltk_data]     5590\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import nltk\n",
    "\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LObmYHIGNpYy"
   },
   "source": [
    "# 2. Load the data\n",
    "\n",
    "Load the data using either of the below methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method 1 - Google Colab\n",
    "- Run the code block below if you're using Google colab for this comprehension and have uploaded the IMDB_reviews.csv file in your Google Drive\n",
    "- Else, skip this part and move to the next cell block and run it in case you're using Jupyter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "df = pd.read_csv('IMDB_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production the filming tech...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically theres a family where a little boy j...  negative\n",
       "4  petter matteis love in the time of money is a ...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6Atg9S7kWyY"
   },
   "source": [
    "### **Question 1:**  *After importing the required packages and library. You have loaded the dataset into a DataFrame. What is the shape of the dataset?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ytzc24AwkWyZ",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of computation, we shall only use 10000 rows for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8Og21h9mOiLU"
   },
   "outputs": [],
   "source": [
    "# Let's take only first 10000 rows for the rest of the analysis for the ease of computation\n",
    "imdb = df.head(10000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money is a ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>fun entertaining movie about wwii german spy j...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>give me a break how can anyone say that this i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>this movie is a bad movie but after watching a...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>this is a movie that was probably made to ente...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>smashing film about filmmaking shows the inten...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     one of the other reviewers has mentioned that ...  positive\n",
       "1     a wonderful little production the filming tech...  positive\n",
       "2     i thought this was a wonderful way to spend ti...  positive\n",
       "3     basically theres a family where a little boy j...  negative\n",
       "4     petter matteis love in the time of money is a ...  positive\n",
       "...                                                 ...       ...\n",
       "9995  fun entertaining movie about wwii german spy j...  positive\n",
       "9996  give me a break how can anyone say that this i...  negative\n",
       "9997  this movie is a bad movie but after watching a...  negative\n",
       "9998  this is a movie that was probably made to ente...  negative\n",
       "9999  smashing film about filmmaking shows the inten...  positive\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiOW6pvw3e4b"
   },
   "source": [
    "# 3. Removing stopwords and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JdgD8EgrP4I9"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Setting English stopwords\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions 2,3,4 are to be answered with the code block given below. Check the corresponding comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe5VbVzHNCBr"
   },
   "source": [
    "- ##### **Question 2:**  You have a function remove_stopwords that  does tokenization and stopword removal of any given text. \n",
    "\n",
    "***You have the code written for tokenizing the text using word_tokenize. As an additional pre-processing step, you want to remove any white spaces occurring before or after each token of tokenized text. Which of the following code does it?***\n",
    "\n",
    "   - Hint: You can check this link:https://www.w3schools.com/python/ref_string_strip.asp for removing white spaces in a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__ubJGveNX9V"
   },
   "source": [
    "- ##### **Question 3:** *You have already saved the English stopwords in a list named â€˜stopwords_listâ€™. Which of the following code best fits the missing code in the removing stopwords from the tokenized text ?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euaXChBCNXvi"
   },
   "source": [
    "- ##### **Question 4:** *What does the statement return filtered_text of the function  remove_stopwords return?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [token.strip() for token in word_tokenize(imdb['review'][678])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(' '.join([x for x in a if x in stopword_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZfwBGSaNeIN",
    "outputId": "e20ff14c-c703-4d0f-bbd4-04d38e957db9"
   },
   "outputs": [],
   "source": [
    "###Code Block for Question 2,3,4\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "#You can either remove stopwords before or after stemming.\n",
    "#But since this is a review context, we expect users to have used many different words \n",
    "#And we did stemming before filtering for stopwords.\n",
    "\n",
    "#Removing the stopwords. This function takes the value of a single review text as an argument. \n",
    "#Tokenize the text, remove the stopwords and return the cleaned review text\n",
    "def remove_stopwords(text):\n",
    "    tokens =  word_tokenize(text)##Tokenize the text \n",
    "\n",
    "    #########(Question 2)############\n",
    "    ##Strip any extra spaces in each word of the list tokens\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    #########(Question 3)############\n",
    "    #Removing stop words from the tokens and creating a list containing only non-stopword tokens\n",
    "    # Logic - convert each token in tokens to lowercase and check if it is a stopword.\n",
    "    #If it is not a stopword, then add it to the filtered_tokens list\n",
    "    \n",
    "    filtered_tokens=[]\n",
    "    for token in tokens:\n",
    "        ## write your if condition statement here to filter stop_words:\n",
    "        if token.lower() not in stopword_list:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # Individual tokens(words) are joined with whitespace as a separator to create a complete sentence\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    #########(Question 4)############ \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if you have written the function ***remove_stopwords*** correctly, you can run the above function on a sample review and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2942"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Example\n",
    "len(imdb['review'][678]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original length of this review is 4202. After tokenization and stopword removal it should become 2942"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Running the following code to verify\n",
    "##Make sure the function remove_stopwords has been correctly written above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adenoid hynkel lowly soldier world war one rises subsequent years become ruthless dictator toumania creates aggressive antisemitic war machine cultivates little toothbrush moustache sound like anyone knowfrom safety hollywood chaplin uses soapbox exhort europe take arms defy hitler mussolini given united states 1940 year neutrality ahead strong desire embroil europes civil strife remember hitler declared war usa way round surprising chaplin allowed distribute immoderate polemicthe story involves one hand vulgar repellant hynkel reign terror ineptly presides life jewish ghetto every single person friendly humane brave etc etc chaplin hynkel also plays jewish barber little hero ghetto tramp name needless say chaplin writes directs stars composes music cateringin 1940 full truth yet known third reich chaplin forgiven something less perfect historical foresight even standards day gets hitler badly wrong comedian sentimentalist chaplin tries ridicule hitler making hynkel silly hapless humanise hynkel notverywarlike soldier fools around big gun upsidedown aeroplane becomes endearing rather despicable dictator inspects subordinates technical innovations dont work parachute hat bulletproof uniform etc passages meant make us think reallife nazis incompetent swept aside fact hynkels regime made cute likeable bumbling bodgeryin truth chaplins day already passed made illconsidered polemic heart still dinosaur silent screen check humour gags like staggering street semiconscious pantomime coins puddings hero schultz meant represent yardstick european decency hynkel judged schultz looks like character operetta nazi way believable schultz figure existed would say fuehrers face cause doomed failure built upon stupid ruthless persecution innocent people schultz come cellars jewish ghetto object exhaustive manhunt persist wearing ruritanian uniform chaplin yet know full horrors auschwitzbirkenau treblinka nazi concentration camp offers us hopelessly kilter grim spirit age usual chaplin thinks terms silent comedy setpieces loosely pegged onto narrative clothes line knockabout scrapping stormtroopers shaving man accompaniment brahms globe ballet watch segment filmed reversepaulette goddard unremittingly perfect hannah people ghetto impossibly nice jewish osterlich ridiculously idyllic hannah quite literally good true brave defiant resourceful hardworking course beautiful canary judaism ghetto cage gee aint cute asks barber gives makeover cute far answer doesnt come close ringing true chaplin made caricature wouldnt wonderful speech chaplin puts mouth typical author wordy emotionally cloyingjack oakie great napaloni fascist dictator bacteria brings whiff muchneeded comic brio proceedings films underlying weakness remains napaloni silly ineffective fear anyway stuttering stopstart backprojected train fine chaplinesque example gag persisted far beyond comic worthand jewish barber acquire immaculate hynkel uniform'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(imdb['review'][678])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have verified that the above function is working correctly, run the following code to ensure that the function is applied on all the reviews in the imdb DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "YC4L296nOJr3"
   },
   "outputs": [],
   "source": [
    "#Apply function on review column. Removing stopwords from each review in the dataframe\n",
    "imdb['review']=imdb['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       one reviewers mentioned watching 1 oz episode ...\n",
       "1       wonderful little production filming technique ...\n",
       "2       thought wonderful way spend time hot summer we...\n",
       "3       basically theres family little boy jake thinks...\n",
       "4       petter matteis love time money visually stunni...\n",
       "                              ...                        \n",
       "9995    fun entertaining movie wwii german spy julie a...\n",
       "9996    give break anyone say good hockey movie know m...\n",
       "9997    movie bad movie watching endless series bad ho...\n",
       "9998    movie probably made entertain middle school ea...\n",
       "9999    smashing film filmmaking shows intense strange...\n",
       "Name: review, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b66eeabd5b7b8c251f8b8ddf331140a64bcd514",
    "id": "2dMoCIjINrGI"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXHh5h-W1yFw"
   },
   "source": [
    "- Stemming is rule-based, it omits the last few letters like 'ing', 'ed', 'es' and more. It is fast but may create strange words.\n",
    "- Lemmatizing is dictionary-based, where it translates all words to the root form, like 'went' to 'go', 'going' to 'go' and more. \n",
    "Generally we prefer lemmatizing, but it might take some time in large datasets. \n",
    "- For the ease of computation we will use stemming in this analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mbzVCYwPuYl"
   },
   "source": [
    "### **Question 5:** Now that you have finished writing your stopwords removal function, the next step is to do stemming/lemmatization. Next, we will define a function named ***simple_stemmer*** which uses PorterStemmer to stem any review text passed to it as an argument. \n",
    "\n",
    "### Fill in the missing code for the simple_stemmer to work correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def simple_stemmer(text):\n",
    "\n",
    "#########(Question 5)############ \n",
    "    porter=PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for word in word_tokenize(text):\n",
    "        stem_word = porter.stem(word)\n",
    "        stemmed_words.append(stem_word)     \n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sweep'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter.stem('sweeping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if you have written the function ***simple_stemmer*** correctly, you can run the above function on sample review and view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Example\n",
    "len(imdb['review'][456])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original length of this review is 2509. After stemming it should become 2225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Running the following code to verify\n",
    "##Make sure the function simple_stemmer has been correctly written above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2225"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simple_stemmer(imdb['review'][456]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have verified that the above function is working correctly, run the following code to ensure that the function is applied on all the reviews in the imdb DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "o0t_bTcG3e4g"
   },
   "outputs": [],
   "source": [
    "# Applying stemming to all reviews in the dataframe column 'review'\n",
    "imdb['review'] = imdb['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9joxeClO3e4g"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh good would never thought possibl see thriller wors domest disturb soon arm rotten plot terribl edit stilt act headacheinduc style sorri word sanctimoni kind movi almost forc reevalu entir genr film bad even thriller condemn complet failur seem littl betternow sanctimoni terribl film also succe difficult task rip better movi pathet job right main titl noth blatant attempt reproduc one se7en impress someth didnt smell quit right soon movi start seri corni wannab hip quickcut full gori imag bombast color knew smell come fromit turn two policemen rather policeman jim renart michael par policewoman dorothi smith jennif rubin investig murder spree vancouv serial killer known monkey killer menac chill nicknam uh work method kill quit lot peopl see nut appar work follow proverb see evil hear evil speak evil cut eye ear tongu victim far six eye six ear three tongu ingeni fashion renart smith figur monkey killer probabl go kill three peopl well probabl want complet number 666 suddenli film focus tom gerrick casper van dien young success goodlook businessman dread temper that ripoff american psycho kick inso follow life two polic offic young psychopath none interest least final meet along way disco renart bare miss gerrick unintent offer us one funniest scene recent memori renart goe back disco club well script tell us suspect place one singl punch stomach renard get rid big guard block path guard never heard scene strike anyon els complet unrealisticanyway anoth murder gerrick turn wit smith especi renart immedi suspect might killer typic basic instinct fashion smith get date young businessman assumpt might discov true identityi wont spoil end quit simpli embarrass contradict plot hole issu never get resolv especi one last scene brutal mass murder suppos shock sad come laughabl overdon nonsens frankli cant imagin anyon could laugh itat 87 minut sanctimoni realli push never care one singl charact flat mention bore know exactli first time meet never pull stori scene connect weak plot devic downright unnecessari place act rang averag van dien downright atroci rubin support cast music abysm gener techno photographi one worst ever seen cours like everi fiasco genr provid littl bit gratuit nudity310'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of randomly selected review text from the dataframe after stemming\n",
    "imdb['review'][456]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the assessments for the first part of the comprehension.\n",
    "You can continue working on the next set of assessments after you have completed the Document Clustering session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session: Document Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the in-module demonstration, you saw an example, where you performed clustering on text data\n",
    "\n",
    "You can also peform other types of machine learning models on text data as well. **In this assessment, you shall be building a logistic  regression model on text data to predict sentiment of the review**\n",
    "\n",
    "This is a guided assessment to teach you additional concepts on top of the given in-module content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 1 Data Preprocessing\n",
    "\n",
    "#### 1.1 - Converting the labels to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to perform the necessary data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one review mention watch 1 oz episod youll hoo...  positive\n",
       "1  wonder littl product film techniqu unassum old...  positive\n",
       "2  thought wonder way spend time hot summer weeke...  positive\n",
       "3  basic there famili littl boy jake think there ...  negative\n",
       "4  petter mattei love time money visual stun film...  positive"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we shall convert the reviews from positive/negative to 1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labelling the sentiment data.\n",
    "\n",
    "#Label Binarizer converts Categorical data and outputs into a Numpy array of 0s and 1s. Label Binarizer is used to encode column data.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "\n",
    "#Transformed sentiment data\n",
    "imdb['sentiment']=lb.fit_transform(imdb['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now positive sentiment will be encoded as 1 and negative would be encoded as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one review mention watch 1 oz episod youll hoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonder littl product film techniqu unassum old...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonder way spend time hot summer weeke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basic there famili littl boy jake think there ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei love time money visual stun film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one review mention watch 1 oz episod youll hoo...          1\n",
       "1  wonder littl product film techniqu unassum old...          1\n",
       "2  thought wonder way spend time hot summer weeke...          1\n",
       "3  basic there famili littl boy jake think there ...          0\n",
       "4  petter mattei love time money visual stun film...          1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Check the DataFrame again\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 - How many reviews have a positive sentiment in the imdb DataFrame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write the code to find the number of positive sentiment reviews\n",
    "positive_sentiment = imdb[imdb['sentiment'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5028, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sentiment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Create a Tf-Idf object and fit the reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to convert the data to a tf-idf object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 - Write the code to import the necessary libraries and create a TF-IDF vectorizer object with all the default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Write the code for importing the necessary methods and create a TF-IDF vectorizer object\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the rest of the cells to convert the imdb to a tf_idf object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the reviews for fitting the tf-idf model\n",
    "norm_reviews=imdb.review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the tfidf_vectorizer\n",
    "tv_fit = tfidf_vectorizer.fit(norm_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells to perform a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the data into training and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tv_fit_train, X_tv_fit_test, y_train, y_test = train_test_split(norm_reviews, imdb['sentiment'], test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing train reviews with tfidf model\n",
    "X_tv_fit_train = tv_fit.transform(X_tv_fit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalised test reviews with tfidf model\n",
    "X_tv_fit_test = tv_fit.transform(X_tv_fit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 62938)\n",
      "(2000, 62938)\n"
     ]
    }
   ],
   "source": [
    "print(X_tv_fit_train.shape)\n",
    "print(X_tv_fit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##You can also check the array to which they have been transformed to\n",
    "X_tv_fit_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Modeling Building and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the logistic regression model for the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Training the model\n",
    "lr = LogisticRegression(penalty = 'l2', max_iter = 500, C=1, random_state=42).fit(X_tv_fit_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 - Fill in the missing code to compare the predictions for the test data with the actual label and calculate the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set Accuracy: 0.881\n"
     ]
    }
   ],
   "source": [
    "## Model Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predictions at threshold = 0.5\n",
    "y_pred = lr.predict(X_tv_fit_test)\n",
    "\n",
    "print('Validation set Accuracy: %.3f' % accuracy_score( y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Reading - Building Logistic Regression Model using Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an additional reading for your understanding of how to implement a logistic regresssion model on top of bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Creating a matrix with reviews in row and unique words as columns and frequency of word in review as values.\n",
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer()\n",
    "\n",
    "#Fitting model on entire data\n",
    "cv_fit = cv.fit(norm_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the train and the test dataset separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating sentiment count. imdb['sentiment'] now has the cleaned review text\n",
    "imdb['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_cv_fit_train, X_cv_fit_test, y_train, y_test = train_test_split(norm_reviews, imdb['sentiment'], test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing train reviews with bag of words model\n",
    "X_cv_fit_train = cv_fit.transform(X_cv_fit_train)\n",
    "\n",
    "#Normalised test reviews with bag of words model\n",
    "X_cv_fit_test = cv_fit.transform(X_cv_fit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cv_fit_train.shape)\n",
    "print(X_cv_fit_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the logistic regression model for the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Training the model\n",
    "lr = LogisticRegression(penalty = 'l2', max_iter = 500, C=1, random_state=42).fit(X_cv_fit_train, y_train)\n",
    "\n",
    "\n",
    "#Fitting the model for tf-idf features\n",
    "lr_bow=lr.fit(X_cv_fit_train,y_train)\n",
    "print(lr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bow = lr_bow.predict(X_cv_fit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification report for bag of words \n",
    "from sklearn.metrics import classification_report\n",
    "lr_bow_report=classification_report(y_test, y_pred_bow,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
