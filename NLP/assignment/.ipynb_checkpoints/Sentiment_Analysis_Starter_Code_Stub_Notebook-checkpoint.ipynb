{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE-i_DI0XG9E"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This assignment aims to provide a glimpse of the process of text preprocessing and the procedure of building a fully connected feedforward neural network to understand text.  We will also build another conventional supervised learning model for performance comparison. To obtain a relatively optimal model of neural networks, you need to tune hyperparameters associated the network. \n",
        "\n",
        "# Background\n",
        "\n",
        "This assignment is based on Sentiment Analysis.  Sentiment analysis is one of the popular and highly useful applications in today's world, especially in the digital world. It is used in different fields, such as social media monitoring and market research. \n",
        "\n",
        "\n",
        "# Data \n",
        "\n",
        "To undertake these tasks, we will be using a data set – Cornell Sentence Polarity Dataset. \n",
        "\n",
        "This data was first introduced by Bo Pang and Lillian Lee in a research paper titled ‘Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.’ Published at ACL 2005. \n",
        "\n",
        "The starter notebook dataset contains 5331 positive and 5331 negative snippets of movie reviews. Authors assumed snippets (from Rotten Tomatoes webpages) for reviews marked with ‘fresh’ are positive, and those for reviews marked with ‘rotten’ are negative. \n",
        "\n",
        "# Problem Statement\n",
        "\n",
        "You have to predict the sentiment of each review snippet, whether the review snippet is a positive or a negative one.\n",
        "\n",
        "\n",
        "Specifically in this assignment, you are going to build two models: \n",
        "  - One conventional supervised learning model (non-neural network-based)\n",
        "  - One deep learning based model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to use this Stub Notebook\n",
        "\n",
        "- This notebook provides an overall structure to your assignment, and ensures that you are on the right track towards the end solution. You would find:\n",
        "\n",
        "     - Each of the overall tasks for the assignment\n",
        "     - A breakdown of the necessary steps to complete each task\n",
        "     - Recommended approaches to solve each of the steps\n",
        "     - Checklist to ensure that you're on the right track\n",
        "\n",
        "- Note that there's no single correct approach to solve this assignment. Your approach would be evaluated fairly as long as the broad direction is correct, and you are able to achieve optimal results"
      ],
      "metadata": {
        "id": "myxwVfa13Fs0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lthtl__bN2jc"
      },
      "source": [
        "## Task I - Represent each review using an N-gram model\n",
        "\n",
        "For this task, you will perform the following steps:\n",
        "\n",
        "- Data Loading\n",
        "  - Import the necessary libraries\n",
        "  - Load the dataset\n",
        "\n",
        "- Use NLP techniques to preprocess the data set\n",
        "    - Tokenization and Stopword removal\n",
        "    - Stemming\n",
        "    - Lemmatization\n",
        "- Convert each document (i.e., a review) into a numeric vector using an N-gram model, in particular, the **Tf-Idf**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guidelines\n",
        "\n",
        "To perform the above tasks and sub-tasks, perform the following steps:\n",
        "\n",
        "- Load the necessary libraries and methods for this assignment\n",
        "  - Loading and working with data sets - *pandas* and *NumPy*\n",
        "  - Visualizing data - *pyplot* (from *matplotlib*) and *seaborn*\n",
        "  - NLP techniques - To perform the text processing techniques use `nltk`. Also, download the necessary methods and corpuses from `nltk` for performing the steps such as  \n",
        "    - Tokenization\n",
        "    - Stop word removal\n",
        "    - Stemming and Lemmatization\n",
        "  -  Processing data:\n",
        "    - *LabelEncoder* (from *sklearn.preprocessing*)\n",
        "    - *train_test_split* (from *sklearn.model_selection*)\n",
        "  - Conventional Machine Learning Models: Load the necessary packages for creating and evaluating the conventional machine learning models such as \n",
        "    - Logistic Regression\n",
        "    - Decision Tree\n",
        "    - Random Forests\n",
        "    - kNN\n",
        "\n",
        "  - Deep Learning Models: Import the necessary packages and methods for creating deep learning models\n",
        "    - *tensorflow*\n",
        "    - *keras*\n",
        "    - all the necessary submodules and methods from the above to train and evaluate neural networks modules\n",
        "  - Suppressing warnings\n",
        "    "
      ],
      "metadata": {
        "id": "xcj3UCOe72sC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-no7l22nFOCo"
      },
      "outputs": [],
      "source": [
        "# Import libraries to load datasets\n",
        "\n",
        "\n",
        "# Import libraries for visualization\n",
        "\n",
        "\n",
        "# Importing libraries for NLP techniques \n",
        "\n",
        "\n",
        "# Importing packages for other pre-processing\n",
        "\n",
        "\n",
        "# Import packages for building conventional supervised learning models\n",
        "\n",
        "\n",
        "# Import methods for building neural networks\n",
        "\n",
        "\n",
        "# Write code to suppress warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wt2nBg9fRHoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Load the dataset and check a few reviews\n",
        "     - If you're using your local machine, make sure that the data set is in the right folder\n",
        "     - If you're using Google colab you can simply the upload the csv file containing the data to your Google Drive and mount your Google Drive in your Colab VM to access the data."
      ],
      "metadata": {
        "id": "KrgXT_f0RJbO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ud1zs_4FSQu7"
      },
      "outputs": [],
      "source": [
        "# Write code to read the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the 5 rows"
      ],
      "metadata": {
        "id": "GR5i6GIEYV0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7jPn0T1P76W"
      },
      "source": [
        "### Tokenization and Stopwords removal\n",
        "\n",
        "---\n",
        "\n",
        " - Initialize the tokenizer that you'll use. Note: This will impact your overall model building process.\n",
        " - Load the stop words from the nltk stopwords corpus and store it in a list *stopword_list*\n",
        " - Write a function `remove_stopwords` that takes a piece of text, tokenizes it and removes all the stopwords using the *stopword_list*\n",
        "  - Input: a piece of text (`str`)\n",
        "  - Output: the same piece of text, tokenized and with the stopwords removed.\n",
        " - Apply the function on a few rows from the data set and observe the results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVCMIBomTVZA"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer. You can try out a few methods from the nltk.tokenize module and see which one performs best\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function \"remove_stopwords\" that takes a piece of text as input and outputs the tokenized version along with the stopwords removed.\n",
        "# You should use the tokenizer and \"stopword_list\" that you created in the previous code cells for developing this function"
      ],
      "metadata": {
        "id": "M7ZUHjLrY1ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vyKrbf6XUOs5"
      },
      "outputs": [],
      "source": [
        "# Apply function on a few review rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checklist**:\n",
        "\n",
        "Check whether the function `remove_stopwords` is working as intended.\n",
        "- Is it tokenizing the review snippet correctly?\n",
        "- Are the unnecessary characters and stopwords removed?\n",
        "\n",
        "Before applying the function on the entire data set, see if you can improve the tokenizer and/or your stopword list further.\n",
        "\n",
        "**References**:\n",
        "\n",
        "In case your tokenizer isn't performing satisfactorily, or if you're getting errors,  You can check out the following links for your reference\n",
        "  - You can use `toktok Tokenizer` [Here's a link](https://www.nltk.org/api/nltk.tokenize.toktok.html) for your reference.\n",
        "  - You might have to download `punkt` module in case you are getting errors. [Here's a link](https://www.nltk.org/api/nltk.tokenize.punkt.html) for your reference\n",
        "\n",
        "**Next Steps**:\n",
        "\n",
        "Once you're done with the verification and improvisation, the next step is to apply the `remove_stopwords` on the entire data."
      ],
      "metadata": {
        "id": "RU5klz59Yks1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply \"remove_stopwords\" on all the reviews in the dataset"
      ],
      "metadata": {
        "id": "E1ksSF5zYjwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI2T9TBxQzhi"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "- Initialize the stemmer that you're going to use. We recommend using `PorterStemmer` from `nltk.porter` module\n",
        "- Create a function `simple_stemmer` that takes a given piece of text and outputs the stemmed version of the individual words using the stemmer initialized earlier\n",
        "    - Input: a piece of text (`str`)\n",
        "    - Output: stemmed version of the text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the stemmer"
      ],
      "metadata": {
        "id": "nQiYtOMgs6Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0Keda2BUIL1"
      },
      "outputs": [],
      "source": [
        "# Define \"simple_stemmer\"\n",
        "\n",
        "# Apply function on a few reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checklist and Next Steps:\n",
        "\n",
        "- Check if the `simple_stemmer` function is working as intended. Once verified, go ahead and apply it on the entire reviews column."
      ],
      "metadata": {
        "id": "o8NUlFeUrhvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply \"simple_stemmer\" on all the reviews in the dataset\n"
      ],
      "metadata": {
        "id": "Fcb7dy_3sDA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMyWQU7nRbm4"
      },
      "source": [
        "### Lemmatization\n",
        "\n",
        "- Initialize the Lemmatized that you're going to use. We recommend using 'WordNetLemmatizer'\n",
        "- Create a function `simple_lemmatize` that takes a given piece of text and outputs the lemmatizedd version of the individual words using the lemmatizer initialized earlier\n",
        "    - Input: a piece of text (`str`)\n",
        "    - Output: lemmatized version of the text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the lemmatizer\n"
      ],
      "metadata": {
        "id": "G25EUAPzs2U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKSQM1VXRe7x"
      },
      "outputs": [],
      "source": [
        "# Define \"simple_lemmatize\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function on a few reviews"
      ],
      "metadata": {
        "id": "Rc3Gl_v-tC4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checklist and Next Steps:\n",
        "\n",
        "- Check if the `simple_lemmatize` function is working as intended. Once verified, go ahead and apply it on the entire reviews column."
      ],
      "metadata": {
        "id": "1bFHnnVhtH6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply \"simple_lemmatize\" on all the reviews in the dataset"
      ],
      "metadata": {
        "id": "JabvOHmqtdaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Uem7LWP61xL1"
      },
      "outputs": [],
      "source": [
        "# Review the dataset after using NLP techniques to pre-process it\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYq_Qen9RfWW"
      },
      "source": [
        "### Tf-Idf Data Preparation\n",
        "\n",
        "#### Guidelines\n",
        "\n",
        "- Perform a train_test split of the reviews data. You can choose a split of your choice, along with any random state.\n",
        "- Use the `TfidfVectorizer` object to create a n-gram model for both the train and validation reviews\n",
        "- For this assignment, we recommend using max_features as `500`, though you can tweak the parameters to creat your final data set\n",
        "- Also, ngram_range should be set to (1,2) \n",
        "- After `TfidfVectorizer` method has been initialized, use it to fit and transform the train data. Use the same tf-idf model to transform the validation data as well.\n",
        "- Check the final shape of train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QM7QVgFlTL20"
      },
      "outputs": [],
      "source": [
        "# Perform a train-test split of the reviews data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TfidfVectorizer object and use it to fit and transform the train set"
      ],
      "metadata": {
        "id": "GYx-_3b0Xbhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the same model, transform the validation set\n"
      ],
      "metadata": {
        "id": "l-l9AHlNXtFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF002HERsGZ0",
        "outputId": "318ddefb-514e-4230-8c0e-e7fd7be457b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tfidf_train: (8529, 500)\n",
            "Tfidf_val: (2133, 500)\n"
          ]
        }
      ],
      "source": [
        "# Check the shape of the train and validation sets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding sentiments\n",
        "\n",
        "#### Guidelines\n",
        "- As you might have seen in the dataset, the sentiments are encoded as 'pos' and 'neg'\n",
        "- We need to convert these to 1 and 0 respectively."
      ],
      "metadata": {
        "id": "LTqLAXHAX5Uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pCsVY4sdsNyF"
      },
      "outputs": [],
      "source": [
        "# Split the target variable to train and validation sets\n",
        "\n",
        "# Using LabelEncoder, convert 'pos' and 'neg' classes to '1' and '0' in the training data set output\n",
        "\n",
        "# Using LabelEncoder, convert 'pos' and 'neg' classes to '1' and '0' in the validation  data set output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afHKKVtpRx5h"
      },
      "source": [
        "# Task II - Build a conventional supervised machine learning model\n",
        "\n",
        "## Guidelines\n",
        "\n",
        "**Note** : The minimum requirement here is to build at least one conventional machine learning model. Though we recommend you try out multiple ML models and see which one is performing the best.\n",
        "\n",
        "Here's our suggested approach:\n",
        "\n",
        " - Train different classification models on the training data set and test its performance\n",
        "    - Logistic regression model\n",
        "    - Decision tree model\n",
        "    - kNN model\n",
        "    - Random forest classifier model\n",
        "    - Light gradient boosted tree classifier\n",
        " - Analyze the performance of all the models and select the one that performs with good accuracy and robustness\n",
        " - Perform hyperparameter tuning for the best model to further improve the model performance\n",
        " - Display the final model performance parameters like accuracy, confusion matrix, ROC and AUC.\n",
        "\n",
        " ### References\n",
        "\n",
        " - We strongly urge you to check the code that you learned in the Machine Learning course (You can refer to the Model Selection session for an overview of multiple ML models quickly).\n",
        " - Here's a list of documentations for different ML models that you can build here:\n",
        "  - [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) \n",
        "  - [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
        "  - [kNN](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "  - [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
        "  - [LGBM ](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u0owJrjCsX1o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykrcIdDiUeKv"
      },
      "source": [
        "# Task III - Build a deep learning model\n",
        "\n",
        "## Guidelines\n",
        "\n",
        "For this task, you will perform the following steps:\n",
        "  - Convert the data set stored in as a sparse matrix to a sparse tensor for the Keras model\n",
        "    - A reference code is provided below\n",
        "  - Create a simple fully-connected feedforward neural network\n",
        "  - Fit the model on the training data set\n",
        "  - Test the accuracy of the model on the validation data set\n",
        "  - Show how the accuracy changes with every epoch during the training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reference code for converting the sparse Tf-idf matrix to a sparse tensor\n",
        "\n",
        "Converting the sparse matrix of a dataset obtained from Tf-Idf vectorization to a sparse tensor.This is done as Keras runs on Tensorflow backend and a Keras model accepts a sparse tensor as input, not a sparse matrix.\n",
        "\n",
        "You can do the same easily using the `tf.SparseTensor` method([documentation](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)) along with the `.tocoo()` method ([documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.tocoo.html))\n",
        "\n",
        "The steps are as follows:\n",
        "\n",
        "-  The .tocoo() method converts the tf-idf matrix to a coordinate format.\n",
        "-  the tf.SparseTensor() method takes the output of the .tocoo() method and creates a SparseTensor"
      ],
      "metadata": {
        "id": "JxgCf_GL66aD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A sample code has been provided here"
      ],
      "metadata": {
        "id": "y8yoUM6y8p-F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8D81sk77YJw"
      },
      "outputs": [],
      "source": [
        "#def convert_sparse_matrix_to_sparse_tensor(X):\n",
        "    #coo = X.tocoo()\n",
        "    #indices = np.mat([coo.row, coo.col]).transpose()\n",
        "    #return tf.SparseTensor(indices, coo.data, coo.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the sparse matrix from the above function, you have to do sparse reordering because tensorflow accepts input in row-major format. You can do the same using the `tf.sparse.reorder` method\n",
        "\n",
        "You can check out its documentation [here](https://www.tensorflow.org/api_docs/python/tf/sparse/reorder)"
      ],
      "metadata": {
        "id": "PZOln_R_76D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## After converting the tf-idf data to a SparseTensor using the code snippet above, use the code below to do sparse reordering.\n",
        "##tf.sparse.reorder(X)\n",
        "## Here X is the sparse tensor output of the \"convert_sparse_matrix_to_sparse_tensor\" function"
      ],
      "metadata": {
        "id": "-FYYjieC73Yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "sB5s_Yva_sVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Neural Network\n",
        "\n",
        "Now that you saw how to convert the data to the correct format, it's time to implement it.\n",
        "\n"
      ],
      "metadata": {
        "id": "nAfCv6eJADbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the sparse tensor input for both train and validation sets"
      ],
      "metadata": {
        "id": "ufzK_Es7CLSM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6LA6GmLu78Rz"
      },
      "outputs": [],
      "source": [
        "# Converting sparse matrix to sparse tensor to submit as input to the Keras model\n",
        "# Use the reference code that you learnt earlier\n",
        "# Perform both the steps for train and validation sets\n",
        "\n",
        "\n",
        "## Write your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the neural network function\n",
        "\n",
        "You can now go ahead and build the neural network. Here are a sample set of instructions that you may follow for this\n",
        "\n",
        "\n",
        "- Start by defining a function called `create_nn` with two parameters: activation_function and hidden1_neurons.\n",
        "\n",
        "- Inside the function, create a sparse input layer for a neural network using keras.Input(). Set the shape parameter to the correct value to indicate the input shape and `sparse=True` to specify that the input is a sparse tensor. Assign this input layer to a variable called `input`.\n",
        "\n",
        "- Create the first hidden layer of the neural network using layers.Dense(). Set the units parameter to hidden1_neurons (which has a default value of 256) and the activation parameter to activation_function. Pass the input layer as the input to this hidden layer. Assign this hidden layer to a variable called hidden1.\n",
        "\n",
        "- Create the second hidden layer of the neural network using layers.Dense(). Set the units parameter to 64 and the activation parameter to activation_function. Pass the hidden1 layer as the input to this hidden layer. Assign this hidden layer to a variable called hidden2.\n",
        "\n",
        "- Create the output layer of the neural network using layers.Dense(). Set the units parameter to 1 (since it has only one node). Pass the hidden2 layer as the input to this output layer. Assign this output layer to a variable called output.\n",
        "\n",
        "- Use keras.Model() to create the neural network model. Pass the input layer as the first argument and the output layer as the second argument. This function call will create and return the fully connected feedforward neural network.\n",
        "Finally, add a return statement to return the created model.\n",
        "\n",
        "*Remember to import the necessary libraries, such as tensorflow and keras, before running the code*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZerR3HRBBG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7K_6w5161bs"
      },
      "outputs": [],
      "source": [
        "# Defining a fully-connected feedforward neural network\n",
        "# You may write the code using the instructions shared above or follow your own approach"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the FCFNN instance\n",
        "\n",
        "- Create an instance of the fully connected feedforward neural network.\n",
        "- Set the learning rate value to 0.01. This value will be used when compiling the neural network.\n",
        "- Compile the neural network using the compile() method of nn1. \n",
        "- **Important** - Set the loss parameter to `'binary_crossentropy'`, since the output layer has only 1 node. \n",
        "- Set the metrics parameter to ['accuracy'] to track the accuracy during training. \n",
        "- For the optimizer parameter, create an instance of the RMSprop optimizer \n",
        "- Call the summary() method on the instance to print a summary of the neural network architecture.\n",
        "\n",
        "*Remember to import the necessary libraries, such as tensorflow, keras, and RMSprop, before running the code.*"
      ],
      "metadata": {
        "id": "SBh5DQXkDQr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zhFTIqPT6-2M"
      },
      "outputs": [],
      "source": [
        "## Build the FCFNN instance using the instructions shared above or you can follow your own approach"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build a simple FCFNN model\n",
        "\n",
        "- Train the neural network model on the dataset using the fit() method of the FCFNN instance you built previously\n",
        "- Pass the training data and the validation data\n",
        "- Set the epochs parameter to 10 to specify the number of training epochs.\n",
        "- The fit() method will return a history object that contains information about the training process. Assign this object to a variable"
      ],
      "metadata": {
        "id": "QOxo-mlSELHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m-BlEoIC7kFx"
      },
      "outputs": [],
      "source": [
        "# Train the neural network model on the dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert Neural Network History to a DataFrame and Plot it\n",
        " \n",
        "- Create a DataFrame containing the training history of the neural network.\n",
        "- Add a new column to the DataFrame called 'epoch' that represents the epochs of the training process.\n",
        "- After that plot the training and validation accuracies as functions of epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "YWH1D9IkFPtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-wDL8mOz7i_g"
      },
      "outputs": [],
      "source": [
        "# Convert the neural network history object into a data frame to view its specifics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OU3nlVwg7zyq"
      },
      "outputs": [],
      "source": [
        "# View the training and validation accuracies as functions of epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the validation accuracy"
      ],
      "metadata": {
        "id": "dBUZvkoIF6o1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9n2ZYiZ47sS-"
      },
      "outputs": [],
      "source": [
        "# Compute the final accuracy of the model on the validation data set using the 'evaluate()' method\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "Now that you have built a simple FCFNN model, it's time to optimize the model using hyperparameter tuning.\n",
        "\n",
        "#### Guidelines\n",
        "Here are a few guidelines on how to do the same:\n",
        "\n",
        "- Choose multiple activation functions such as 'sigmoid' and 'relu'.\n",
        "\n",
        "- Choose multiple values for hidden neurons such as 128 and 256.\n",
        "\n",
        "- Create an empty DataFrame to store the performance results of the nn models that we plan to build\n",
        "\n",
        "- Run a nested loop across all the different activation functions in activation_function_list, and over the number of neurons in hidden1_neurons_list. and create a neural network for each instance.\n",
        "\n",
        "- Complie the model similar to how built the simple FCFNN model. Use the different parameters accordingly.\n",
        "\n",
        "- Create a DataFrame with the relevant information (activation function, number of neurons, train accuracy, and validation accuracy) for the current model.\n",
        "\n",
        "- Concatenate the same with the previous empty DataFrame that we created. After the nested loop finishes, the DataFrame will contain the performance results for each model."
      ],
      "metadata": {
        "id": "06-YE5T1F_7Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U-q9QKDjCz76"
      },
      "outputs": [],
      "source": [
        "## Perform hyperparameter tuning using the guidelines shared above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8nV_j33tLCn6"
      },
      "outputs": [],
      "source": [
        "## Show the performance results for each of the neural network models that we build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "covHWUATEoU-"
      },
      "outputs": [],
      "source": [
        "# Visualize the training accuracies as functions of epochs for different values of learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p6RMwDW-KRx8"
      },
      "outputs": [],
      "source": [
        "# Visualize the validation accuracies as functions of epochs for different values of learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the optimal model\n",
        "\n",
        "Once you have evaluated the performance for all the combination of hyperparameters, it's time to build the final model using the optimal values.\n",
        "\n",
        "### Guidelines\n",
        "\n",
        "- You can evaluate how the training and validation accuracies are changing for the different combination of hyperparameters in the previous step.This should give you an idea of the most optimal performance values that you can expect.\n",
        "\n",
        "- Identify the hyperparameters for the best model, and use it to retrain the model.\n",
        "\n",
        "- *Note that the validation accuracy may not be at par with the conventional models*. You should expect this and need not worry in case you get low values for validation accuracy. "
      ],
      "metadata": {
        "id": "3G3ie3hRRGIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6x_jI7bAE3Do"
      },
      "outputs": [],
      "source": [
        "# Retrain the model with the optimal combination of hyperparameters and save its training history\n",
        "# Follow the same guidelines as you have done previously to build the neural networks model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-5gTrumkLQ1n"
      },
      "outputs": [],
      "source": [
        "# View the training and validation accuracies as functions of epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final validation accuracy"
      ],
      "metadata": {
        "id": "VnlFB4hbR_uw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GNb2VkMMLV1A"
      },
      "outputs": [],
      "source": [
        "# Compute the final accuracy of the model on the validation data set using the 'evaluate()' method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2gFD3_XNbIU"
      },
      "source": [
        "## Results\n",
        "\n",
        "- Document the results of both the conventional and the deep learning models here\n",
        "\n",
        "- Which model would you finally choose?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Remarks\n",
        "\n",
        "Congratulations! You have now reached the end of this assignment. With this, you are now well equipped with the skills to tackle advanced NLP problems that require DL applications as well. \n",
        "\n",
        "In fact, if you are felt that the performance of the basic neural network models left much to be desired, don't worry! In the upcoming weeks you'll learn some state of the art DL/NLP architectures such as transformers which perform significantly better and are responsible for some of the current advances in AI. We hope you have a great experience ahead!"
      ],
      "metadata": {
        "id": "oI47Vv0dTkhB"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}