{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzHOLwhMtIdK"
   },
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfrFreETmkMy"
   },
   "source": [
    "## Session 1:  Basic Text Analysis\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdJ3Vt2kzWjW"
   },
   "source": [
    "For the demonstrations in this module, we shall be using the NLTK library. **NLTK** is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries (src: https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8311,
     "status": "ok",
     "timestamp": 1663378361206,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "PUjgCvxglhHF",
    "outputId": "c59f31aa-06bf-4a80-cd36-2d555dc8da72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "##Install the NLTK library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5FpFnW1PHWi"
   },
   "source": [
    "\n",
    "\n",
    "Let's start with basic NLP operations, usually used for text preprocessing \n",
    "to improve the quality of data for better subsequent tasks, such as: \n",
    "\n",
    "- stopword removal\n",
    "- word/sentence tokenization\n",
    "- part-of-speech (POS)\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- Bag of words\n",
    "- Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM_uH4itvZ9L"
   },
   "source": [
    "## Stopword Removal and Tokenization\n",
    "\n",
    "- **Stopword Removal**: Many frequently occurring words that are not important for understanding semantics, also called *stopwords*, can be removed.\n",
    "- **Tokenization**: Splitting text into smaller elements (characters, words, sentences, paragraphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1973,
     "status": "ok",
     "timestamp": 1663378367017,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "86bHw3jXNbZK",
    "outputId": "88df3cd3-b801-4822-ea8b-8e004c272c57",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We need to download the stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjSljuazWjZ"
   },
   "source": [
    "Next, we shall be downloading the *Punkt* sentence tokenizer. Read more: https://www.nltk.org/_modules/nltk/tokenize/punkt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 939,
     "status": "ok",
     "timestamp": 1663378368934,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "c86sDCNONbZM",
    "outputId": "a9d9db09-e571-4c15-b859-1f0487414244"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1663378369212,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "MKg9HvDHNbZN",
    "outputId": "42470b70-40ae-4eda-8d16-bca6eb1e05f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "## We have stopwords in multiple languages\n",
    "stops_en = stopwords.words('english')\n",
    "stops_ge = stopwords.words('german')\n",
    "print(stops_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1663378370066,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "ZXtQASvWNbZN",
    "outputId": "36986066-205c-4c07-f7a3-7e3b60be12f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'airline']\n"
     ]
    }
   ],
   "source": [
    "# customize your stop word list by adding words\n",
    "stops_en.append('airline')\n",
    "print(stops_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4evIzq9BNbZQ"
   },
   "outputs": [],
   "source": [
    "# sentence / word tokenization\n",
    "cnn = 'The Cable News Network is a multinational news-based pay television channel headquartered in Atlanta, Georgia. It is owned by CNN Global, which is part of Warner Bros, Discovery. It was founded in 1980 by American media proprietor Ted Turner and Reese Schonfeld as a 24-hour cable news channel.'\n",
    "\n",
    "word_tokenize(cnn)\n",
    "#sent_tokenize(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1663378374611,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "D5KT6wDqNbZT",
    "outputId": "377945ec-fb6a-48ed-857a-826c2d6c450b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this : a stop word.\n",
      "first\n",
      "sentence\n",
      "this : a stop word.\n",
      "second\n",
      "sentence\n"
     ]
    }
   ],
   "source": [
    "# a combination of tokenization and stopword removal\n",
    "sent = 'This is the first sentence, and this is the second sentence.'\n",
    "words = word_tokenize(sent.lower())\n",
    "\n",
    "for word in words:\n",
    "  if len(word) <= 3:\n",
    "    continue\n",
    "  if word in stops_en:\n",
    "    print(word,': a stop word.')\n",
    "  else:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CP2TWgFCwFHG"
   },
   "source": [
    "## POS tagging\n",
    "\n",
    "- **Part-of-speech (POS)** tagging: Figuring out what are nouns, verbs, adjectives, etc. \n",
    "- It refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRZ8SLobNbZV"
   },
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "from nltk import pos_tag \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1663378379837,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "FXLh4D0wNbZV",
    "outputId": "4f4d2a27-bc4f-4d0d-8c9e-c888bf7cfd95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('like', 'VBP'), ('that', 'DT'), ('awesome', 'JJ'), ('movie', 'NN'), (',', ','), ('especially', 'RB'), ('the', 'DT'), ('great', 'JJ'), ('director', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sent = 'I like that awesome movie, especially the great director.'\n",
    "words = word_tokenize(sent)\n",
    "tagged = pos_tag(words) \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfXtGXkxwTeE"
   },
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "- **Stemming** - Removal or \"stemming\" of the last few words of a certain word.\n",
    "- **Lemmatization** - Merging modified versions of \"same\" word to be analyzed as a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1663378381919,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "J_p_nnXiNbZW",
    "outputId": "a3166ea4-a064-468a-e3ff-97599e888799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , i , i\n",
      "liked , like , lik\n",
      "that , that , that\n",
      "awesome , awesom , awesom\n",
      "movie , movi , movy\n",
      ", , , , ,\n",
      "especially , especi , espec\n",
      "the , the , the\n",
      "director , director , direct\n",
      "was , wa , was\n",
      "the , the , the\n",
      "best , best , best\n",
      "guy , guy , guy\n",
      ". , . , .\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "\n",
    "sent = 'I liked that awesome movie, especially the director was the best guy.'\n",
    "words = word_tokenize(sent)\n",
    "for w in words:\n",
    "    print(w,',',porter.stem(w),',',lancaster.stem(w)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2043,
     "status": "ok",
     "timestamp": 1663378385931,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "soImeUoANbZX",
    "outputId": "9e3fbb04-a6c2-4366-8883-c6aff0cc15b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I , I\n",
      "liked , liked\n",
      "that , that\n",
      "awesome , awesome\n",
      "movie , movie\n",
      ", , ,\n",
      "especially , especially\n",
      "the , the\n",
      "director , director\n",
      "now , now\n",
      "was , wa\n",
      "moving , moving\n",
      "to , to\n",
      "another , another\n",
      "states , state\n",
      ". , .\n",
      "I , I\n",
      "am , am\n",
      "liking , liking\n",
      "the , the\n",
      "show , show\n",
      "as , a\n",
      "well , well\n",
      ". , .\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "sent = 'I liked that awesome movie, especially the director now was moving to another states. I am liking the show as well.'\n",
    "words = word_tokenize(sent)\n",
    "for t in words:\n",
    "    print(t,',',wnl.lemmatize(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q14zClHfzWjd"
   },
   "source": [
    "## Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20353,
     "status": "ok",
     "timestamp": 1663378484951,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "RduO4lJs3mfz",
    "outputId": "ac3792c3-0e4e-4c26-fc61-37945857ea64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pywsd\n",
      "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.9 MB 2.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.21.6)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pywsd) (3.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.15.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.3.5)\n",
      "Collecting wn==0.0.23\n",
      "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 31.6 MB 1.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (2022.6.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2.8.2)\n",
      "Building wheels for collected packages: wn\n",
      "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792926 sha256=4996c9f66dc825e98015cd390274ea3d0627482aa500cb2b978fc76ede7c70e1\n",
      "  Stored in directory: /root/.cache/pip/wheels/ec/47/17/409766c99dd470f34c512000b90b83f34747c2c975769654d7\n",
      "Successfully built wn\n",
      "Installing collected packages: wn, pywsd\n",
      "Successfully installed pywsd-1.2.5 wn-0.0.23\n"
     ]
    }
   ],
   "source": [
    "!pip install pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10335,
     "status": "ok",
     "timestamp": 1663378625563,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "WEupNzhwzWjd",
    "outputId": "b3ed0641-bff8-4d8f-b892-1b901b03983e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: Synset('depository_financial_institution.n.01')\n",
      "Definition: a financial institution that accepts deposits and channels the money into lending activities\n",
      "===\n",
      "Sense: Synset('bank.n.01')\n",
      "Definition: sloping land (especially the slope beside a body of water)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "took 4.277987241744995 secs.\n"
     ]
    }
   ],
   "source": [
    "# Word Sense Disambiguation\n",
    "\n",
    "from pywsd.lesk import simple_lesk\n",
    "\n",
    "bank_sents = ['I went to the bank to deposit my money', 'The river bank was full of dead fishes']\n",
    "\n",
    "#plant_sents = ['The workers at the industrial plant were overworked', 'The plant was no longer bearing flowers']\n",
    "\n",
    "answer = simple_lesk(bank_sents[0],'bank')\n",
    "print(\"Sense:\", answer)\n",
    "print(\"Definition:\",answer.definition())\n",
    "\n",
    "print('===')\n",
    "\n",
    "answer = simple_lesk(bank_sents[1],'bank')\n",
    "print(\"Sense:\", answer)\n",
    "print(\"Definition:\",answer.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMCIm1oNzWje"
   },
   "source": [
    "## Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1663379043473,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "lH0QWKADrW9L",
    "outputId": "90e05df3-22d5-4d74-99e3-7a2ee3fc14ce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1663379081844,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "1t-WM975rl3B",
    "outputId": "c84c4d32-52dd-4bf3-ee99-182addfa6ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  What/WP\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  weather/NN\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  and/CC\n",
      "  (GPE Chicago/NNP)\n",
      "  today/NN\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "#ex = 'European authorities fined Google a record $5.1 billion on Wednesday for abusing its power in the mobile phone market and ordered the company to alter its practices'\n",
    "ex = 'What is the weather in New York and Chicago today?'\n",
    "words = word_tokenize(ex)\n",
    "tags = pos_tag(words)\n",
    "\n",
    "ne_tree = nltk.ne_chunk(tags)\n",
    "\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRlUgO7JSPlH"
   },
   "source": [
    "<a id='lab2'></a>\n",
    "## Session 2: Document Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0I3JMPOzWje"
   },
   "source": [
    "### The task here is to cluster scientific papers based on their textual titles, basically separating them into a few groups. The features we use to do this document clustering task is TF-IDF values of top unique words across documents. \n",
    "\n",
    "### The clustering algorithm we use for this demonstration is KMeans. \n",
    "\n",
    "### The main objective of this task is to learn how to represent a document using TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldrXx18YScaA"
   },
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1663379772123,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "5-MkuxXFKAY7",
    "outputId": "7a5ec1e4-0265-4954-a182-c9f671f60a81"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-61fb8359-a65d-4d7c-9a6f-5b3a7b961274\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>Neural Network Ensembles  Cross Validation  an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61fb8359-a65d-4d7c-9a6f-5b3a7b961274')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-61fb8359-a65d-4d7c-9a6f-5b3a7b961274 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-61fb8359-a65d-4d7c-9a6f-5b3a7b961274');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     id                                              title\n",
       "0     1  Self-Organization of Associative Database and ...\n",
       "1    10  A Mean Field Theory of Layer IV of Visual Cort...\n",
       "2   100  Storing Covariance by the Associative Long-Ter...\n",
       "3  1000  Bayesian Query Construction for Neural Network...\n",
       "4  1001  Neural Network Ensembles  Cross Validation  an..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "papers = pd.read_csv('papers.csv',header=0)\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOHPEw8cSuA9"
   },
   "source": [
    "### 2. Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1663379893043,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "P2-UsL7cKcjp",
    "outputId": "b0391e4a-5303-40cb-c238-c7682baac044"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e3b7b114-3425-46e0-a1e3-0ea025ab1e4e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>processed_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>self-organization of associative database and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>a mean field theory of layer iv of visual cort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>storing covariance by the associative long-ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>bayesian query construction for neural network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>Neural Network Ensembles  Cross Validation  an...</td>\n",
       "      <td>neural network ensembles  cross validation  an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e3b7b114-3425-46e0-a1e3-0ea025ab1e4e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e3b7b114-3425-46e0-a1e3-0ea025ab1e4e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e3b7b114-3425-46e0-a1e3-0ea025ab1e4e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     id                                              title  \\\n",
       "0     1  Self-Organization of Associative Database and ...   \n",
       "1    10  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "2   100  Storing Covariance by the Associative Long-Ter...   \n",
       "3  1000  Bayesian Query Construction for Neural Network...   \n",
       "4  1001  Neural Network Ensembles  Cross Validation  an...   \n",
       "\n",
       "                                     processed_title  \n",
       "0  self-organization of associative database and ...  \n",
       "1  a mean field theory of layer iv of visual cort...  \n",
       "2  storing covariance by the associative long-ter...  \n",
       "3  bayesian query construction for neural network...  \n",
       "4  neural network ensembles  cross validation  an...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers['processed_title'] = papers['title'].map(lambda x:x.lower())\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1663379952035,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "C1Jps_plKzEk",
    "outputId": "f974b858-4b20-4111-be22-52f97d5c2375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n",
      "['self-organization of associative database and its applications', 'a mean field theory of layer iv of visual cortex and its application to artificial neural networks', 'storing covariance by the associative long-term potentiation and depression of synaptic strengths in the hippocampus', 'bayesian query construction for neural network models', 'neural network ensembles  cross validation  and active learning', 'using a neural net to instantiate a deformable model', 'plasticity-mediated competitive learning', 'iceg morphology classification using an analogue vlsi neural network', 'real-time control of a tokamak plasma using neural networks', 'pulsestream synapses with non-volatile analogue amorphous-silicon memories']\n"
     ]
    }
   ],
   "source": [
    "titles = list(papers['processed_title'])\n",
    "print(len(titles))\n",
    "print(titles[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaslX_ahJSIS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bubDmFwzTGPL"
   },
   "source": [
    "### 3. Construct features using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKano_fZLG_3"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english',use_idf=True, ngram_range=(1,1))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(titles).toarray()\n",
    "print(tfidf_matrix.shape)\n",
    "\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_matrix[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij3NTbtqTRjl"
   },
   "source": [
    "### 4. Run clustering and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1860,
     "status": "ok",
     "timestamp": 1663380598301,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "_aYxSAHMMtRT",
    "outputId": "d1302729-5705-421e-8c62-3412ae6db8be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04505905414792544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "k = 30\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "# evaluation\n",
    "print(metrics.silhouette_score(tfidf_matrix, km.labels_, sample_size=100, random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1663380633661,
     "user": {
      "displayName": "Kunpeng Zhang",
      "userId": "09274433828486852799"
     },
     "user_tz": 240
    },
    "id": "MLxw62sdNdpu",
    "outputId": "204bb2ec-03b6-4478-e94f-561ed1bd0483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 19, 3, 17, 17, 20, 2, 14, 6, 10, 2, 5, 6, 13, 9, 9, 14, 24, 19, 17, 27, 9, 2, 20, 18, 20, 11, 6, 0, 0, 20, 10, 28, 0, 22, 25, 5, 0, 15, 6, 2, 20, 22, 6, 1, 11, 24, 29, 0, 2, 1, 2, 15, 4, 18, 8, 6, 18, 0, 6, 16, 0, 13, 25, 6, 0, 0, 9, 25, 19, 1, 2, 9, 5, 22, 6, 18, 17, 2, 17, 28, 20, 6, 2, 15, 0, 2, 26, 0, 15, 20, 23, 2, 26, 26, 6, 17, 2, 6, 19, 2, 14, 6, 23, 5, 0, 0, 25, 5, 16, 25, 6, 25, 18, 0, 6, 0, 0, 19, 9, 0, 0, 2, 6, 27, 20, 12, 9, 13, 0, 19, 0, 6, 1, 19, 25, 28, 10, 19, 4, 6, 2, 15, 0, 11, 13, 25, 9, 2, 0, 21, 0, 0, 28, 0, 2, 7, 20, 0, 9, 20, 3, 27, 27, 24, 25, 2, 27, 27, 13, 15, 27, 25, 10, 0, 17, 0, 27, 14, 6, 6, 2, 7, 20, 22, 15, 2, 17, 5, 0, 10, 0, 17, 0, 6, 1, 0, 2, 14, 0, 0, 25, 27, 2, 21, 12, 15, 13, 17, 28, 22, 0, 11, 0, 21, 27, 27, 25, 17, 6, 20, 15, 17, 26, 26, 17, 25, 4, 24, 5, 0, 0, 24, 22, 28, 4, 13, 29, 2, 9, 13, 0, 28, 0, 2, 1, 9, 2, 0, 23, 21, 23, 2, 2, 29, 0, 8, 13, 13, 0, 9, 12, 6, 9, 0, 14, 2, 6, 15, 20, 0, 1, 4, 17, 17, 5, 6, 12, 5, 15, 2, 25, 17, 27, 2, 0, 27, 1, 9, 9, 10, 7, 25, 6, 2, 19, 0, 0, 22, 2, 5, 6, 17, 15, 2, 7, 3, 14, 17, 21, 20, 0, 27, 4, 4, 0, 13, 6, 1, 13, 0, 16, 15, 24, 20, 29, 0, 29, 6, 26, 0, 0, 27, 1, 15, 26, 0, 0, 22, 4, 17, 14, 0, 5, 27, 23, 9, 1, 0, 0, 19, 17, 1, 4, 27, 0, 0, 5, 11, 0, 24, 29, 15, 13, 0, 2, 0, 25, 27, 4, 1, 7, 6, 0, 27, 0, 0, 0, 19, 2, 2, 26, 2, 0, 9, 11, 17, 28, 19, 17, 15, 14, 27, 5, 0, 25, 6, 9, 5, 4, 20, 0, 4, 0, 27, 3, 6, 1, 12, 29, 0, 14, 14, 19, 0, 0, 23, 16, 20, 2, 2, 2, 0, 2, 0, 0, 16, 2, 27, 15, 27, 11, 23, 0, 6, 13, 0, 0, 6, 2, 10, 10, 8, 1, 2, 11, 15, 0, 23, 0, 17, 20, 20, 19, 0, 15, 20, 15, 19, 13, 0, 0, 9, 22, 8, 0, 23, 18, 0, 29, 5, 11, 10, 2, 0, 16, 2, 21, 0, 9, 27, 0, 4, 29, 6, 0, 24, 8, 16, 0, 13, 11, 19, 20, 22, 19, 7, 3, 1, 12, 0, 6, 14, 24, 6, 10, 9, 2, 25, 23, 1, 0, 19, 17, 0, 15, 7, 0, 14, 26, 2, 21, 29, 5, 25, 27, 11, 15, 0, 0, 12, 25, 13, 20, 0, 24, 20, 0, 20, 0, 1, 0, 0, 8, 18, 4, 3, 21, 25, 8, 8, 5, 0, 20, 6, 13, 11, 9, 27, 21, 2, 0, 15, 0, 27, 0, 17, 1, 0, 1, 9, 13, 16, 0, 8, 0, 28, 1, 17, 0, 8, 0, 8, 20, 4, 1, 27, 13, 0, 9, 0, 0, 3, 22, 2, 5, 23, 0, 20, 0, 22, 24, 13, 2, 16, 19, 3, 4, 9, 2, 25, 27, 17, 9, 5, 17, 12, 27, 2, 5, 0, 27, 17, 1, 14, 21, 0, 0, 12, 2, 12, 0, 8, 6, 5, 14, 12, 2, 15, 9, 2, 6, 9, 2, 2, 22, 0, 14, 13, 13, 2, 5, 17, 2, 17, 27, 1, 0, 6, 9, 19, 27, 0, 0, 2, 12, 17, 0, 1, 28, 0, 13, 0, 28, 9, 15, 0, 6, 0, 14, 8, 0, 28, 21, 0, 13, 0, 2, 9, 2, 7, 13, 1, 0, 0, 23, 22, 4, 23, 1, 22, 9, 2, 8, 20, 9, 9, 21, 23, 7, 24, 7, 21, 28, 0, 21, 21, 20, 21, 2, 9, 4, 5, 11, 18, 12, 0, 2, 2, 2, 2, 10, 6, 0, 5, 22, 28, 20, 18, 0, 21, 21, 5, 1, 28, 28, 7, 0, 22, 15, 0, 26, 0, 4, 10, 0, 6, 0, 14, 20, 11, 2, 22, 0, 13, 2, 11, 0, 0, 18, 20, 5, 15, 0, 22, 1, 2, 12, 22, 2, 5, 14, 22, 12, 13, 12, 1, 4, 2, 12, 12, 0, 0, 9, 13, 19, 0, 18, 19, 0, 29, 0, 20, 0, 23, 0, 9, 7, 11, 2, 6, 0, 0, 4, 8, 0, 0, 28, 0, 2, 1, 15, 20, 8, 9, 29, 22, 6, 21, 0, 0, 9, 0, 6, 2, 0, 8, 17, 12, 17, 2, 12, 0, 14, 0, 2, 2, 1, 9, 5, 20, 0, 4, 0, 0, 7, 0, 0, 11, 20, 14, 6, 0, 23, 0, 27, 11, 28, 0, 29, 27, 13, 3, 17, 2, 2, 29, 11, 22, 6, 19, 7, 2, 6, 27, 28, 0, 22, 0, 12, 27, 0, 0, 0, 8, 6, 11, 13, 0, 12, 12, 8, 21, 16, 27, 27, 0, 2, 2, 1, 0, 20, 0, 9, 2, 9, 20, 0, 0, 2, 20, 25, 22, 16, 20, 0, 2, 29, 6, 0, 15, 0, 0, 0, 12, 0, 8, 11, 27, 6, 10, 0, 29, 21, 14, 20, 0, 0, 29, 14, 6, 0, 0, 0, 0, 2, 25, 0, 11, 0, 0, 16, 8, 0, 21, 2, 0, 0, 0, 12, 2, 25, 24, 9, 14, 9, 8, 6, 18, 5, 9, 15, 0, 19, 0, 8, 18, 28, 3, 3, 10, 0, 22, 11, 16, 29, 19, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.tolist())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
