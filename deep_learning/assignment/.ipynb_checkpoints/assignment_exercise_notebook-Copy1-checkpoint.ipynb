{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKKJxfEj6o1C"
   },
   "outputs": [],
   "source": [
    "# Use this cell to import all the required packages and methods\n",
    "import typing\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import load_img, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TRhX8p81jZZ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lh8skyqy7Qb6"
   },
   "outputs": [],
   "source": [
    "# Use this cell to define a function that loads images and their labels\n",
    "\n",
    "# Define a function that loads images and their labels\n",
    "def load_data(mainfolder: str = \"weather\"):\n",
    "    '''\n",
    "    Loads the images from the main folder in a list and creates a list\n",
    "    containing labels for the images\n",
    "\n",
    "    Args:\n",
    "        mainfolder (str): A string that describes the address of the parent data\n",
    "            folder in the memory\n",
    "    \n",
    "    Returns:\n",
    "        list_of_images (PIL): A list containing PIL Image instances for each image\n",
    "        image_labels (List[str]): A list containing their respective labels in the form \n",
    "            of a string\n",
    "    '''\n",
    "    list_of_images = []\n",
    "    image_labels: typing.List[str] = []\n",
    "    \n",
    "    for dir in os.listdir(mainfolder):\n",
    "        for imgs in os.listdir(mainfolder+'/'+dir):\n",
    "            list_of_images.append(load_img(mainfolder+\"/\"+dir+'/'+imgs))\n",
    "            image_labels.append(dir)\n",
    "    \n",
    "    return list_of_images, image_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRjwNJj67LP_"
   },
   "source": [
    "Finally, use the *load_data()* function to load the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkeH5Ba17Qmv"
   },
   "outputs": [],
   "source": [
    "# Use this cell to load the training and testing data sets and store them in the appropriate variables\n",
    "\n",
    "# Load the training data\n",
    "X_train, y_train = load_data('weather/train')\n",
    "\n",
    "# Load the testing data\n",
    "X_test, y_test = load_data('weather/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for img, label in zip(X_train, y_train):\n",
    "    print(img, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFs4KGYsFFoi"
   },
   "source": [
    "## Task 3 - Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame(zip(X_train, y_train), columns=['image', 'label'])\n",
    "test_dataset = pd.DataFrame(zip(X_test, y_test), columns=['image', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtOjjNY-7o3_"
   },
   "outputs": [],
   "source": [
    "train_sample_each_category = train_dataset.groupby(by=['label']).sample(1)\n",
    "test_sample_each_category = test_dataset.groupby(by=['label']).sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!explorer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCGMWY557tFf"
   },
   "outputs": [],
   "source": [
    "# Use this cell to view a few images from the testing data set for each class\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 20), layout='constrained')\n",
    "count = 0\n",
    "for img, label in zip(test_sample_each_category['image'], test_sample_each_category['label']):\n",
    "    ax[count].set_title(label, fontsize='24')\n",
    "    ax[count].imshow(img)\n",
    "    ax[count].set_xlabel(f\"Size of Image is: {img.size}\", fontsize=18)\n",
    "    ax[count].grid(alpha=0)\n",
    "    count += 1\n",
    "fig.savefig('classwise_images.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgFwMv1-7yvI"
   },
   "source": [
    "Print the number of training and testing data points available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gq6kfIl7fjJ"
   },
   "outputs": [],
   "source": [
    "# Use this cell to inspect the number of training and testing data points available\n",
    "\n",
    "print(f\"Length of trainig data: {train_dataset['image'].count()}\")\n",
    "print(f\"Length of test data: {test_dataset['image'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x=['Training', 'Testing'], y=[train_dataset['image'].count(), test_dataset['image'].count()])\n",
    "ax.set_title(\"Length of datasets\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.savefig('dataset_len.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9EwlEKf4VSx"
   },
   "source": [
    "Store the number of classes in this classification exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZvQ7qt84QsQ"
   },
   "outputs": [],
   "source": [
    "# Use this cell to extract and store the total class count\n",
    "\n",
    "# Extract and store the total class count\n",
    "n_classes =  np.unique(y_test).shape[0]\n",
    "print('Number of classes =', n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhR9fFwoyo2u"
   },
   "source": [
    "visualze the class balance in the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsq9lAHR9bF9"
   },
   "outputs": [],
   "source": [
    "# Use this cell to visualize the class balance in the training data set\n",
    "ax = sns.barplot(train_dataset.groupby(by='label').count().reset_index(), x='label', y='image')\n",
    "ax.set_title(\"Class Balance in training Dataset\")\n",
    "ax.bar_label(ax.containers[0], fontsize=10)\n",
    "plt.savefig(\"class_balance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_sizes_df = pd.DataFrame()\n",
    "testing_image_sizes_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_sizes_df['width'], training_image_sizes_df['height'], training_image_sizes_df['aspect_ratio']  = zip(*train_dataset['image'].apply(lambda x: (x.size[0], x.size[1], round(x.size[0] / x.size[1], 2))))\n",
    "testing_image_sizes_df['width'], testing_image_sizes_df['height'], testing_image_sizes_df['aspect_ratio'] = zip(*test_dataset['image'].apply(lambda x: (x.size[0], x.size[1], round(x.size[0] / x.size[1], 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "points = ax[0].scatter(training_image_sizes_df['width'], training_image_sizes_df['height'], alpha=0.5, s=training_image_sizes_df['aspect_ratio']*100, picker=True)\n",
    "ax[0].set_title(\"Image Resolution\")\n",
    "ax[0].set_xlabel(\"Width\", size=14)\n",
    "ax[0].set_ylabel(\"Height\", size=14)\n",
    "\n",
    "points = ax[1].scatter(training_image_sizes_df['width'], training_image_sizes_df['height'], alpha=0.5, s=training_image_sizes_df['aspect_ratio']*100, picker=True)\n",
    "ax[1].set_title(\"Zoomed to 1400 x 1400 resolution\")\n",
    "ax[1].set_xlabel(\"Width\", size=14)\n",
    "ax[1].set_ylabel(\"Height\", size=14)\n",
    "ax[1].set_ylim(0, 800)\n",
    "ax[1].set_xlim(0, 1400)\n",
    "\n",
    "points = ax[2].scatter(training_image_sizes_df['width'], training_image_sizes_df['height'], alpha=0.5, s=training_image_sizes_df['aspect_ratio']*100, picker=True)\n",
    "ax[2].set_title(\"Zoomed to 250 x 500 resolution\")\n",
    "ax[2].set_xlabel(\"Width\", size=14)\n",
    "ax[2].set_ylabel(\"Height\", size=14)\n",
    "ax[2].set_ylim(100, 250)\n",
    "ax[2].set_xlim(0, 500)\n",
    "\n",
    "plt.savefig(\"image_dimensions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pN32d_-e-S7"
   },
   "source": [
    "# Stage 2 - Data Preparation\n",
    "In this stage, you will perform some basic essential data preparation methods on your image data so that they are ready for use in CNNs. \n",
    "\n",
    "To prepare the data, you will first need to resize the images in the training and testing data sets to the same size. This can be done using some methods provided to us in the *tensorflow* library. Second, you will convert the data into a format that is suitable to be fed into a Keras CNN model.\n",
    "\n",
    "You will achieve this by completing the following tasks:\n",
    "- Task 4 - Resize all images to the same dimension\n",
    "- Task 5 - Prepare the data for feeding into CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gdR-anNzc9en"
   },
   "source": [
    "## Task 4 - Resize all images to the same dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJLixTd_9LVS"
   },
   "outputs": [],
   "source": [
    "# Use this cell to define a function that resizes image dimensions\n",
    "\n",
    "# Define a function that resizes image dimensions\n",
    "def resize_images(input_images: typing.List[PIL] | np.array = [], new_dims: typing.Tuple[int, int] = 200, col_name: str = None):\n",
    "    '''\n",
    "    Resizes all the images in a list to a square with its side given.\n",
    "\n",
    "    Args:\n",
    "        input_images: A list of images to be resized. Each input image \n",
    "            must be a PIL image.\n",
    "        new_dims: An integer specifying the desired dimensions of the output \n",
    "            images. Each output image will have the same height and width.\n",
    "        col_name (str): Column name which consists the images in dataset.\n",
    "            (use when pass an dataframe as input_images)\n",
    "\n",
    "    Returns:\n",
    "        resized_images: A list of resized images. Each output image is a PIL \n",
    "            image with the specified dimensions.\n",
    "    '''\n",
    "\n",
    "    # Type checker for the input images.\n",
    "\n",
    "    resized_images = [array_to_img(tf.image.resize(img_to_array(x), new_dims)) for x in input_images]\n",
    "    return resized_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5-u1kiICkTm"
   },
   "source": [
    "Next, decide on a new image dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HJlPTT4O3wF"
   },
   "outputs": [],
   "source": [
    "# Use this cell to set the new dimensions for all images\n",
    "\n",
    "# Set the new dimensions for all images\n",
    "new_image_dims = (64, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aR9F29z3U151"
   },
   "source": [
    "Use the *resize_images()* method to resize your training and testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vg-Myu4bc9eo"
   },
   "outputs": [],
   "source": [
    "# Use this cell to standardize the dimensions for all images in the training and testing data sets\n",
    "\n",
    "# Standardize the dimensions for all images in the training data set\n",
    "X_train = np.array(resize_images(X_train, new_image_dims))\n",
    "\n",
    "# Standardize the dimensions for all images in the testing data set\n",
    "X_test = np.array(resize_images(X_test, new_image_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pUTayLDEZFMA"
   },
   "source": [
    "View some images from the resized data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.DataFrame(zip(X_train, y_train), columns=['image', 'label'])\n",
    "test_dataset = pd.DataFrame(zip(X_test, y_test), columns=['image', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCGMWY557tFf"
   },
   "outputs": [],
   "source": [
    "# Use this cell to view a few images from the testing data set for each class\n",
    "test_sample_each_category = test_dataset.groupby(by=['label']).sample(1)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 20), layout='constrained')\n",
    "fig.text(0.43, 0.7, \"Test Dataset Images\", fontsize=26)\n",
    "count = 0\n",
    "for img, label in zip(test_sample_each_category['image'], test_sample_each_category['label']):\n",
    "    ax[count].set_title(label, fontsize='24')\n",
    "    ax[count].imshow(img)\n",
    "    ax[count].set_xlabel(f\"Size of Image is: {img.size}\", fontsize=18)\n",
    "    ax[count].grid(alpha=0)\n",
    "    count += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_each_category = train_dataset.groupby(by=['label']).sample(1)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 20), layout='constrained')\n",
    "# fig.text(0.43, 0.7, \"Training Dataset Images\", fontsize=26)\n",
    "count = 0\n",
    "for img, label in zip(train_sample_each_category['image'], train_sample_each_category['label']):\n",
    "    ax[count].set_title(label, fontsize='24')\n",
    "    ax[count].imshow(img)\n",
    "    ax[count].set_xlabel(f\"Size of Image is: {img.size}\", fontsize=18)\n",
    "    ax[count].grid(alpha=0)\n",
    "    count += 1\n",
    "plt.savefig('resized_imgs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf_FjNUvZK1w"
   },
   "source": [
    "Finally, save the resized data set into variables that will be useful later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-qehKcMvESUI"
   },
   "outputs": [],
   "source": [
    "# Use this cell to store the resized images and labels for later use\n",
    "\n",
    "# Store the resized images for later use\n",
    "X_pre_NN = X_train\n",
    "\n",
    "# Store the labels for later use\n",
    "y_pre_NN = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_NN_df = pd.DataFrame(zip(X_pre_NN, y_pre_NN), columns=['image', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkI6k7yUaOd9"
   },
   "source": [
    "## Task 5 - Prepare the data for feeding into CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDfemRwUdme2"
   },
   "source": [
    "### Description\n",
    "\n",
    "In this section, you will perform a set of data preparation steps that enables CNNs to work on the data. Currently, *X_train* and *X_test* are lists of images, whereas *y_train* and *y_test* are lists of strings. Keras models can accept data in the form of *numpy* arrays. Therefore, you will convert the data into arrays.\n",
    "\n",
    "For the input data, you need to convert the list of images into an *numpy* array. Note that the pixel gray levels are currently in the range $[0, 255]$. You will rescale these to $[0, 1]$. This is a common pre-processing step in image classification tasks, as it helps the model to converge faster during training.\n",
    "\n",
    "For the output data, you will need to encode the different labels as one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([img_to_array(x) / 255 for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([img_to_array(x) / 255 for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwDnOozKZyi-"
   },
   "source": [
    "Now convert the image labels to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Cloudy', 'Sunny', 'Rainy']\n",
    "encoded_classes = [list(y) for y in set([tuple(x) for x in y_train])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v for k, v in zip(classes, encoded_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_encoded = label_encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test_encoded, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_img(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60auiw63W5u3"
   },
   "source": [
    "Finally, save the input dimensions for the CNNs in a variable so that you can call or refer to it when building CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1], X_train.shape[2], X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qse2vmKcO3wH"
   },
   "outputs": [],
   "source": [
    "# Use this cell to store the input dimensions for the CNNs\n",
    "\n",
    "# Store the input dimensions for the CNNs\n",
    "inputdims = X_train.shape[1], X_train.shape[2], X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqeEvgNrhgx9"
   },
   "source": [
    "# Stage 3 - Simple Model\n",
    "In this stage, you will build, train and evaluate a basic CNN model on the data and analyze its performance. You will build the model using a function. This will help you change the configuration of your model during execution. You will also test the performance of the performance of the model by training it multiple times and then judging the distribution of final accuracies and loss values.\n",
    "\n",
    "You will do all this with the help of the following tasks:\n",
    "- Task 6 - Define a function to build a CNN model\n",
    "- Task 7 - Create a simple CNN model and analyze its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dXVboDq56UjB"
   },
   "source": [
    "## Task 6 - Define a function to build a CNN model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7L3Mxot0JKW"
   },
   "source": [
    "Define the *create_cnn()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7GkwZu23cC5"
   },
   "outputs": [],
   "source": [
    "# Use this cell to define a function that creates and compiles a CNN\n",
    "\n",
    "# Define a function that creates and compiles a CNN\n",
    "def create_cnn(layers_config: typing.List[str], input_dims: typing.Tuple[int],  num_classes: int, learning_rate_value: float = 0.01):\n",
    "    '''\n",
    "    Creates and compiles a convolutional neural network (CNN) with the specified layers configuration and learning rate.\n",
    "\n",
    "    Args:\n",
    "        layers_config (List[str]): A list of strings specifying the configuration of each layer in the CNN. \n",
    "        input_dims (tuple): Tuple specifying the dimensions of inputs for the models to handle.\n",
    "        num_classes: Number of classes to classify in the output.\n",
    "        learning_rate_value (float): A float specifying the learning rate to be used by the optimizer during training.\n",
    "    \n",
    "    Returns:\n",
    "        cnn: A compiled CNN with the specified layers configuration and learning rate.\n",
    "    '''\n",
    "\n",
    "    # Initializing Sequential model.\n",
    "    cnn = Sequential()\n",
    "\n",
    "    # Add Input layer with the provided input dimensions.\n",
    "    cnn.add(Input(shape=input_dims))\n",
    "\n",
    "    # Iterate over layers_config to add layers to the model.\n",
    "    for layer_str in layers_config:\n",
    "        # Extract layer type and parameters.\n",
    "        layer_type, *params = layer_str.split('_')\n",
    "\n",
    "        if layer_type == 'c': # Conv2D layer.\n",
    "            filters, kernel_size = map(int, params)\n",
    "            cnn.add(Conv2D(filters=filters, kernel_size=kernel_size, activation='relu', padding='same'))\n",
    "        elif layer_type == 'm': # MaxPooling2D layer.\n",
    "            pool_size = int(params[0])\n",
    "            cnn.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "        elif layer_type == 'f': # flatten Layer.\n",
    "            cnn.add(Flatten())\n",
    "        elif layer_type == 'd': # dense layer.\n",
    "            units = int(params[0])\n",
    "            cnn.add(Dense(units=units, activation='relu'))\n",
    "\n",
    "    cnn.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    cnn.compile(optimizer=Adam(learning_rate=learning_rate_value),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glspmB89jv5g"
   },
   "source": [
    "### Checklist\n",
    "- Defined a function *create_cnn()* with given keyword arguments and return variables\n",
    "- Followed the recommended specifications in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeuiBG-E0XvC"
   },
   "source": [
    "## Task 7 - Create a simple CNN model and analyze its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYBp4v4jk4m7"
   },
   "source": [
    "### Description\n",
    "\n",
    "In this task, you will create a simple CNN model and train it on the training data multiple times and record its performance in each training iteration. You will then analyze the model's performance by summarizing its performance over the various training trials.\n",
    "\n",
    "The reason why we want to train it multiple times is that your model may not train the same way each time. To get a good understanding of a model's performance, you need to train it a few times under the same settings and conditions and record its performance each time. You can then judge the model based on the distribution of its performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GXCM9dTvFVn"
   },
   "source": [
    "First, save the number of trials in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg8mm9PRO3wH"
   },
   "outputs": [],
   "source": [
    "# Use this cell to set the number of trials for each model training instance\n",
    "\n",
    "# Set the number of trials for each model training instance\n",
    "num_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKhQxi_7vJUC"
   },
   "source": [
    "Now save the number of epochs and the validation split in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eEWn-q27PKS"
   },
   "outputs": [],
   "source": [
    "# Use this cell to set the number of CNN training epochs and the validation split fraction\n",
    "\n",
    "# Set the number of epochs for CNN training\n",
    "n_epochs = 10\n",
    "\n",
    "# Set the validation split fraction\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf7BbRh_vR44"
   },
   "source": [
    "Finally, create train and evaluate your CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXfL16gkO3wI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to create, train and evaluate your simple CNN model on the data multiple times and store and view the performance results\n",
    "training_hist = [None] * num_trials\n",
    "performance_df = pd.DataFrame()\n",
    "\n",
    "layers_config = ['c_2_3', 'm_2', 'c_4_3', 'm_2', 'f', 'd_8']\n",
    "learning_rate_value = 0.01\n",
    "\n",
    "for i in range(0, num_trials):\n",
    "    results = []\n",
    "    cnn = create_cnn(layers_config=layers_config, input_dims=inputdims, num_classes=num_classes, learning_rate_value=learning_rate_value)\n",
    "\n",
    "    print(f\"Training iteration {i}\")\n",
    "    cnn.summary()\n",
    "    print('\\n')\n",
    "    cnn_history = cnn.fit(X_train, y_train, validation_split=val_split, epochs=n_epochs)\n",
    "\n",
    "    training_hist[i] = pd.DataFrame(cnn_history.history)\n",
    "    training_hist[i]['epoch'] = cnn_history.epoch\n",
    "\n",
    "performance_df = reduce(lambda left, right: pd.merge(left, right, how='outer'), training_hist)\n",
    "performance_df.loc['Mean'] = performance_df.mean()\n",
    "performance_df.loc['Median'] = performance_df.median()\n",
    "performance_df.loc['Max'] = performance_df.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'epoch', y = 'accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Iteration');\n",
    "plt.savefig(\"simple_model_result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJkDhj8CvZ4q"
   },
   "source": [
    "### Checklist\n",
    "- Saved values for number of trials, number of epochs, and validation split in variables\n",
    "- Created and trained a simple CNN model multiple times and evaluated its performance\n",
    "- Displayed the model's performance over multiple trials and the summary statistics in the given format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQynJ6lDpREI"
   },
   "source": [
    "# Stage 4 - Data Augmentation\n",
    "Data augmentation is a technique that is commonly used in deep learning to artificially increase the size of the training data set.\n",
    "\n",
    "You can add augmented images to your your training data to improve the class balance in the training data set and also to increase the size of the training data.\n",
    "\n",
    "In this stage, you will augment your training data to increase the training data size and improve the class balance in the training data set. You will then retrain your basic CNN model on the augmented data set and analyze its performance over multiple training trials.\n",
    "\n",
    "In the first three tasks of this stage, you will write functions and helper functions to perform data augmentation. Whereas in the fourth task, you will use those functions to actually perform data augmentation. The tasks are given below:\n",
    "- Task 8 - Create a transformed image\n",
    "- Task 9 - Divide the data according to class\n",
    "- Task 10 - Augment the data\n",
    "- Task 11 - Create a simple CNN model using the augmented data and analyze its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdEpg1NfpWY4"
   },
   "source": [
    "## Task 8 - Create a transformed image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1fvMqNGog0A"
   },
   "source": [
    "### Description\n",
    "In the case of image classification, data augmentation is done by applying random transformations to the images in the training data set, such as rotation, flipping, and zooming. By doing so, the model is exposed to a greater variety of training samples, which can help improve its ability to generalize to unseen data.\n",
    "\n",
    "In this task, you will define a function to randomly transform a given image using one of the predefined transformations. This function will be used later to augment the data. \n",
    "\n",
    "But recall that we performed some data preparation steps on our data set in Task 5. Recall also that we saved our data in two variables *X_pre_NN* and *y_pre_NN*. We will perform data augmentation using this unprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAPTbXXa25lM"
   },
   "source": [
    "Retrieve the training data that you saved in an earlier stage before conducting data preparation on it for feeding into CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "St7MoE360F22"
   },
   "outputs": [],
   "source": [
    "# Use this cell to retrieve the training data that you saved in an earlier stage\n",
    "\n",
    "# Retrieve the training images\n",
    "X_train = X_pre_NN\n",
    "\n",
    "# Retrieve the training labels\n",
    "y_train = y_pre_NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGzMefV9EFlP"
   },
   "source": [
    "Now, define the function *random_transform()* that takes in an image and creates a new image from it using a random transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "558GTVv3LvmI"
   },
   "outputs": [],
   "source": [
    "# Use this cell to define a function that takes in an input image and creates a new image out of it using some random augmentation\n",
    "\n",
    "# Define a function that takes in an input image and creates a new image out of it using some random augmentation\n",
    "def random_transform(input_image):\n",
    "    \"\"\"\n",
    "    Takes in an input image and creates a new image out of it using a  \n",
    "        random transformation.\n",
    "    Args:\n",
    "        input_image: A PIL image object.\n",
    "\n",
    "    Returns:\n",
    "        output_image: A PIL image object, the augmented image.\n",
    "    \"\"\"\n",
    "\n",
    "    # convert PIL image to numpy array.\n",
    "    input_array = img_to_array(input_image)\n",
    "\n",
    "    # Randomly choose transformation.\n",
    "    transformation = np.random.choice([\n",
    "        'flip_left_right',\n",
    "        'flip_up_down',\n",
    "        'rot90'\n",
    "    ])\n",
    "\n",
    "    if transformation == 'flip_left_right':\n",
    "        print(\"Flipping the image left to right.\")\n",
    "        output_array = tf.image.flip_left_right(input_array)\n",
    "    elif transformation == 'flip_up_down':\n",
    "        print(\"Flipping the image up to down.\")\n",
    "        output_array = tf.image.flip_up_down(input_array)\n",
    "    elif transformation == 'rot90':\n",
    "        k = np.random.randint(1, 4)  # Randomly choose rotation angle (1, 2, or 3)\n",
    "        print(\"Rotating the image\")\n",
    "        output_array = tf.image.rot90(input_array, k)\n",
    "\n",
    "    # convert numpy array back to PIL Image.\n",
    "    output_image = array_to_img(output_array)\n",
    "\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBx7VuFXbWQB"
   },
   "source": [
    "You can now test your function on any input image and view the results. Note that if you defined your function correctly, each time you run the following cell, a random augmentation would be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JJmihXm5uk4"
   },
   "outputs": [],
   "source": [
    "# Use this cell to apply the \"random_transform()\" function on an image and view the results\n",
    "# Note: This is a sample execution and the actual augmentation function will be defined in Task 10\n",
    "\n",
    "random_transform(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 10), layout='constrained')\n",
    "# fig.text(0.43, 0.7, \"Training Dataset Images\", fontsize=26)\n",
    "count = 0\n",
    "for img in [random_transform(X_train[0]), random_transform(X_train[1]), random_transform(X_train[2])]:\n",
    "    ax[count].imshow(img)\n",
    "    ax[count].set_xlabel(f\"Size of Image is: {img.size}\", fontsize=18)\n",
    "    ax[count].grid(alpha=0)\n",
    "    count += 1\n",
    "plt.savefig('transformed_imgs.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGbmv0fJ4-KC"
   },
   "source": [
    "### Checklist\n",
    "- Retrieved data stored in *X_pre_NN* and *y_pre_NN*\n",
    "- Defined the *random_transform()* function using the keyword arguments and return variables described above\n",
    "- Experimented with the *random_transform()* function and verified that the function randomly augments an input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tUurp187z5G"
   },
   "source": [
    "## Task 9 - Divide the data by class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8EhsEIEC7w_"
   },
   "source": [
    "Visualize the class balance in the training data set using a bar plot or a count plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(X_train, y_train), columns=['image', 'label']).groupby(by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwL1ZzK2pjJa"
   },
   "outputs": [],
   "source": [
    "# Use this cell to visualize the class balance in the training data set\n",
    "ax = sns.barplot(train_df.reset_index(), x='label', y='image')\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxUyqaGHDPQi"
   },
   "source": [
    "You can see that the classes in the training data are quite imbalanced. Moreover, the number of training samples is quite small as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zj0c9MIGDVsG"
   },
   "source": [
    "Now, define the function *divide_data_by_class()* that divides the input images and labels into subsets based on their corresponding class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCvJCnDl2k74"
   },
   "outputs": [],
   "source": [
    "# Use this cell to define a function that takes in the list of training images and labels and returns them class-wise\n",
    "\n",
    "# Define a function that takes in the list of training images and returns them class-wise\n",
    "def divide_data_by_class(input_images, image_labels):\n",
    "    '''\n",
    "    Divides the input images and labels into subsets based on their corresponding class labels.\n",
    "\n",
    "    Args:\n",
    "        input_images: A list of input images to be divided based on class \n",
    "            labels. Each input image must be a PIL image.\n",
    "        image_labels: A list that contains corresponding labels for each \n",
    "            image in input_images. Each label is a string.\n",
    "\n",
    "    Returns:\n",
    "        classwise_images: A list of lists, where each sublist contains the \n",
    "            input images corresponding to a unique class label.\n",
    "        classwise_labels: A list of lists, where each sublist contains the \n",
    "            labels corresponding to the input images in classwise_images list.\n",
    "    '''\n",
    "\n",
    "   # Initialize dictionaries to store images and labels by class\n",
    "    classwise_images = {}\n",
    "    classwise_labels = {}\n",
    "\n",
    "    # Iterate over input images and labels\n",
    "    for image, label in zip(input_images, image_labels):\n",
    "        # Check if class label already exists in dictionaries\n",
    "        if label not in classwise_images:\n",
    "            classwise_images[label] = []\n",
    "            classwise_labels[label] = []\n",
    "        \n",
    "        # Add image and label to respective class\n",
    "        classwise_images[label].append(image)\n",
    "        classwise_labels[label].append(label)\n",
    "\n",
    "    # Convert dictionaries to lists of lists\n",
    "    classwise_images = list(classwise_images.values())\n",
    "    classwise_labels = list(classwise_labels.values())\n",
    "\n",
    "    return classwise_images, classwise_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R31r7dsNc3Oz"
   },
   "source": [
    "You can now use the *divide_data_by_class()* function on the data set and view the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1J4w3y69Fsw"
   },
   "source": [
    "### Checklist\n",
    "- Visualized class balance in the data set\n",
    "- The *divide_data_by_class()* function returns two lists that contain three lists each\n",
    "- Experimented with the *divide_data_by_class()* function and ensured that it is functioning properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9vahsAL-MbY"
   },
   "source": [
    "## Task 10 - Augment the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oxMJJE0-QYW"
   },
   "source": [
    "### Description\n",
    "\n",
    "With these two functions, *random_transform()* and *divide_data_by_class()* in hand, you can define the main augmentation function called *augment_data()*. This function takes in an image data set and the factor by which the data set needs to be augmented. This function should:\n",
    "1. Divide the data set into different classes using the *divide_data_by_class()* function.\n",
    "2. Calculate the final size of each class. This is done by multiplying the factor by the size of the largest class.\n",
    "3. Use the *random_transform()* function to create images till all three classes achieve the required size.\n",
    "\n",
    "After defining the function, try augmenting the data set and visualize the class balance again to verify your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnAm8VCqE6K3"
   },
   "source": [
    "Define the *augment_data()* function that works on the original training data and produces an augmented training data set from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2Y7n9SRuLQB"
   },
   "outputs": [],
   "source": [
    "# Use this cell to define a function that augments the training data\n",
    "\n",
    "# Define a function to augment the training data\n",
    "def augment_data(input_images, image_labels, data_size_factor):\n",
    "    '''\n",
    "    Augments the training data by randomly applying data augmentation techniques.\n",
    "\n",
    "    Args:\n",
    "        input_images: A list of input images.\n",
    "        image_labels: A list of labels for the input images.\n",
    "        data_size_factor: A scaling factor for the size of the augmented data.\n",
    "            This will be used to calculate the final size of each class by \n",
    "            multiplying data_size_factor with the size of the largest class and \n",
    "            then rounding to the nearest integer.\n",
    "\n",
    "    Returns:\n",
    "        new_images: The augmented images\n",
    "        new_labels: The labels corresponding to the new images\n",
    "    '''\n",
    "\n",
    "    # Divide input data by class\n",
    "    classwise_images, classwise_labels = divide_data_by_class(input_images, image_labels)\n",
    "\n",
    "    # Find the size of the largest class\n",
    "    max_class_size = max(len(images) for images in classwise_images)\n",
    "\n",
    "    # Calculate target size for each class after augmentation\n",
    "    target_size = round(max_class_size * data_size_factor)\n",
    "\n",
    "    # Augment data for each class\n",
    "    new_images = []\n",
    "    new_labels = []\n",
    "    for images, labels in zip(classwise_images, classwise_labels):\n",
    "        num_images_to_augment = target_size - len(images)\n",
    "        if num_images_to_augment > 0:\n",
    "            new_images.extend(images)\n",
    "            new_labels.extend(labels)\n",
    "            for _ in range(num_images_to_augment):\n",
    "                # Randomly choose an image to augment\n",
    "                image_to_augment = random.choice(images)\n",
    "                # Apply random transformation\n",
    "                augmented_image = random_transform(image_to_augment)\n",
    "                # Add augmented image and label\n",
    "                new_images.append(augmented_image)\n",
    "                new_labels.append(labels[0])  # Assume all images in the class have the same label\n",
    "    \n",
    "    return new_images, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3sIim7QIblx"
   },
   "source": [
    "Use the *augment_data()* function to augment the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cekPqdEPrlOf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to augment your training data\n",
    "\n",
    "# Augment your training data using the \"augment_data()\" function\n",
    "X_train, y_train = augment_data(X_train, y_train, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OSIFpQDQI5t2"
   },
   "source": [
    "Visualize the class balance in the augmented training data set using a bar plot or a count plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(zip(X_train, y_train), columns=['image', 'label']).groupby(by='label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwL1ZzK2pjJa"
   },
   "outputs": [],
   "source": [
    "# Use this cell to visualize the class balance in the training data set\n",
    "ax = sns.barplot(train_df.reset_index(), x='label', y='image')\n",
    "ax.bar_label(ax.containers[0])\n",
    "ax.set_title(\"Training data length after augmentation.\")\n",
    "plt.savefig(\"trainig_data_after_augmentaiton.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYs5mLbgOs6W"
   },
   "source": [
    "### Checklist\n",
    "- Experimented with the *augment_data()* function and made sure that it is working properly\n",
    "  - For example, if the *augment_data()* function is used on the original training data with a *data_size_factor* of 1, each class in the output data has 285 images, and the total number of images in the new list is 855. If *data_size_factor* was 2 instead, then each class in the output data has 570 images, and the total number images in the new list is 1710.\n",
    "- Decided on a suitable value for the augmentation factor and augmented the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yT19TQ9uO3wK"
   },
   "source": [
    "## Task 11 - Create a simple CNN model using the augmented data and analyze its performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_p-2mT7OdWK"
   },
   "source": [
    "### Description\n",
    "In this task, you will retrain your simple CNN model multiple times using the augmented data and record its performance in each training instance. You will then analyze its performance by summarizing its performance over the multiple training trials.\n",
    "\n",
    "But before you do that, you will need to perform all the data prepration steps that you did earlier so that the data is ready to be fed into CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_image(image_list: typing.List[PIL], new_image_dims: int):\n",
    "    resized_imgs = resize_images(image_list, new_image_dims)\n",
    "\n",
    "    rescaled_imgs = np.array([img_to_array(x) / 255 for x in resized_imgs])\n",
    "\n",
    "    return rescaled_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(image_labels: typing.List[str]):\n",
    "    num_classes: int = len(set(image_labels))\n",
    "\n",
    "    labels = list(set(image_labels))\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    labels_encoded = label_encoder.fit_transform(image_labels)\n",
    "    labels_encoded = to_categorical(labels_encoded, num_classes=num_classes)\n",
    "    return labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pre_process_image(X_train, new_image_dims)\n",
    "y_train = encode_labels(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = pre_process_image(X_test, new_image_dims)\n",
    "# y_test = encode_labels(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg8mm9PRO3wH"
   },
   "outputs": [],
   "source": [
    "# Use this cell to set the number of trials for each model training instance\n",
    "\n",
    "# Set the number of trials for each model training instance\n",
    "num_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKhQxi_7vJUC"
   },
   "source": [
    "Now save the number of epochs and the validation split in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eEWn-q27PKS"
   },
   "outputs": [],
   "source": [
    "# Use this cell to set the number of CNN training epochs and the validation split fraction\n",
    "\n",
    "# Set the number of epochs for CNN training\n",
    "n_epochs = 10\n",
    "\n",
    "# Set the validation split fraction\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nf7BbRh_vR44"
   },
   "source": [
    "Finally, create train and evaluate your CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdims = X_train.shape[1], X_train.shape[2], X_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXfL16gkO3wI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to create, train and evaluate your simple CNN model on the data multiple times and store and view the performance results\n",
    "training_hist = [None] * num_trials\n",
    "performance_df = pd.DataFrame()\n",
    "\n",
    "layers_config = ['c_8_3', 'm_2', 'c_12_3', 'm_2', 'f', 'd_24']\n",
    "learning_rate_value = 0.001\n",
    "\n",
    "for i in range(0, num_trials):\n",
    "    results = []\n",
    "    cnn = create_cnn(layers_config=layers_config, input_dims=inputdims, num_classes=num_classes, learning_rate_value=learning_rate_value)\n",
    "\n",
    "    print(f\"Training iteration {i}\")\n",
    "    cnn.summary()\n",
    "    print('\\n')\n",
    "    cnn_history = cnn.fit(X_train, y_train, validation_split=val_split, epochs=n_epochs)\n",
    "\n",
    "    training_hist[i] = pd.DataFrame(cnn_history.history)\n",
    "    training_hist[i]['epoch'] = cnn_history.epoch\n",
    "\n",
    "performance_df = reduce(lambda left, right: pd.merge(left, right, how='outer'), training_hist)\n",
    "performance_df.loc['Mean'] = performance_df.mean()\n",
    "performance_df.loc['Median'] = performance_df.median()\n",
    "performance_df.loc['Max'] = performance_df.max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'epoch', y = 'accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oILYFrV8SZjI"
   },
   "source": [
    "You should be able to observe, from the performance data frame, that the basic CNN model that is trained on the augmented data performs better, in general, than the basic CNN model that you trained earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQD6QVaLTsH7"
   },
   "source": [
    "### Checklist\n",
    "- Converted the augmented input data into arrays\n",
    "- Scaled the augmented input data\n",
    "- Encoded the output data as integers\n",
    "- Performed one-hot encoding on output data\n",
    "- Retrained the simple CNN model on the augmented data\n",
    "- Created a data frame to analyze the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sMxCklY3_qC"
   },
   "source": [
    "# Stage 5 - Optimal Model\n",
    "In this stage, you will train your CNN model on the augmented data set and tune it for network structure and learning rate. You will do this by completing Task 12 - Tune the CNN Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "426-5HeNO3wL"
   },
   "source": [
    "## Task 12 - Tune the CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCABSJVtYzZ0"
   },
   "source": [
    "### Description\n",
    "\n",
    "In this task, you will train your CNN model on the augmented data set and tune it for network structure and learning rate.\n",
    "\n",
    "A grid search is not recommended here since we are working with sparse data. Instead, it is recommended to use simple loops and record the performance of the various model specifications. This also ensures that the same validation split and consequently the same validation data is used, so the model performance analyses are not biased, at least within the context of this assignment.\n",
    "\n",
    "Additionally note that you are required to restrict the complexity of your model. In general, more complex networks have a higher likelihood of overfitting. As part of this assignment, you are required to make sure that your networks contain no more than 500,000 trainable parameters.\n",
    "\n",
    "Note that you are required to report a performance data frame as you did earlier for each of the model specifications that you consider in this stage. You are free to report these as separate data frames or within a single data frame.\n",
    "\n",
    "For instance, if you consider 2 values for network configuration and 2 values for learning rate, then there are 4 unique model specifications. Consequently, you must train each of these model types multiple times (as specified earlier) and record the performance in each trial. You may, therefore, report 4 separate data frames (whose format you should be familiar with by this stage) or a single data frame that contains all the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqtep-w_a39L"
   },
   "source": [
    "Tune your CNN model for network structure and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1uuakDDfVK0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to tune your CNN model for network structure and learning rate\n",
    "layers_config_list = [\n",
    "    # Small network configurations\n",
    "    ['c_12_3', 'm_2', 'c_14_3', 'm_2', 'f', 'd_24'],\n",
    "    ['c_16_3', 'm_4', 'c_24_3', 'm_2', 'f', 'd_32'],\n",
    "]\n",
    "\n",
    "learning_rate_list = [0.01, 0.001, 0.002]\n",
    "\n",
    "# Initialize dataframe to record performance metrics\n",
    "results_df = pd.DataFrame(columns=['Layers Configuration', 'Learning Rate', 'Train Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "# Define maximum allowable trainable parameters\n",
    "max_trainable_params = 5_00_000\n",
    "\n",
    "# Loop through combinations of network configurations and learning rates\n",
    "for layers_config in layers_config_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "        # Create and compile the model\n",
    "        cnn = create_cnn(layers_config=layers_config, input_dims=inputdims, learning_rate_value=learning_rate, num_classes=num_classes)\n",
    "        cnn.summary()\n",
    "\n",
    "        if cnn.count_params() > max_trainable_params:\n",
    "            print(f\"Max {max_trainable_params} trainable params allowed! Please create a small network.\")\n",
    "            break\n",
    "\n",
    "        cnn_history = cnn.fit(X_train, y_train, epochs=n_epochs, validation_split=val_split)\n",
    "        \n",
    "        \n",
    "        # Record performance metrics\n",
    "        train_accuracy = cnn_history.history['accuracy'][-1]\n",
    "        val_accuracy = cnn_history.history['val_accuracy'][-1]\n",
    "        \n",
    "        # Append results to dataframe\n",
    "        results_df.loc[len(results_df)] = {'Layers Configuration': layers_config,\n",
    "                                        'Learning Rate': learning_rate,\n",
    "                                        'Train Accuracy': train_accuracy,\n",
    "                                        'Validation Accuracy': val_accuracy}\n",
    "                                       \n",
    "\n",
    "# Find optimal values for network config and learning rate\n",
    "optimal_row = results_df.loc[results_df['Validation Accuracy'].idxmax()]\n",
    "optimal_layer_config = optimal_row['Layers Configuration']\n",
    "optimal_learning_rate = optimal_row['Learning Rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_layer_config, optimal_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFmJJd_qNIt2"
   },
   "source": [
    "You should take some time and study the results of your hyperparameter tuning. Once you are satisfied with your analysis, decide on the optimal values of the hyperparameters to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJYh8Z38SwzZ"
   },
   "source": [
    "### Checklist\n",
    "- Decided on the values of hyperparameters over which to tune the model\n",
    "- Trained multiple models for each combination of hyperparameters\n",
    "- The optimal model has good and consistent validation accuracy\n",
    "- The optimal model doesn't have more than 500,000 trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3mjrlJJO3wM"
   },
   "source": [
    "# Stage 6 - Testing\n",
    "In this stage, you will train your optimal model multiple times on the augmented data set until you are satisfied with its performance on the validation data. You will then test your optimal model on the hold-out test data set, which has not been used up until this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYbxPf5hO3wM"
   },
   "source": [
    "## Task 13 - Train your optimal model satisfactorily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8WpOBpX32lB"
   },
   "source": [
    "### Description\n",
    "\n",
    "In this section, you will train your optimal model on the augmented data set multiple times until you are satisfied with its performance on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.90\n",
    "performance_test = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk8XZrRqO3wM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while performance_test[1] < threshold:\n",
    "    # Use this cell to train your optimal model on the augment data set until you are satisfied with its validation performance\n",
    "    optimal_cnn = create_cnn(layers_config=optimal_layer_config, input_dims=inputdims, learning_rate_value=optimal_learning_rate, num_classes=num_classes)\n",
    "    optimal_cnn.summary()\n",
    "    optimal_cnn_history = optimal_cnn.fit(X_train, y_train, epochs=10, validation_split=val_split)\n",
    "    performance_test = optimal_cnn.evaluate(X_test, y_test)\n",
    "    print(f\"****************** The accuracy is {performance_test[1]} ******************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kT-zaS3WO3wM"
   },
   "outputs": [],
   "source": [
    "# Use this cell to test your optimal model on the hold-out test data set\n",
    "\n",
    "# Obtain the perfomance metrics of the optimal model on the testing data set using the \"evaluate()\" method\n",
    "performance_test = optimal_cnn.evaluate(X_test, y_test)\n",
    "\n",
    "print('The accuracy of the model on the testing data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(\"best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq0s4S5XXzbO"
   },
   "source": [
    "### Checklist\n",
    "- Decided on a minimum validation accuracy\n",
    "- Trained multiple models till that accuracy is achieved\n",
    "- Your optimal model does well on the testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEevkruUX70f"
   },
   "source": [
    "Now that you have completed all the tasks in the assignment, please move on to create your analysis report, and subsequently prepare to submit the required files to the platform."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
