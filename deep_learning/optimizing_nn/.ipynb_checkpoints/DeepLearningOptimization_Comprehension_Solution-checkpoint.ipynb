{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a845d97",
   "metadata": {
    "id": "mIEb3H8DUnjy"
   },
   "source": [
    "# Task 1 - Setup and Data Preparation\n",
    "For this task, you will:\n",
    "- Import necessary packages for executing the code\n",
    "- Install the EMNIST package\n",
    "- Load the EMINST (letters) data and study its basic features such as its shape\n",
    "- Convert the pixel gray levels of the images into the range [0,1]\n",
    "- One-hot encode the class labels in the data\n",
    "- Flatten the image data into 1-D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421b96a",
   "metadata": {
    "id": "a82e79ad"
   },
   "outputs": [],
   "source": [
    "# Import 'numpy' and 'pandas' to work with numbers and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import 'pyplot' from 'matplotlib' and 'seaborn' for visualizations\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import methods for building neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Import 'GridSearchCV' for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import 'KerasClassifier' from 'keras' for connecting neural networks with 'sklearn' and 'GridSearchCV'\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Import method to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef2ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the EMNIST package\n",
    "# Note: If you haven't already installed the EMNIST package, run the following code to do so\n",
    "# !pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e09f763",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "a251d675",
    "outputId": "80cb8cc4-9d84-459c-a381-5ecf6d003770",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the 'letters' data (training and testing) from 'emnist'\n",
    "# Note: The EMNIST data size is about 536 MB, so the download may take a couple of minutes\n",
    "from emnist import extract_training_samples, extract_test_samples\n",
    "X_train, y_train = extract_training_samples('letters')\n",
    "X_test, y_test = extract_test_samples('letters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360727fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data shape: ', X_train.shape) # (124800, 28, 28) --- 124800 images, each 28x28 pixels\n",
    "print('Test data shape: ', X_test.shape) # (20800, 28, 28) --- 20800 images, each 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bead67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the frequency of the unique class labels in the training data\n",
    "unique, counts = np.unique(y_train, return_counts = True)\n",
    "print('Train labels: ', dict(zip(unique, counts)))\n",
    "\n",
    "# Look at the frequency of the unique class labels in the testing data\n",
    "unique, counts = np.unique(y_test, return_counts = True)\n",
    "print('Test labels: ', dict(zip(unique, counts)))\n",
    "\n",
    "print('\\n')\n",
    "print('Note that the labels 1, 2, 3, ..., 26 represent the 26 letters of the English alphabet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 25 EMNIST images from the training data and view the images\n",
    "indices = np.random.randint(0, X_train.shape[0], size = 25)\n",
    "\n",
    "images = X_train[indices]\n",
    "labels = y_train[indices]\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    image = images[i]\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pixel gray level values from the range [0, 255] to the range [0,1]\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4621aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a few training data images and their corresponding labels\n",
    "indices = np.random.randint(0, X_train.shape[0], size = 10)\n",
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "indexcount = 0\n",
    "for data_index in indices:\n",
    "    indexcount = indexcount + 1\n",
    "    plt.subplot(1, 10, indexcount)\n",
    "    plt.imshow(X_train[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_train[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a few testing data images and their corresponding labels\n",
    "indices = np.random.randint(0, X_test.shape[0], size = 10)\n",
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "indexcount = 0\n",
    "for data_index in indices:\n",
    "    indexcount = indexcount + 1\n",
    "    plt.subplot(1, 10, indexcount)\n",
    "    plt.imshow(X_test[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_test[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bfe3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the class labels to one-hot encoded vectors using the 'to_categorical()' function\n",
    "num_classes = 26\n",
    "\n",
    "# Note: Reduce all y labels by 1 to ensure that labeling starts at 0 and ends at 25\n",
    "y_train = to_categorical(y_train - 1, num_classes)\n",
    "y_test = to_categorical(y_test - 1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dab6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a couple of training data images and their corresponding labels\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "for data_index in np.arange(0, 2, 1):\n",
    "    plt.subplot(1, 2, data_index + 1)\n",
    "    plt.imshow(X_train[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_train[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a couple of testing data images and their corresponding labels\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "for data_index in np.arange(0, 2, 1):\n",
    "    plt.subplot(1, 2, data_index + 1)\n",
    "    plt.imshow(X_test[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_test[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images by converting them into a list of values\n",
    "image_vector_size = 28 * 28\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], image_vector_size)\n",
    "X_test = X_test.reshape(X_test.shape[0], image_vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a135f",
   "metadata": {
    "id": "e359ec34"
   },
   "source": [
    "# Task 2 - FCFNN\n",
    "For this task, you will perform the following steps:\n",
    "- Create a function to build a neural network with fixed and variable values for different hyperparameters\n",
    "  - Fixed hyperparameters\n",
    "    - Number of hidden layers = 1\n",
    "    - Number of neurons in the hidden layer = 128\n",
    "    - Activation function for all output layer neurons = *softmax*\n",
    "    - Loss function = *categorical_crossentropy*\n",
    "  - Variable hyperparameters\n",
    "    - Activation function for all neurons except the output layer ones (default = *sigmoid*)\n",
    "    - Learning rate (default = 0.001 with *RMSprop* optimizer)\n",
    "- The *batch_size* hyperparameter will be treated as variable and will be tuned during the training phase (*Keras* default = *None* which is 32)\n",
    "- Build a default neural network using the custom function and train it on the training data\n",
    "- Check its performance on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a neural network model and specify default values for variable hyperparameters\n",
    "# Note: The number of hidden layers is fixed at 1\n",
    "# Note: The number of neurons in the hidden layer is fixed at 128\n",
    "# Note: The output layer activation function is fixed as 'softmax'\n",
    "# Note: The loss function is fixed as 'categorical_crossentropy'\n",
    "def create_nn(activation_function = 'sigmoid',\n",
    "              learning_rate_value = 0.001):\n",
    "    \n",
    "    # Declare an instance of an artificial neural network model using the 'Sequential()' method\n",
    "    nn = Sequential()\n",
    "    \n",
    "    # Add a hidden layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 128 - This specifies the number of neurons in the hidden layer\n",
    "    # Note: Set the 'input_shape' parameter to (image_vector_size, ) - This specifies the number of input features for each record\n",
    "    # Note: Set the 'activation' parameter to 'activation_function' - This specifies the activation function parameter defined in the custom function\n",
    "    nn.add(Dense(units = 128,\n",
    "                 input_shape = (image_vector_size, ),\n",
    "                 activation = activation_function))\n",
    "    \n",
    "    # Add the output layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 'num_classes' - Multiclass classification with 26 classes requires 26 output neurons\n",
    "    # Note: Set the 'activation' parameter to 'softmax' - The softmax activation function is commonly used for output layer neurons in multiclass classification tasks\n",
    "    nn.add(Dense(units = num_classes,\n",
    "                 activation = 'softmax'))\n",
    "    \n",
    "    # Compile the model using the 'compile()' method\n",
    "    # Note: Set the 'loss' parameter to 'categorical_crossentropy' - The categorical crossentropy loss function is commonly used for multiclass classification tasks\n",
    "    # Note: Set the 'metrics' parameter to 'accuracy' - This records the accuracy of the model along with the loss during training\n",
    "    # Note: Set the 'optimizer' parameter to 'RMSprop' and set its 'learning_rate' parameter to 'learning_rate_value' - This specifies the learning rate value defined in the custom function\n",
    "    nn.compile(loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy'],\n",
    "               optimizer = RMSprop(learning_rate = learning_rate_value))\n",
    "    \n",
    "    return(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf62e1d",
   "metadata": {
    "id": "6a328625",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a default neural network using the 'create_nn' function and train it on the training data\n",
    "nn1 = create_nn()\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'validation_split' parameter to 0.2 - This sets aside 20% of the training data as validation data\n",
    "# Note: Set the 'epochs' parameter to 200 - This specifies the scope of loss computations and parameter updates\n",
    "# Note: Set the 'batch_size' to 'X_train.shape[0]' - This specifies the batch size as the complete training data set instead of the default value of 32\n",
    "nn1.summary()\n",
    "print('\\n')\n",
    "nn1_history = nn1.fit(X_train, y_train, batch_size = X_train.shape[0], validation_split = 0.2, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73705c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the neural network history object into a data frame to view its specifics\n",
    "hist = pd.DataFrame(nn1_history.history)\n",
    "hist['epoch'] = nn1_history.epoch\n",
    "hist['epoch'] = hist['epoch'].apply(lambda x: x + 1)\n",
    "hist.set_index('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2820d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the testing data set using the 'evaluate()' method\n",
    "performance_test = nn1.evaluate(X_test, y_test)\n",
    "\n",
    "print('The loss value of the model on the test data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the test data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2c254",
   "metadata": {},
   "source": [
    "# Task 3 - FCFNN Hyperparameter Tuning: Batch Size\n",
    "For this task, you will perform the following steps:\n",
    "- Build a default neural network using the custom function and train it on the training data for different batch sizes\n",
    "  - Note that less epochs are required to train the model if the training data is divided into batches\n",
    "- View the impact of batch size on the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27457b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a range of batch sizes to tune for\n",
    "batch_size_list = [32, 128, 512, 2048]\n",
    "\n",
    "# Create and train a neural network model for each value of batch size\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 4\n",
    "indexcount = -1\n",
    "\n",
    "for current_batch_size in batch_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a default neural network model using the 'create_nn' function\n",
    "    nn = create_nn()\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'epochs' parameter to 10 - A smaller number of epochs is required to train the model if the training data is divided into batches\n",
    "    # Note: Set the 'batch_size' to 'current_batch_size' - This specifies the current batch size\n",
    "    print('\\n Training and validation for batch size = {} - START \\n'.format(current_batch_size))\n",
    "    nn.summary()\n",
    "    print('\\n')\n",
    "    nn_history = nn.fit(X_train, y_train, batch_size = current_batch_size, validation_split = 0.2, epochs = 10)\n",
    "    print('\\n Training and validation for batch size = {} - END \\n'.format(current_batch_size))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(nn_history.history)\n",
    "    hist[indexcount]['epoch'] = nn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Batch Size': current_batch_size,\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][9],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][9]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b79fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of batch size\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Batch Size', y = 'Train Accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = performance_df, x = 'Batch Size', y = 'Validation Accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Batch Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training accuracies as functions of epochs for different values of batch size\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_batch_size in batch_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'batch size =' + str(current_batch_size))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy as a Function of Epoch for Different Values of Batch Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the validation accuracies as functions of epochs for different values of batch size\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_batch_size in batch_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'val_accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'batch size =' + str(current_batch_size))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy as a Function of Epoch for Different Values of Batch Size');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e6f89",
   "metadata": {},
   "source": [
    "# Task 4 - FCFNN Hyperparameter Tuning: Activation Function\n",
    "For this task, you will perform the following steps:\n",
    "- Build a neural network using the custom function and train it on the training data using different activation functions\n",
    "  - For this task, the training data will not be divided into batches, and consequently a higher number of epochs will be required to train the model\n",
    "- View the impact of the type of activation function on the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4a23d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a list of activation functions to tune for\n",
    "activation_function_list = ['sigmoid', 'relu']\n",
    "\n",
    "# Create and train a neural network model for each type of activation function\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 2\n",
    "indexcount = -1\n",
    "\n",
    "for current_activation_function in activation_function_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a neural network model using the 'create_nn' function\n",
    "    # Note: Set the 'activation_function' parameter to 'current_activation_function' - This specifies the current activation function\n",
    "    nn = create_nn(activation_function = current_activation_function)\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'epochs' parameter to 200\n",
    "    # Note: Set the 'batch_size' to 'X_train.shape[0]'\n",
    "    print('\\n Training and validation for {} activation function - START \\n'.format(current_activation_function))\n",
    "    nn.summary()\n",
    "    print('\\n')\n",
    "    nn_history = nn.fit(X_train, y_train, batch_size = X_train.shape[0], validation_split = 0.2, epochs = 200)\n",
    "    print('\\n Training and validation for {} activation function - END \\n'.format(current_activation_function))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(nn_history.history)\n",
    "    hist[indexcount]['epoch'] = nn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Activation Function': current_activation_function,\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][199],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][199]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbef651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of activation function type\n",
    "performance_df.set_index('Activation Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c185150a",
   "metadata": {},
   "source": [
    "# Task 5 - FCFNN Hyperparameter Tuning: Learning Rate\n",
    "For this task, you will perform the following steps:\n",
    "- Build a neural network using the custom function and train it on the training data for different learning rates\n",
    "  - For this task, the training data will not be divided into batches, and consequently a higher number of epochs will be required to train the model\n",
    "- View the impact of learning rate on the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f59c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a range of learning rates to tune for\n",
    "learning_rate_list = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Create and train a neural network model for each value of learning rate\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 4\n",
    "indexcount = -1\n",
    "\n",
    "for current_learning_rate in learning_rate_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a neural network model using the 'create_nn' function\n",
    "    # Note: Set the 'learning_rate_value' parameter to 'current_learning_rate' - This specifies the current learning rate\n",
    "    nn = create_nn(learning_rate_value = current_learning_rate)\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'epochs' parameter to 200\n",
    "    # Note: Set the 'batch_size' to 'X_train.shape[0]'\n",
    "    print('\\n Training and validation for learning rate = {} - START \\n'.format(current_learning_rate))\n",
    "    nn.summary()\n",
    "    print('\\n')\n",
    "    nn_history = nn.fit(X_train, y_train, batch_size = X_train.shape[0], validation_split = 0.2, epochs = 200)\n",
    "    print('\\n Training and validation for learning rate = {} - END \\n'.format(current_learning_rate))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(nn_history.history)\n",
    "    hist[indexcount]['epoch'] = nn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Learning Rate': current_learning_rate,\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][199],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][199]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of learning rate\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Learning Rate', y = 'Train Accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'Learning Rate', y = 'Validation Accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Learning Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dd7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training accuracies as functions of epochs for different values of learning rate\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_learning_rate in learning_rate_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'learning rate = ' + str(current_learning_rate))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy as a Function of Epoch for Different Values of Learning Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490337b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the validation accuracies as functions of epochs for different values of learning rate\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_learning_rate in learning_rate_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'val_accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'learning rate = ' + str(current_learning_rate))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy as a Function of Epoch for Different Values of Learning Rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6199df",
   "metadata": {},
   "source": [
    "# Task 6 - FCFNN Hyperparameter Tuning: Combinations of Hyperparameters\n",
    "For this task, you will perform the following steps:\n",
    "- Build a neural network (tuned for batch size and learning rate) using *keras* and train it on the training data\n",
    "  - Note that less epochs are required to train the model if the training data is divided into batches\n",
    "- Test its performance on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe6c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a basic neural network object using the 'KerasClassifier()' method\n",
    "# Note: Set the 'build_fn' parameter to 'create_nn' - This converts the 'create_nn' function into a 'KerasClassifier' object\n",
    "base_grid_model = KerasClassifier(build_fn = create_nn)\n",
    "\n",
    "# Define a list of 'activation_function', 'batch_size' and 'learning_rate_value' parameters and store it in a parameter grid dictionary\n",
    "# Note: 'KerasClassifier()' recognizes the 'batch_size' parameter even though it is not defined explicitly in 'create_nn()'\n",
    "parameters_grid = {'activation_function': ['sigmoid', 'relu'],\n",
    "                   'batch_size': [32, 512],\n",
    "                   'learning_rate_value': [0.01, 0.1]}\n",
    "\n",
    "# Perform a grid search using the 'GridSearchCV()' method to obtain a grid on which to fit the training data\n",
    "# Note: Set the 'estimator' parameter to 'base_grid_model' - This specifies the estimator to be used by 'GridSearchCV()'\n",
    "# Note: Set the 'param_grid' parameter to 'parameters_grid' - This specifies the grid of parameters to search over\n",
    "# Note: Set the 'cv' parameter to 2 - This specifies the number of folds in the cross-validation process\n",
    "# Note: Set the 'verbose' parameter to 4 - This helps show more relevant information during training\n",
    "grid = GridSearchCV(estimator = base_grid_model,\n",
    "                    param_grid = parameters_grid,\n",
    "                    cv = 2,\n",
    "                    verbose = 4)\n",
    "\n",
    "# Train the model on the training data using the 'fit()' method\n",
    "# Note: Set the 'epochs' parameter to 10\n",
    "# Note: The 'validation_split' parameter isn't particularly required since cross-validation is already in place\n",
    "grid_model = grid.fit(X_train, y_train, epochs = 10)\n",
    "\n",
    "# Print the optimal values of 'activation_function', 'batch_size' and 'learning_rate_value'\n",
    "best_activation_function = grid_model.best_params_['activation_function']\n",
    "best_batch_size = grid_model.best_params_['batch_size']\n",
    "best_learning_rate = grid_model.best_params_['learning_rate_value']\n",
    "best_accuracy = grid_model.best_score_\n",
    "\n",
    "print('The optimal type of activation function is', best_activation_function)\n",
    "print('The optimal value of batch size is', best_batch_size)\n",
    "print('The optimal value of learning rate is ', best_learning_rate)\n",
    "print('The accuracy of the model with these optimal parameters is ', best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838332c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrain the model with the optimal combination of hyperparameters and save its training history\n",
    "\n",
    "# Use the 'create_nn' function to create a neural network with the optimal values of 'activation_function', 'batch_size' and 'learning_rate_value'\n",
    "# Note: Set the 'activation_function' parameter to 'best_activation_function' - This specifies the optimal value for the 'activation_function' parameter\n",
    "# Note: Set the 'learning_rate_value' parameter to 'best_learning_rate' - This specifies the optimal value for the 'learning_rate_value' parameter\n",
    "nn2 = create_nn(activation_function = best_activation_function, learning_rate_value = best_learning_rate)\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'validation_split' parameter to 0.2\n",
    "# Note: Set the 'epochs' parameter to 10\n",
    "# Note: Set the 'batch_size' to 'best_batch_size' - This specifies the optimal value for the 'batch_size' parameter\n",
    "nn2.summary()\n",
    "print('\\n')\n",
    "nn2_history = nn2.fit(X_train, y_train, batch_size = best_batch_size, validation_split = 0.2, epochs = 10)\n",
    "hist = pd.DataFrame(nn2_history.history)\n",
    "hist['epoch'] = nn2_history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565ad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the testing data set using the 'evaluate()' method\n",
    "performance_test = nn2.evaluate(X_test, y_test)\n",
    "\n",
    "print('The loss value of the model on the test data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the test data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f681898",
   "metadata": {},
   "source": [
    "# Task 7 - Early Stopping\n",
    "For this task, you will perform the following steps:\n",
    "- Build a neural network (with the same settings as *nn2*) using the custom function and train it on the training data with early stopping criteria\n",
    "- Test its performance on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fdb1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use basic early stopping with patience to prevent overfitting in 'nn2'\n",
    "\n",
    "# Create a neural network using the 'create_nn' function with optimal hyperparameter settings\n",
    "# Note: Set the 'activation_function' parameter to 'best_activation_function'\n",
    "# Note: Set the 'learning_rate_value' parameter to 'best_learning_rate'\n",
    "nn3 = create_nn(activation_function = best_activation_function,\n",
    "                learning_rate_value = best_learning_rate)\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'batch_size' parameter to 'best_batch_size'\n",
    "# Note: Set the 'validation_split' parameter to 0.2\n",
    "# Note: Set the 'epochs' parameter to 20\n",
    "# Note: Set the 'callbacks' parameter to [EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1)]\n",
    "nn3.summary()\n",
    "print('\\n')\n",
    "nn3_history = nn3.fit(X_train,\n",
    "                      y_train,\n",
    "                      batch_size = best_batch_size,\n",
    "                      validation_split = 0.2,\n",
    "                      epochs = 20,\n",
    "                      callbacks = [EarlyStopping(monitor = 'val_loss',\n",
    "                                                 mode = 'min',\n",
    "                                                 verbose = 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fd1d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use advanced early stopping with patience to prevent overfitting in 'nn2'\n",
    "\n",
    "# Create a neural network using the 'create_nn' function with optimal hyperparameter settings\n",
    "# Note: Set the 'activation_function' parameter to 'best_activation_function'\n",
    "# Note: Set the 'learning_rate_value' parameter to 'best_learning_rate'\n",
    "nn3 = create_nn(activation_function = best_activation_function,\n",
    "                learning_rate_value = best_learning_rate)\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'batch_size' parameter to 'best_batch_size'\n",
    "# Note: Set the 'validation_split' parameter to 0.2\n",
    "# Note: Set the 'epochs' parameter to 20\n",
    "# Note: Set the 'callbacks' parameter to [EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 2, verbose = 1)]\n",
    "nn3.summary()\n",
    "print('\\n')\n",
    "nn3_history = nn3.fit(X_train,\n",
    "                      y_train,\n",
    "                      batch_size = best_batch_size,\n",
    "                      validation_split = 0.2,\n",
    "                      epochs = 20,\n",
    "                      callbacks = [EarlyStopping(monitor = 'val_loss',\n",
    "                                                 mode = 'min',\n",
    "                                                 patience = 2,\n",
    "                                                 verbose = 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the neural network history object into a data frame to view its specifics\n",
    "hist = pd.DataFrame(nn3_history.history)\n",
    "hist['epoch'] = nn3_history.epoch\n",
    "hist['epoch'] = hist['epoch'].apply(lambda x: x + 1)\n",
    "hist.set_index('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ac98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39932f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the testing data set using the 'evaluate()' method\n",
    "performance_test = nn3.evaluate(X_test, y_test)\n",
    "\n",
    "print('The loss value of the model on the test data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the test data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f4104",
   "metadata": {},
   "source": [
    "# Task 8 - Dropouts\n",
    "For this task, you will perform the following steps:\n",
    "- Redefine the custom function to build a neural network with an additional dropout layer and variable dropout rates\n",
    "- Create a neural network (with the same settings as *nn2* and an additional dropout layer) using the new custom function and train it on the training data\n",
    "- View the impact of dropout rate on the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f826bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a neural network model and specify a default value for dropout rate\n",
    "# Note: The number of hidden layers is fixed at 1\n",
    "# Note: The number of neurons in the hidden layer is fixed at 128\n",
    "# Note: The activation function is fixed as 'best_activation_function'\n",
    "# Note: The output layer activation function is fixed as 'softmax'\n",
    "# Note: The loss function is fixed as 'categorical_crossentropy'\n",
    "# Note: The learning rate is fixed as 'best_learning_rate'\n",
    "# Note: The number of dropout layers is fixed at 1\n",
    "def create_dropout_nn(dropout_rate = 0.5):\n",
    "    \n",
    "    # Declare an instance of an artificial neural network model using the 'Sequential()' method\n",
    "    nn = Sequential()\n",
    "    \n",
    "    # Add a hidden layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 128\n",
    "    # Note: Set the 'input_shape' parameter to (image_vector_size, )\n",
    "    # Note: Set the 'activation' parameter to 'best_activation_function'\n",
    "    nn.add(Dense(units = 128,\n",
    "                 input_shape = (image_vector_size, ),\n",
    "                 activation = best_activation_function))\n",
    "    \n",
    "    # Add a dropout layer using the 'add()' and 'Dropout()' methods\n",
    "    # Note: Set the 'rate' parameter to 'dropout_rate' - This specifies the dropout rate parameter defined in the custom function\n",
    "    nn.add(Dropout(rate = dropout_rate))\n",
    "    \n",
    "    # Add the output layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 'num_classes'\n",
    "    # Note: Set the 'activation' parameter to 'softmax'\n",
    "    nn.add(Dense(units = num_classes,\n",
    "                 activation = 'softmax'))\n",
    "    \n",
    "    # Compile the model using the 'compile()' method\n",
    "    # Note: Set the 'loss' parameter to 'categorical_crossentropy'\n",
    "    # Note: Set the 'metrics' parameter to 'accuracy'\n",
    "    # Note: Set the 'optimizer' parameter to 'RMSprop' and set its 'learning_rate' parameter to 'best_learning_rate'\n",
    "    nn.compile(loss = 'categorical_crossentropy',\n",
    "               metrics = ['accuracy'],\n",
    "               optimizer = RMSprop(learning_rate = best_learning_rate))\n",
    "    \n",
    "    return(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b397a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a range of dropout rates to tune for\n",
    "dropout_rate_list = [0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "# Create and train a neural network model for each value of dropout rate\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 4\n",
    "indexcount = -1\n",
    "\n",
    "for current_dropout_rate in dropout_rate_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a neural network model using the 'create_dropout_nn' function\n",
    "    # Note: Set the 'dropout_rate' parameter to 'current_dropout_rate' - This specifies the current dropout rate\n",
    "    nn = create_dropout_nn(dropout_rate = current_dropout_rate)\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'epochs' parameter to 10\n",
    "    # Note: Set the 'batch_size' to 'best_batch_size'\n",
    "    print('\\n Training and validation for dropout rate = {} - START \\n'.format(current_dropout_rate))\n",
    "    nn.summary()\n",
    "    print('\\n')\n",
    "    nn_history = nn.fit(X_train, y_train, batch_size = best_batch_size, validation_split = 0.2, epochs = 10)\n",
    "    print('\\n Training and validation for dropout rate = {} - END \\n'.format(current_dropout_rate))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(nn_history.history)\n",
    "    hist[indexcount]['epoch'] = nn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Dropout Rate': current_dropout_rate,\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][9],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][9]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec0d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of dropout rate\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Dropout Rate', y = 'Train Accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'Dropout Rate', y = 'Validation Accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Dropout Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Dropout Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e757a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3M4_v4b.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
