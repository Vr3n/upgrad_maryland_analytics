{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a845d97"
   },
   "source": [
    "# Task 1 - Setup and Data Preparation\n",
    "For this task, you will:\n",
    "- Import necessary packages for executing the code\n",
    "- Install the EMNIST package\n",
    "- Load the EMINST (letters) data and study its basic features such as its shape\n",
    "- Convert the pixel gray levels of the images into the range [0,1]\n",
    "- One-hot encode the class labels in the data\n",
    "- Flatten the image data into 1-D arrays for FCFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4421b96a"
   },
   "outputs": [],
   "source": [
    "# Import 'numpy' and 'pandas' to work with numbers and dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import 'pyplot' from 'matplotlib' and 'seaborn' for visualizations\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import methods for building neural networks\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Import 'GridSearchCV' for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Import 'KerasClassifier' from 'keras' for connecting neural networks with 'sklearn' and 'GridSearchCV'\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "# Import method to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ef2ba56"
   },
   "outputs": [],
   "source": [
    "# Install the EMNIST package\n",
    "# Note: If you haven't already installed the EMNIST package, run the following code to do so\n",
    "# !pip install emnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6e09f763",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the 'letters' data (training and testing) from 'emnist'\n",
    "# Note: The EMNIST data size is about 536 MB, so the download may take a couple of minutes\n",
    "from emnist import extract_training_samples, extract_test_samples\n",
    "X_train, y_train = extract_training_samples('letters')\n",
    "X_test, y_test = extract_test_samples('letters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "360727fa"
   },
   "outputs": [],
   "source": [
    "print('Train data shape: ', X_train.shape) # (124800, 28, 28) --- 124800 images, each 28x28 pixels\n",
    "print('Test data shape: ', X_test.shape) # (20800, 28, 28) --- 20800 images, each 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26bead67"
   },
   "outputs": [],
   "source": [
    "# Look at the frequency of the unique class labels in the training data\n",
    "unique, counts = np.unique(y_train, return_counts = True)\n",
    "print('Train labels: ', dict(zip(unique, counts)))\n",
    "\n",
    "# Look at the frequency of the unique class labels in the testing data\n",
    "unique, counts = np.unique(y_test, return_counts = True)\n",
    "print('Test labels: ', dict(zip(unique, counts)))\n",
    "\n",
    "print('\\n')\n",
    "print('Note that the labels 1, 2, 3, ..., 26 represent the 26 letters of the English alphabet.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d61176eb"
   },
   "outputs": [],
   "source": [
    "# Sample 25 EMNIST images from the training data and view the images\n",
    "indices = np.random.randint(0, X_train.shape[0], size = 25)\n",
    "\n",
    "images = X_train[indices]\n",
    "labels = y_train[indices]\n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    image = images[i]\n",
    "    plt.imshow(image, cmap = 'gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2a45fb3e"
   },
   "outputs": [],
   "source": [
    "# Convert the pixel gray level values from the range [0, 255] to the range [0,1]\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4621aa2"
   },
   "outputs": [],
   "source": [
    "# View a few training data images and their corresponding labels\n",
    "indices = np.random.randint(0, X_train.shape[0], size = 10)\n",
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "indexcount = 0\n",
    "for data_index in indices:\n",
    "    indexcount = indexcount + 1\n",
    "    plt.subplot(1, 10, indexcount)\n",
    "    plt.imshow(X_train[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_train[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fc6b60c"
   },
   "outputs": [],
   "source": [
    "# View a few testing data images and their corresponding labels\n",
    "indices = np.random.randint(0, X_test.shape[0], size = 10)\n",
    "plt.figure(figsize = (16, 8))\n",
    "\n",
    "indexcount = 0\n",
    "for data_index in indices:\n",
    "    indexcount = indexcount + 1\n",
    "    plt.subplot(1, 10, indexcount)\n",
    "    plt.imshow(X_test[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_test[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01bfe3b9"
   },
   "outputs": [],
   "source": [
    "# Convert the class labels to one-hot encoded vectors using the 'to_categorical()' function\n",
    "num_classes = 26\n",
    "\n",
    "# Note: Reduce all y labels by 1 to ensure that labeling starts at 0 and ends at 25\n",
    "y_train = to_categorical(y_train - 1, num_classes)\n",
    "y_test = to_categorical(y_test - 1, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dab6bc4"
   },
   "outputs": [],
   "source": [
    "# View a couple of training data images and their corresponding labels\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "for data_index in np.arange(0, 2, 1):\n",
    "    plt.subplot(1, 2, data_index + 1)\n",
    "    plt.imshow(X_train[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_train[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43af5403"
   },
   "outputs": [],
   "source": [
    "# View a couple of testing data images and their corresponding labels\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "for data_index in np.arange(0, 2, 1):\n",
    "    plt.subplot(1, 2, data_index + 1)\n",
    "    plt.imshow(X_test[data_index], cmap = 'gray')\n",
    "    plt.title(str(y_test[data_index]))\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e02f0efd"
   },
   "outputs": [],
   "source": [
    "# Flatten the images by converting them into a list of values\n",
    "image_vector_size = 28 * 28\n",
    "\n",
    "X_train_fcfnn = X_train.reshape(X_train.shape[0], image_vector_size)\n",
    "X_test_fcfnn = X_test.reshape(X_test.shape[0], image_vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c0a135f"
   },
   "source": [
    "# Task 2 - FCFNN\n",
    "For this task, you will perform the following steps:\n",
    "- Build a neural network with the following specifications:\n",
    "  - Number of hidden layers = 1\n",
    "  - Number of neurons in the hidden layer = 128\n",
    "  - Activation function for all neurons except the output layer ones = *sigmoid*\n",
    "  - Activation function for all output layer neurons = *softmax*\n",
    "  - Loss function = *categorical_crossentropy*\n",
    "  - Learning rate = 0.01 (with *RMSprop* optimizer)\n",
    "- Train the network on the training data with a *batch_size* of 512\n",
    "- Check its performance on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ff0d0416",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare an instance of an artificial neural network model using the 'Sequential()' method\n",
    "nn1 = ##### CODE HERE #####\n",
    "\n",
    "# Add a hidden layer using the 'add()' and 'Dense()' methods\n",
    "# Note: Set the 'units' parameter to 128 - This specifies the number of neurons in the hidden layer\n",
    "# Note: Set the 'input_shape' parameter to (image_vector_size, ) - This specifies the number of input features for each record\n",
    "# Note: Set the 'activation' parameter to 'sigmoid' - This specifies the activation function for this layer\n",
    "nn1.##### CODE HERE #####\n",
    "\n",
    "# Add the output layer using the 'add()' and 'Dense()' methods\n",
    "# Note: Set the 'units' parameter to 'num_classes' - Multiclass classification with 26 classes requires 26 output neurons\n",
    "# Note: Set the 'activation' parameter to 'softmax' - The softmax activation function is commonly used for output layer neurons in multiclass classification tasks\n",
    "nn1.##### CODE HERE #####\n",
    "\n",
    "# Compile the model using the 'compile()' method\n",
    "# Note: Set the 'loss' parameter to 'categorical_crossentropy' - The categorical crossentropy loss function is commonly used for multiclass classification tasks\n",
    "# Note: Set the 'metrics' parameter to 'accuracy' - This records the accuracy of the model along with the loss during training\n",
    "# Note: Set the 'optimizer' parameter to 'RMSprop' and set its 'learning_rate' parameter to 0.01 - This specifies the learning rate value\n",
    "nn1.##### CODE HERE #####\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'validation_split' parameter to 0.2 - This sets aside 20% of the training data as validation data\n",
    "# Note: Set the 'epochs' parameter to 10 - This specifies the scope of loss computations and parameter updates\n",
    "# Note: Set the 'batch_size' to 512 - This specifies the batch size as 512 instead of the default value of 32\n",
    "nn1.summary()\n",
    "print('\\n')\n",
    "nn1_history = nn1.##### CODE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73705c1a"
   },
   "outputs": [],
   "source": [
    "# Convert the neural network history object into a data frame to view its specifics\n",
    "hist = pd.DataFrame(nn1_history.history)\n",
    "hist['epoch'] = nn1_history.epoch\n",
    "hist['epoch'] = hist['epoch'].apply(lambda x: x + 1)\n",
    "hist.set_index('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf05c834"
   },
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f2820d7a"
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the testing data set using the 'evaluate()' method\n",
    "performance_test = nn1.evaluate(X_test_fcfnn, y_test)\n",
    "\n",
    "print('The loss value of the model on the test data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the test data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-azlkdLJHvcE"
   },
   "source": [
    "# Task 3 - CNN: Hyperparameter Tuning\n",
    "For this task, you will perform the following steps:\n",
    "- Define a function that creates a CNN with the previously fixed specifications for the FCFNN part and the following fixed and variable hyperparameters for the CNN part:\n",
    "  - Fixed hyperparameters:\n",
    "    - Number of *Conv2D* and *MaxPooling2D* layer pairs = 1\n",
    "  - Variable hyperparameters:\n",
    "    - Number of filters in the *Conv2D* layer (set default as 1)\n",
    "    - Filter size of filters in the *Conv2D* layer (set default as (3, 3))\n",
    "      - Fix the strides for *Conv2D* layer filters as the *Keras* default (which is (1, 1))\n",
    "    - Filter size of kernels in the *MaxPooling2D* layer (set default as (2, 2))\n",
    "      - Fix the strides for *MaxPooling2D* layer filters as the *Keras* default (same as the filter size)\n",
    "- Train the network on the training data set and observe its performance\n",
    "- Tune the CNN model for the following hyperparameters:\n",
    "  - Number of filters in the convolution layer\n",
    "  - Size of filters in the convolution layer\n",
    "  - Size of filters in the max pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DBQfLUM1IWmi"
   },
   "outputs": [],
   "source": [
    "# Define a function to create a CNN model with fixed and variable parameters along with previously fixed settings for FCFNN\n",
    "# Note: The number of CNN layer pairs is fixed at 1\n",
    "# Note: The strides of the convolution layer filters is fixed as the Keras default (which is (1, 1))\n",
    "# Note: The strides of the max pooling layer filters is fixed as the Keras default (same as the filter size)\n",
    "def create_cnn(n_filters = 1,\n",
    "               filter_size = (3, 3),\n",
    "               pool_filter_size = (2, 2)):\n",
    "  \n",
    "    # Declare an instance of a CNN model using the 'Sequential()' method\n",
    "    cnn = ##### CODE HERE #####\n",
    "    \n",
    "    # Add a convolution layer using the 'add()' and 'Conv2D()' methods\n",
    "    # Note: Set the 'filters' parameter to 'n_filters' - This specifies the number of fiters in the convolution layer\n",
    "    # Note: Set the 'kernel_size' parameter to 'filter_size' - This specifies the size of the filters in the convolution layer\n",
    "    # Note: Leave the 'strides' parameter at its default value - Keras uses (1, 1) by default\n",
    "    # Note: Set the 'input_shape' parameter to '(28, 28, 1)' - This specifies the shape of the input given to the convolution layer   \n",
    "    cnn.##### CODE HERE #####\n",
    "    \n",
    "    # Add a max pooling layer using the 'add()' and 'MaxPooling2D()' methods\n",
    "    # Note: Set the 'pool_size' parameter to 'pool_filter_size' - This specifies the size of the filters in the maxpooling layer\n",
    "    # Note: Leave the 'strides' parameter at its default value - Keras uses the filter size as the stride by default\n",
    "    cnn.##### CODE HERE #####\n",
    "    \n",
    "    # Add a flatten layer using the 'add()' and 'Flatten()' methods\n",
    "    cnn.##### CODE HERE #####\n",
    "    \n",
    "    # Add a hidden layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 128\n",
    "    # Note: Set the 'activation' parameter to 'sigmoid'\n",
    "    cnn.##### CODE HERE #####\n",
    "    \n",
    "    # Add the output layer using the 'add()' and 'Dense()' methods\n",
    "    # Note: Set the 'units' parameter to 'num_classes'\n",
    "    # Note: Set the 'activation' parameter to 'softmax'\n",
    "    cnn.##### CODE HERE #####\n",
    "    \n",
    "    # Compile the model using the 'compile()' method\n",
    "    # Note: Set the 'loss' parameter to 'categorical_crossentropy'\n",
    "    # Note: Set the 'metrics' parameter to 'accuracy'\n",
    "    # Note: Set the 'optimizer' parameter to 'RMSprop' and set its 'learning_rate' parameter to 0.01\n",
    "    cnn.##### CODE HERE #####\n",
    "    \n",
    "    return(cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kmDwAQnTlf_"
   },
   "source": [
    "## Subtask 1 - Hyperparameter Tuning: Number of Convolution Filters\n",
    "For this task, you will tune the CNN for the number of filters in the convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca8f59c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a range of number of filters in the convolution layer to tune for\n",
    "n_filters_list = [1, 4, 16, 32]\n",
    "\n",
    "# Create and train a CNN model for each value of number of filters\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 4\n",
    "indexcount = -1\n",
    "\n",
    "for current_n_filters in n_filters_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a CNN model using the 'create_cnn' function\n",
    "    # Note: Set the 'n_filters' parameter to 'current_n_filters' - This specifies the current number of filters\n",
    "    cnn = create_cnn(##### CODE HERE #####)\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'batch_size' parameter to 512 and the 'epochs' parameter to 10\n",
    "    print('\\n Training and validation for {} convolution filters - START \\n'.format(current_n_filters))\n",
    "    cnn.summary()\n",
    "    print('\\n')\n",
    "    cnn_history = cnn.##### CODE HERE #####\n",
    "    print('\\n Training and validation for {} convolution filters - END \\n'.format(current_n_filters))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(cnn_history.history)\n",
    "    hist[indexcount]['epoch'] = cnn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Number of Convolution Filters': current_n_filters,\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][9],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][9]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b05b3d7e"
   },
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of number of convolution layer filters\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Number of Convolution Filters', y = 'Train Accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'Number of Convolution Filters', y = 'Validation Accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Number of Convolution Filters')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Number of Convolution Filters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50dd7149"
   },
   "outputs": [],
   "source": [
    "# View the training accuracies as functions of epochs for different values of number of convolution layer filters\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_n_filters in n_filters_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'number of filters = ' + str(current_n_filters))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy as a Function of Epoch for Different Values of Number of Convolution Filters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "490337b7"
   },
   "outputs": [],
   "source": [
    "# View the validation accuracies as functions of epochs for different values of number of convolution layer filters\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_n_filters in n_filters_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'val_accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'number of filters = ' + str(current_n_filters))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy as a Function of Epoch for Different Values of Number of Convolution Filters');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIQ2ZSMAV7Sv"
   },
   "source": [
    "## Subtask 2 - Hyperparameter Tuning: Convolution Filter Size\n",
    "For this task, you will tune the CNN for the size of filters in the convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqmc33mxV7TM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a range of sizes of the filters in the convolution layer to tune for\n",
    "filter_size_list = [3, 5, 7, 9]\n",
    "\n",
    "# Create and train a CNN model for each value of filter size\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 4\n",
    "indexcount = -1\n",
    "\n",
    "for current_filter_size in filter_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a CNN model using the 'create_cnn' function\n",
    "    # Note: Set the 'filter_size' parameter to '(current_filter_size, current_filter_size)' - This specifies the current filter size\n",
    "    cnn = create_cnn(##### CODE HERE #####)\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'batch_size' parameter to 512 and the 'epochs' parameter to 10\n",
    "    print('\\n Training and validation for convolution filter size = {} - START \\n'.format((current_filter_size, current_filter_size)))\n",
    "    cnn.summary()\n",
    "    print('\\n')\n",
    "    cnn_history = cnn.##### CODE HERE #####\n",
    "    print('\\n Training and validation for convolution filter size = {} - END \\n'.format((current_filter_size, current_filter_size)))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(cnn_history.history)\n",
    "    hist[indexcount]['epoch'] = cnn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Convolution Layer Filter Size': str((current_filter_size, current_filter_size)),\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][9],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][9]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn4tZCNVV7TQ"
   },
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of convolution layer filter size\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Convolution Layer Filter Size', y = 'Train Accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'Convolution Layer Filter Size', y = 'Validation Accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Convolution Layer Filter Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Convolution Layer Filter Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIIcoq6_V7TR"
   },
   "outputs": [],
   "source": [
    "# View the training accuracies as functions of epochs for different values of convolution layer filter size\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_filter_size in filter_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'filter size = ' + str(current_filter_size))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy as a Function of Epoch for Different Values of Convolution Layer Filter Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PSCPud8NV7TS"
   },
   "outputs": [],
   "source": [
    "# View the validation accuracies as functions of epochs for different values of convolution layer filter size\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_filter_size in filter_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'val_accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'filter size = ' + str(current_filter_size))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy as a Function of Epoch for Different Values of Convolution Layer Filter Size');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-n3JlMjYpkz"
   },
   "source": [
    "## Subtask 3 - Hyperparameter Tuning: Max Pooling Filter Size\n",
    "For this task, you will tune the CNN for the size of filters in the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzNV4-uhYplA",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Declare a range of sizes of the filters in the max pooling layer to tune for\n",
    "pool_size_list = [2, 4, 7, 14]\n",
    "\n",
    "# Create and train a CNN model for each value of maxpooling layer filter size\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "hist = [None] * 4\n",
    "indexcount = -1\n",
    "\n",
    "for current_pool_size in pool_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    \n",
    "    # Create a CNN model using the 'create_cnn' function\n",
    "    # Note: Set the 'pool_filter_size' parameter to '(current_pool_size, current_pool_size)' - This specifies the current max pooling layer filter size\n",
    "    cnn = create_cnn(##### CODE HERE #####)\n",
    "    \n",
    "    # Capture the training history of the model using the 'fit()' method\n",
    "    # Note: Set the 'validation_split' parameter to 0.2\n",
    "    # Note: Set the 'batch_size' parameter to 512 and the 'epochs' parameter to 10\n",
    "    print('\\n Training and validation for max pooling filter size = {} - START \\n'.format((current_pool_size, current_pool_size)))\n",
    "    cnn.summary()\n",
    "    print('\\n')\n",
    "    cnn_history = cnn.##### CODE HERE #####\n",
    "    print('\\n Training and validation for max pooling filter size = {} - END \\n'.format((current_pool_size, current_pool_size)))\n",
    "    \n",
    "    hist[indexcount] = pd.DataFrame(cnn_history.history)\n",
    "    hist[indexcount]['epoch'] = cnn_history.epoch\n",
    "    \n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Max Pooling Filter Size': str((current_pool_size, current_pool_size)),\n",
    "                                  'Train Accuracy': hist[indexcount]['accuracy'][9],\n",
    "                                  'Validation Accuracy': hist[indexcount]['val_accuracy'][9]})\n",
    "    \n",
    "    performance_df = pd.concat([performance_df, tempdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWjtOZPAYplB"
   },
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of maxpooling layer filter size\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Max Pooling Filter Size', y = 'Train Accuracy', color = 'red', label = 'Train')\n",
    "sns.lineplot(data = performance_df, x = 'Max Pooling Filter Size', y = 'Validation Accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('MaxPool Layer Filter Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of MaxPool Layer Filter Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP2Lix_tYplB"
   },
   "outputs": [],
   "source": [
    "# View the training accuracies as functions of epochs for different values of maxpooling layer filter size\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_pool_size in pool_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'max pooling filter size = ' + str((current_pool_size, current_pool_size)))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.title('Training Accuracy as a Function of Epoch for Different Values of Max Pooling Layer Filter Size');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iljaKDmCYplC"
   },
   "outputs": [],
   "source": [
    "# View the validation accuracies as functions of epochs for different values of maxpooling layer filter size\n",
    "plt.figure(figsize = (14, 4))\n",
    "colorlist = ['purple', 'green', 'blue', 'red']\n",
    "\n",
    "indexcount = -1\n",
    "for current_pool_size in pool_size_list:\n",
    "    indexcount = indexcount + 1\n",
    "    sns.lineplot(data = hist[indexcount],\n",
    "                 x = 'epoch',\n",
    "                 y = 'val_accuracy',\n",
    "                 color = colorlist[indexcount],\n",
    "                 label = 'max pooling filter size = ' + str((current_pool_size, current_pool_size)))\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy as a Function of Epoch for Different Values of Max Pooling Layer Filter Size');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac5jn7bqcTPv"
   },
   "source": [
    "# Task 4 - Hyperparameter Tuning: Combinations of Hyperparameters\n",
    "For this task, you will perform the following steps:\n",
    "- Tune the CNN model on a combination of the following hyperparameters using *GridSearchCV*\n",
    "  - Number of convolution layer filters\n",
    "  - Convolution filter size\n",
    "  - Max pooling filter size\n",
    "- Retrain the CNN model with potentially optimal values for the different hyperparameters\n",
    "- View the performance of the model on the training, validation and testing data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eO026AKFcmRk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a basic CNN object using the 'KerasClassifier()' method\n",
    "# Note: Set the 'build_fn' parameter to 'create_cnn' - This converts the 'create_cnn' function into a 'KerasClassifier' object\n",
    "base_grid_model = KerasClassifier(##### CODE HERE #####)\n",
    "\n",
    "# Define a list of 'n_filters', 'filter_size' and 'pool_filter_size' parameters and store it in a parameter grid dictionary\n",
    "parameters_grid = {'n_filters': [4, 16],\n",
    "                   'filter_size': [(3, 3), (5, 5)],\n",
    "                   'pool_filter_size': [(2, 2), (4, 4)]}\n",
    "\n",
    "# Perform a grid search using the 'GridSearchCV()' method to obtain a grid on which to fit the training data\n",
    "# Note: Set the 'estimator' parameter to 'base_grid_model' - This specifies the estimator to be used by 'GridSearchCV()'\n",
    "# Note: Set the 'param_grid' parameter to 'parameters_grid' - This specifies the grid of parameters to search over\n",
    "# Note: Set the 'cv' parameter to 2 - This specifies the number of folds in the cross-validation process\n",
    "# Note: Set the 'verbose' parameter to 4 - This helps show more relevant information during training\n",
    "grid = GridSearchCV(##### CODE HERE #####)\n",
    "\n",
    "# Train the model on the training data using the 'fit()' method\n",
    "# Note: Set the 'batch_size' parameter to 512 and the 'epochs' parameter to 10\n",
    "# Note: The 'validation_split' parameter isn't particularly required since cross-validation is already in place\n",
    "grid_model = grid.fit(##### CODE HERE #####)\n",
    "\n",
    "# Print the optimal values of 'n_filters', 'filter_size' and 'pool_filter_size'\n",
    "best_n_filters = grid_model.best_params_['n_filters']\n",
    "best_filter_size = grid_model.best_params_['filter_size']\n",
    "best_pool_filter_size = grid_model.best_params_['pool_filter_size']\n",
    "best_accuracy = grid_model.best_score_\n",
    "\n",
    "print('\\n The optimal value of number of convolution filters is', best_n_filters)\n",
    "print('\\n The optimal value of convolution filter size is', best_filter_size)\n",
    "print('\\n The optimal value of maxpooling filter size is', best_pool_filter_size)\n",
    "print('\\n The accuracy of the model with these optimal parameters is ', best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "838332c1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrain the model with the optimal combination of hyperparameters and save its training history\n",
    "\n",
    "# Use the 'create_cnn' function to create a CNN with the optimal values of 'n_filters', 'filter_size' and 'pool_filter_size'\n",
    "# Note: Set the 'n_filters' parameter to 'best_n_filters' - This specifies the optimal value for the 'n_filters' parameter\n",
    "# Note: Set the 'filter_size' parameter to 'best_filter_size' - This specifies the optimal value for the 'filter_size' parameter\n",
    "# Note: Set the 'pool_filter_size' parameter to 'best_pool_filter_size' - This specifies the optimal value for the 'pool_filter_size' parameter\n",
    "cnn1 = create_cnn(##### CODE HERE #####)\n",
    "\n",
    "# Capture the training history of the model using the 'fit()' method\n",
    "# Note: Set the 'validation_split' parameter to 0.2\n",
    "# Note: Set the 'batch_size' parameter to 512 and the 'epochs' parameter to 10\n",
    "cnn1.summary()\n",
    "print('\\n')\n",
    "cnn1_history = cnn1.##### CODE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the neural network history object into a data frame to view its specifics\n",
    "hist = pd.DataFrame(cnn1_history.history)\n",
    "hist['epoch'] = cnn1_history.epoch\n",
    "hist['epoch'] = hist['epoch'].apply(lambda x: x + 1)\n",
    "hist.set_index('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "776c3172"
   },
   "outputs": [],
   "source": [
    "# View the training and validation accuracies as functions of epoch\n",
    "plt.figure(figsize = (14, 4))\n",
    "\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'accuracy', color = 'red', label = 'Training')\n",
    "sns.lineplot(data = hist, x = 'epoch', y = 'val_accuracy', color = 'blue', label = 'Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy as a Function of Epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0565ad1c"
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy of the model on the testing data set using the 'evaluate()' method\n",
    "performance_test = cnn1.evaluate(X_test, y_test)\n",
    "\n",
    "print('The loss value of the model on the test data is {}'.format(performance_test[0]))\n",
    "print('The accuracy of the model on the test data is {}'.format(performance_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
